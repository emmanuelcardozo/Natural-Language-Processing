{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"..\\Desafio 2\\img\\img.png\" width=\"500\" align=\"right\">\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "\n",
    "\n",
    "## TP2\n",
    "\n",
    "### Alumno: Emmanuel Cardozo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se realizará un Rule-Based BOT utilizando la librería SpaCy para relaizar la tokenización y lematización. Luego se usarán los conceptos de TF-IDF y Similitud del Coseno vistos en la primera clase para la creacion del BOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import bs4 as bs\n",
    "import html\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el diccionario 'en_core_web_sm' y aumentamos la longitud máxima para poder cargar textos de mayor tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m nlp\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000000\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso utilizaremos el texto del libro \"Guía del autoestopista galáctico\" para entrenar nuestro chatbot. El mismo se puede encontrar en el siguiente [link](\"https://archive.org/stream/TheultimateHitchhikersGuide/The%20Hitchhiker%27s%20Guide%20To%20The%20Galaxy_djvu.txt\").\n",
    "\n",
    "Luego extraemos la parte donde se encuentra el texto a procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_article(url):\n",
    "    raw_html = urllib.request.urlopen(url)\n",
    "    raw_html = raw_html.read()\n",
    "    return bs.BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "\n",
    "def extract_text_from_html(html, startDelimiter, endDelimiter):\n",
    "    txt = str(html)\n",
    "    start = txt.find(startDelimiter) + len(startDelimiter)\n",
    "    end = txt.find(endDelimiter)\n",
    "    return txt[start:end]\n",
    "\n",
    "\n",
    "html_article = get_html_article(\n",
    "    \"https://archive.org/stream/TheultimateHitchhikersGuide/The%20Hitchhiker%27s%20Guide%20To%20The%20Galaxy_djvu.txt\"\n",
    ")\n",
    "article = extract_text_from_html(html_article, \"<pre>\", \"</pre>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamos el articulo removiendo caracteres especiales y convertimos los símbolos html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(article):\n",
    "    # Remover caracteres especiales\n",
    "    filtered_string = re.sub(r\"\\[[0-9]*\\]\", \" \", article)\n",
    "    filtered_string = re.sub(r\"\\s+\", \" \", filtered_string)\n",
    "    # Convertir simbolos especiales html. Ejemplo: &amp; -> &\n",
    "    filtered_string = html.unescape(filtered_string)\n",
    "    return filtered_string.lower()\n",
    "\n",
    "\n",
    "article = preprocess_article(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos SpaCy para dividir el texto por oraciones y generamos el Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"douglas adams the ultimate hitchhiker's guide complete & unabridged contents: introduction: a guide to the guide the hitchhiker's guide to the galaxy the restaurant at the end of the universe life, the universe and everything so long, and thanks for all the fish young zaphod plays it safe mostly harmless footnotes introduction: a guide to the guide some unhelpful remarks from the author the history of the hitchhiker's guide to the galaxy is now so complicated that every time i tell it i contradict myself, and whenever i do get it right i'm misquoted.\",\n",
       " 'so the publication of this omnibus edition seemed like a good opportunity to set the record straight - or at least firmly crooked.',\n",
       " \"anything that is put down wrong here is, as far as i'm concerned, wrong for good.\",\n",
       " 'the idea for the title first cropped up while i was lying drunk in a field in innsbruck, austria, in 1971.',\n",
       " 'not particularly drunk, just the sort of drunk you get when you have a couple of stiff gossers after not having eaten for two days straight, on account of being a penniless hitchhiker.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(article)\n",
    "\n",
    "corpus = [sent.text.strip() for sent in doc.sents]\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la función 'get_processed_text' para poder tokenizar y lematizar el texto de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenize', 'this', 'example']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_processed_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_list = [token.lemma_.lower() for token in doc]\n",
    "    filtered_sentence = [w for w in lemma_list if w not in string.punctuation]\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "get_processed_text(\"Tokenizing this example!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos la clase 'TfidfVectorizer' de Sklearn para obtener una matriz TF-IDF del corpus y luego usamos 'cosine_similarity' para encontrar el documento que guarda mas similaridad con el texto ingresado como input. Este documento será la respuesta que se mostrará al usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, corpus):\n",
    "    response = \"\"\n",
    "    # Sumar al corpus la pregunta del usuario para calcular\n",
    "    # su cercania con otros documentos/sentencias\n",
    "    corpus.append(user_input)\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=get_processed_text,  # Obtener los tokens lematizados\n",
    "        stop_words=get_processed_text(\n",
    "            \" \".join(ENGLISH_STOP_WORDS)\n",
    "        ),  # Quitar las \"stop words\" del ingles. Preprocesamos para obtener tokens lematizados correctos\n",
    "    )\n",
    "\n",
    "    # Crear los vectores a partir del corpus\n",
    "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n",
    "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
    "\n",
    "    # Obtener el índice del vector más cercano a nuestra oración\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    response = (\n",
    "        \"I am sorry, I could not understand you\"\n",
    "        if vector_matched == 0\n",
    "        else corpus[similar_sentence_number]\n",
    "    )\n",
    "\n",
    "    corpus.remove(user_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el bot ingresando diferentes preguntas y viendo las respuestas que nos devuelve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(human_text):\n",
    "    print(human_text)\n",
    "    return generate_response(human_text.lower(), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Do you know who is Arthur Dent?\n",
      "A: \"you are arthur dent?\n",
      "\n",
      "Q: Can you tell me a good story?\n",
      "A: \"now you tell me a story.\"\n",
      "\n",
      "Q: Will our planet be destroyed?\n",
      "A: it was destroyed!\"\n",
      "\n",
      "Q: What means being alive?\n",
      "A: we are still alive, and we are about to lose our lives.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Do you know who is Arthur Dent?\",\n",
    "    \"Can you tell me a good story?\",\n",
    "    \"Will our planet be destroyed?\",\n",
    "    \"What means being alive?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = generate_response(question.lower(), corpus)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver las respuestas guardan cierto sentido con la pregunta realizada pero son cortas y no aportan mucha información. Esto posiblemente se deba a que el texto contiene muchas sentencias cortas que a la hora de compararse utilizando la similitud del coseno tienen mucha relación con la pregunta hecha y se eligen como respuesta.\n",
    "\n",
    "Tambien hay que destacar que el chatbot no es capaz de generar texto, simplemente usa las sentencias en el corpus con más similitud, por eso en algunos casos las respuestas pueden no ser del todo precisas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
