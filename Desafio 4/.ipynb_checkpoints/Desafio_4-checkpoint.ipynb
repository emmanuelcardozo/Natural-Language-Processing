{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3yeJGnCYxuF"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"right\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "\n",
    "\n",
    "## TP4\n",
    "\n",
    "### Alumno: Emmanuel Cardozo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Utilizar otro dataset y poner en práctica la predicción de próxima palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv5PEwGzZA9-"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto utilizando la layer Embedding de Keras. Se utilizará esos embeddings junto con layers LSTM para predeccir la próxima posible palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y-QdFbHZYj7C"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTvXlEKQZdqx"
   },
   "source": [
    "### Datos\n",
    "Utilizaremos como dataset canciones de bandas de habla inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IkdPfrQJZdB5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset ya se encuentra descargado\n"
     ]
    }
   ],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "import os\n",
    "import platform\n",
    "if os.access('./Ficciones', os.F_OK) is False:\n",
    "    if os.access('Ficciones.zip', os.F_OK) is False:\n",
    "        if platform.system() == 'Windows':\n",
    "            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n",
    "        else:\n",
    "            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
    "    !unzip -q songs_dataset.zip   \n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6j-3nQ4lZjfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El acercamiento de Almotasim.txt',\n",
       " 'El fin.txt',\n",
       " 'El jardin de los senderos que se bifurcan.txt',\n",
       " 'El milagro secreto.txt',\n",
       " 'El Sur.txt',\n",
       " 'Examen de la obra de Herbert Quain.txt',\n",
       " 'Funes el memorioso.txt',\n",
       " 'La Biblioteca de Babel.txt',\n",
       " 'La forma de la espada.txt',\n",
       " 'La lotería en Babilonia.txt',\n",
       " 'La muerte y la brujula.txt',\n",
       " 'La secta del Fenix.txt',\n",
       " 'Las ruinas circulares.txt',\n",
       " 'Pierre Menard, autor del Quijote.txt',\n",
       " 'Tema del traidor y del héroe.txt',\n",
       " 'Tlon, Uqbar, Orbis Tertius.txt',\n",
       " 'Tres versiones de Judas.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posibles bandas\n",
    "os.listdir(\"./Ficciones/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Gb39v3PaZmRH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmanuel\\AppData\\Local\\Temp\\ipykernel_31360\\4212807855.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('Ficciones/Tlon, Uqbar, Orbis Tertius.txt', sep='/n', header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tlén, Uqbar, Orbis Tertius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Debo a la conjuncién de un espejo y de una enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>espejo inquietaba el fondo de un corredor en u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mejia; la enciclopedia falazmente se llama The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1917) y es una reimpresion literal, pero tambi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                         Tlén, Uqbar, Orbis Tertius\n",
       "1  Debo a la conjuncién de un espejo y de una enc...\n",
       "2  espejo inquietaba el fondo de un corredor en u...\n",
       "3  Mejia; la enciclopedia falazmente se llama The...\n",
       "4  1917) y es una reimpresion literal, pero tambi..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
    "df = pd.read_csv('Ficciones/Tlon, Uqbar, Orbis Tertius.txt', sep='/n', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "riT898QlZnmF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 434\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDoouHp7Zp6D"
   },
   "source": [
    "### 1 - Ejemplo de Preprocesamiento\n",
    "- Hay que transformar las oraciones en tokens.\n",
    "- Dichas oraciones hay que ajustarlas al tamaño fijo de nuestra sentencia de entrada al modelo.\n",
    "- Hay que separar las palabras objetivos (target) que el modelo debe predecir en cada sentencia armada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "m5FeTaGvbDbw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence # equivalente a word_teokenize de nltk\n",
    "from keras_preprocessing.sequence import pad_sequences # se utilizará para padding\n",
    "\n",
    "# largo de la secuencia, incluye seq input + word output\n",
    "train_len = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Zf3O7eK6ZpP8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tlén, Uqbar, Orbis Tertius'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de como transformar una oración a tokens usando keras\n",
    "text = df.loc[0,0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "AOv67Sj7aeFH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tlén', 'uqbar', 'orbis', 'tertius']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text_to_word_sequence(text) # entran oraciones -> salen vectores de N posiciones (tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrlyqkoiaymK"
   },
   "source": [
    "1.1 - Transformar las oraciones en secuencias (tokens) de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "XH_L14Wjaowe"
   },
   "outputs": [],
   "source": [
    "# Recorrer todas las filas y transformar las oraciones\n",
    "# en secuencias de palabras\n",
    "sentence_tokens = []\n",
    "for _, row in df[:None].iterrows():\n",
    "    sentence_tokens.append(text_to_word_sequence(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KASzU4CdaxbZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tlén', 'uqbar', 'orbis', 'tertius'],\n",
       " ['debo',\n",
       "  'a',\n",
       "  'la',\n",
       "  'conjuncién',\n",
       "  'de',\n",
       "  'un',\n",
       "  'espejo',\n",
       "  'y',\n",
       "  'de',\n",
       "  'una',\n",
       "  'enciclopedia',\n",
       "  'el',\n",
       "  'descubrimiento',\n",
       "  'de',\n",
       "  'uqbar',\n",
       "  'el']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo\n",
    "sentence_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "A659lswTbIIB"
   },
   "outputs": [],
   "source": [
    "# Código para hacer el desfazaje de las palabras\n",
    "# según el train_len\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "  seq = tokens[i-train_len:i]\n",
    "  text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "01JEoPPnbgRF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo a nuestros vectores para entrenar el modelo\n",
    "text_sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B0gHnKVa4W_"
   },
   "source": [
    "1.2 - Crear los vectores de palabras (word2vec)\n",
    "\n",
    "Ahora necesitamos pasarlos a números para que lo entienda la red y separar input de output.\n",
    "- El Input seran integers (word2vec)\n",
    "- Mientras que el output será one hot encodeado (labels) del tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "fkPNvXeQcS0U"
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer() \n",
    "\n",
    "# El tokeinzer \"aprende\" las palabras que se usaran\n",
    "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
    "tok.fit_on_texts(text_sequences) \n",
    "\n",
    "# Convertimos las palabras a números\n",
    "# entran palabras -> salen números\n",
    "sequences = tok.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4SIc44IocyQb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora sequences tiene los números \"ID\", largo 4\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "3ro81yCQc1oX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de casos (doc) de entrada\n",
    "print(tok.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "nzAWNfroc4u1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de veces que aparece cada palabra\n",
    "print(len(tok.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "spTBxmFQc6h8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# El índice para cada palabra\n",
    "# El sistema las ordena de las más populares a las menos populares\n",
    "print(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "nUDkjy80c77h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de veces quea aparece cada palabra en cada \"documento\"\n",
    "# (1 documento = 1 caso de entrada)\n",
    "print(tok.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohS5Tao1d2KB"
   },
   "source": [
    "### 2 - Preprocesamiento completo\n",
    "Debemos realizar los mismos pasos que en el ejemplo anterior, pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "63Z2-Se2t27r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Tlén, Uqbar, Orbis Tertius\n",
       "1     Debo a la conjuncién de un espejo y de una enc...\n",
       "2     espejo inquietaba el fondo de un corredor en u...\n",
       "3     Mejia; la enciclopedia falazmente se llama The...\n",
       "4     1917) y es una reimpresion literal, pero tambi...\n",
       "5     de 1902. El hecho se produjo hard unos cinco a...\n",
       "6     esa noche y nos demor6 una vasta polémica sobr...\n",
       "7     persona, cuyo narrador omitiera o desfigurara ...\n",
       "8     contradicciones, que permitieran a unos pocos ...\n",
       "9     adivinacién de una realidad atroz o banal. Des...\n",
       "10    nos acechaba. Descubrimos (en la alta noche es...\n",
       "11    espejos tienen algo monstruoso. Entonces Bioy ...\n",
       "12    heresiarcas de Uqbar habia declarado que los e...\n",
       "13    porque multiplican el numero de los hombres. L...\n",
       "14    sentencia y me contest6 que The Anglo American...\n",
       "15    articulo sobre Uqbar. La quinta (que habiamos ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vistazo a las primeras filas\n",
    "df.loc[:15,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kILsSoxTuHEr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tlén, Uqbar, Orbis Tertius Debo a la conjuncién de un espejo y de una enciclopedia el descubrimiento de Uqbar. El espejo inquietaba el fondo de un corredor en una quinta de la calle Gaona, en Ramos Mejia; la enciclopedia falazmente se llama The Anglo American Cyclopaedia (Nueva York, 1917) y es una reimpresion literal, pero también morosa, de la Encyclopaedia Britannica de 1902. El hecho se produjo hard unos cinco afios. Bioy Casares habia cenado conmigo esa noche y nos demor6 una vasta polémica sobre la ejecucién de una novela en primera persona, cuyo narrador omitiera o desfigurara los hechos e incurriera en diversas contradicciones, que permitieran a unos pocos lectores -a muy pocos lectores- la adivinacién de una realidad atroz o banal. Desde el fondo remoto del corredor, el espejo nos acechaba. Descubrimos (en la alta noche ese descubrimiento es inevitable) que los espejos tienen algo monstruoso. Entonces Bioy Casares record6é que uno de los heresiarcas de Uqbar habia declarado que los espejos y la cépula son abominables, porque multiplican el numero de los hombres. Le pregunté el origen de esa memorable sentencia y me contest6 que The Anglo American Cyclopaedia la registraba, en su articulo sobre Uqbar. La quinta (que habiamos alquilado amueblada) poseia un ejemplar de esa obra. En las ultimas paginas del volumen XLVI dimos con un articulo sobre Upsala; en las primeras del XLVII, con uno sobre Uralt-Altaic Languages, pero ni una palabra sobre Uqbar. Bioy, un poco azorado, interrogé los tomos del indice. Agot6 en vano todas las lecciones imaginables: Ukbar, Ucbar, Ooqbar, Ookbar, Oukbahr... Antes de irse, me dijo que era una regién del Irak o del Asia Menor. Confieso que asenti con alguna incomodidad. Conjeturé que ese pais indocumentado y ese heresiarca anénimo eran una ficci6n improvisada por la modestia de Bioy para justificar una frase. El examen estéril de uno de los atlas de Justus Perthes fortaleciéd mi duda. Al dia siguiente, Bioy me llam6é desde Buenos Aires. Me dijo que tenia a la vista el articulo sobre Uqbar, en el volumen XLVI de la Enciclopedia. No constaba el nombre del heresiarca, pero si la noticia de su doctrina, formulada en palabras casi idénticas a las repetidas por él, aunque -tal vez- literariamente inferiores. El habia recordado: Copulation and mirrors are abominable. El texto de la Enciclopedia decia: «Para uno de esos gnosticos, el visible universo era una ilusi6n o (mas precisamente) un sofisma. Los espejos y la paternidad son abominables (mirrors and fatherhood are abominable) porque lo multiplican y lo divulgan». Le dije, sin faltar a la verdad, que me gustaria ver ese articulo. A los pocos dias lo trajo. Lo cual me sorprendiéd, porque los escrupulosos indices cartograficos de la Erdkunde de Ritter ignoraban con plenitud el nombre de Uqbar. El volumen que trajo Bioy era efectivamente el XLVI de la Anglo-American Cyclopaedia. En la falsa caradtula y en el lomo, la indicacién alfabética (Tor-Ups) era la de nuestro ejemplar, pero en vez de 917 paginas constaba de 921. Esas cuatro paginas adicionales comprendian el articulo sobre Uqbar; no previsto (como habra advertido el Ficciones Jorge Luis Borges lector) por la indicacién alfabética. Comprobamos después que no hay otra diferencia entre los volumenes. Los dos (seguin creo haber indicado) son reimpresiones de la décima Encyclopaedia Britannica. Bioy habia adquirido su ejemplar en uno de tantos remates. Leimos con algun cuidado el articulo. El pasaje recordado por Bioy era tal vez el Unico sorprendente. El resto parecia muy verosimil, muy ajustado al tono general de la obra y (como es natural) un poco aburrido. Releyéndolo, descubrimos bajo su rigurosa escritura una fundamental vaguedad. De los catorce nombres que figuraban en la parte geografica, sdlo reconocimos tres -Jorasan, Armenia, Erzerum-, interpolados en el texto de un modo ambiguo. De los nombres histéricos, uno solo: el impostor Esmerdis el mago, invocado mas bien como una metdfora. La nota parecia precisar las fronteras de Uqbar, pero sus nebulosos puntos de referencia eran rios y crateres y cadenas de esa misma region. Leimos, verbigracia, que las tierras bajas de Tsai Jaldun y el delta del Axa definen la frontera del sur y que en las islas de ese delta procrean los caballos salvajes. Eso, al principio de la pagina 918. En la seccién histérica (pagina 920) supimos que a raiz de las persecuciones religiosas del siglo XIII, los ortodoxos buscaron amparo en las islas, donde perduran todavia sus obeliscos y donde no es raro exhumar sus espejos de piedra. La seccién «Idioma y literatura» era breve. Un solo rasgo memorable: anotaba que la literatura de Uqbar era de cardcter fantastico y que sus epopeyas y sus leyendas no se referian jamas a la realidad, sino a las dos regiones imaginarias de Mlejnas y de Tl6én... La bibliografia enumeraba cuatro volimenes que no hemos encontrado hasta ahora, aunque el tercero -Silas Haslam: Hystory of the Land Called Uqbar, 1874- figura en los catalogos de libreria de Bernard Quaritch’. El primero, Lesbare und lesenswerthe Bemerkungen tiber das Land Ukkbar in Klein-Asien, data de 1641 y es obra de Johannes Valentinus Andrea. El hecho es significativo; un par de afos después, di con ese nombre en las inesperadas paginas de De Quincey (Writings, decimotercer volumen) y supe que era el de un tedlogo aleman que a principios del siglo XVII describié la imaginaria comunidad de la Rosa-Cruz -que otros luego fundaron, a imitacién de lo prefigurado por él. Esta noche visitamos la Biblioteca Nacional. En vano fatigamos atlas, catdalogos, anuarios de sociedades geograficas, memorias de viajeros e historiadores: nadie habia estado nunca en Uqbar. El indice general de la enciclopedia de Bioy tampoco registraba ese nombre. Al dia siguiente, Carlos Mastronardi (a quien yo habia referido el asunto) advirtid en una libreria de Corrientes y Talcahuano los negros y dorados lomos de la Anglo American Cyclopaedia... Entré e interrogé el volumen XLVI. Naturalmente, no dio con el menor indicio de Uqbar. IT Algun recuerdo limitado y menguante de Herbert Ashe, ingeniero de los ferrocarriles del Sur, persiste en el hotel de Adrogué, entre las efusivas madreselvas y en el fondo ilusorio de los espejos. En vida padecié de irrealidad, como tantos ingleses; muerto, no es siquiera el fantasma que ya era entonces. Era alto y desganado y su cansada barba rectangular habia sido roja. Entiendo que era viudo, sin hijos. Cada tantos afios iba a Inglaterra: a visitar (juzgo por unas fotografias que nos mostré) un reloj de sol y unos robles. Mi padre habia estrechado con él (el verbo es excesivo) una de esas amistades inglesas que empiezan por excluir la confidencia y que muy pronto omiten el didlogo. Solian ejercer un intercambio de libros y de periddicos; solian batirse al ajedrez, 1 Haslam ha publicado también A General History of Labyrinths. Ficciones Jorge Luis Borges taciturnamente... Lo recuerdo en el corredor del hotel, con un libro de matematicas en la mano, mirando a veces los colores irrecuperables del cielo. Una tarde, hablamos del sistema duodecimal de numeracién (en el que doce se escribe 10). Ashe dijo que precisamente estaba trasladando no sé qué tablas duodecimales a sexagesimales (en las que sesenta se escribe 10). Agregé que ese trabajo le habia sido encargado por un noruego: en Rio Grande do Sul. Ocho afios que lo conociamos y no habia mencionado nunca su estadia en esa regién... Hablamos de vida pastoril, de capangas, de la etimologia brasilera de la palabra gaucho (que algunos viejos orientales todavia pronuncian gaucho) y nada mas se dijo -Dios me perdone- de funciones duodecimales. En septiembre de 1937 (no estabamos nosotros en el hotel) Herbert Ashe murié de la rotura de un aneurisma. Dias antes, habia recibido del Brasil un paquete sellado y certificado. Era un libro en octavo mayor. Ashe lo dejé en el bar, donde -meses después- lo encontré. Me puse a hojearlo y senti un vértigo asombrado y ligero que no describiré, porque ésta no es la historia de mis emociones sino de Uqbar y Tlén y Orbis Tertius. En una noche del Islam que se llama la Noche de las Noches se abren de par en par las secretas puertas del cielo y es mas dulce el agua en los cantaros; si esas puertas se abrieran, no sentiria lo que en esa tarde senti. El libro estaba redactado en inglés y lo integraban 1001 paginas. En el amarillo lomo de cuero lei estas curiosas palabras que la falsa cardtula repetia: A First Encyclopaedia of Tlén. Vol XI. Hlaer to jangr. No habia indicaciédn de fecha ni de lugar. En la primera pagina y en una hoja de papel de seda que cubria una de las laminas en colores habia estampado un 6valo azul con esta inscripcién: Orbis Tertius. Hacia dos afios que yo habia descubierto en un tomo de cierta enciclopedia pirdtica una somera descripcién de un falso pais; ahora me deparaba el azar algo mas precioso y mas arduo. Ahora tenia en las manos un vasto fragmento metéddico de la historia total de un planeta desconocido, con sus arquitecturas y sus barajas, con el pavor de sus mitologias y el rumor de sus lenguas, con sus emperadores y sus mares, con sus minerales y sus pajaros y sus peces, con su algebra y su fuego, con su controversia teolédgica y metafisica. Todo ello articulado, coherente, sin visible propdsito doctrinal o tono parddico. En el onceno tomo de que hablo hay alusiones a tomos ulteriores y precedentes. Néstor Ibarra, en un articulo ya clasico de la NRF, ha negado que existen esos aldteres; Ezequiel Martinez Estrada y Drieu la Rochelle han refutado, quiza victoriosamente, esa duda. El hecho es que hasta ahora las pesquisas mas diligentes han sido estériles. En vano hemos desordenado las bibliotecas de las dos Américas y de Europa. Alfonso Reyes, harto de esas fatigas subalternas de indole policial, propone que entre todos acometamos la obra de reconstruir los muchos y macizos tomos que faltan: ex ungue leonem. Calcula, entre veras y burlas, que una generacién de tlénistas puede bastar. Ese arriesgado cOmputo nos retrae al problema fundamental: éQuiénes inventaron a Tl6n? El plural es inevitable, porque la hipdétesis de un solo inventor -de un infinito Leibniz obrando en la tiniebla y en la modestia- ha sido descartada undanimemente. Se conjetura que este brave new world es obra de una sociedad secreta de astrénomos, de bidlogos, de ingenieros, de metafisicos, de poetas, de quimicos, de algebristas, de moralistas, de pintores, de geémetras... dirigidos por un oscuro hombre de genio. Abundan individuos que dominan esas disciplinas diversas, pero no los capaces de invenci6n y menos los capaces de subordinar la invencién a un riguroso plan sistematico. Ese plan es tan vasto que la contribucién de cada escritor es infinitesimal. Al principio se creyéd que Tlén era un mero caos, una irresponsable licencia de la imaginacién; ahora se sabe que es un cosmos y las intimas leyes que lo rigen han sido formuladas, siquiera en modo provisional. Basteme recordar que las contradicciones aparentes del onceno tomo son la piedra fundamental de la prueba de que existen los otros: tan Iucido y tan justo es el orden que se ha observado Ficciones Jorge Luis Borges en él. Las revistas populares han divulgado, con perdonable exceso la zoologia y la topografia de Tlén; yo pienso que sus tigres transparentes y sus torres de sangre no merecen, tal vez, la continua atenciédn de todos los hombres. Yo me atrevo a pedir unos minutos para su concepto del universo. Hume notoé para siempre que los argumentos de Berkeley no admitian la menor réplica y no causaban la menor conviccién. Ese dictamen es del todo veridico en su aplicacién a la tierra; del todo falso en Tlén. Las naciones de ese planeta son -congénitamente- idealistas. Su lenguaje y las derivaciones de su lenguaje -la religién, las letras, la metafisica- presuponen el idealismo. El mundo para ellos no es un concurso de objetos en el espacio; es una serie heterogénea de actos independientes. Es sucesivo, temporal, no espacial. No hay sustantivos en la conjetural Ursprache de Tlén, de la que proceden los idiomas «actuales» y los dialectos: hay verbos impersonales, calificados por sufijos (o prefijos) monosilabicos de valor adverbial. Por ejemplo: no hay palabra que corresponda a la palabra luna, pero hay un verbo que seria en espafiol kunecer o tunar. «Surgié la luna sobre el rio» se dice <hlér u fang axaxaxas mlé» 0 sea en su orden: «hacia arriba (upward) detras duradero-fluir lunecié». (Xul Solar traduce con brevedad: «upa tras perfluyue luné». «Upward, behind the onstreaming, it mooned.) Lo anterior se refiere a los idiomas del hemisferio austral. En los del hemisferio boreal (de cuya Ursprache hay muy pocos datos en el onceno tomo) la célula primordial no es el verbo, sino el adjetivo monosilabico. El sustantivo se forma por acumulacién de adjetivos. No se dice luna: se dice aéreo-claro sobre oscuro-redondo o anaranjado-tenue-del cielo o cualquier otra agregacién. En el caso elegido la masa de adjetivos corresponde a un objeto real; el hecho es puramente fortuito. En la literatura de este hemisferio (como en el mundo subsistente de Meinong) abundan los objetos ideales, convocados y disueltos en un momento, segun las necesidades poéticas. Los determina, a veces, la mera simultaneidad. Hay objetos compuestos de dos términos, uno de cardacter visual y otro auditivo: el color del naciente y el remoto grito de un pajaro. Los hay de muchos: el sol y el agua contra el pecho del nadador, el vago rosa trémulo que se ve con los ojos cerrados, la sensacién de quien se deja llevar por un rio y también por el suefio. Esos objetos de segundo grado pueden combinarse con otros; el proceso, mediante ciertas abreviaturas, es practicamente infinito. Hay poemas famosos compuestos de una sola enorme palabra. Esta palabra integra un objeto poético creado por el autor. El hecho de que nadie crea en la realidad de los sustantivos hace, paraddéjicamente, que sea interminable su numero. Los idiomas del hemisferio boreal de Tl6n poseen todos los nombres de las lenguas indoeuropeas y otros muchos mas. No es exagerado afirmar que la cultura clasica de Tl6n comprende una sola disciplina: la psicologia. Las otras estan subordinadas a ella. He dicho que los hombres de ese planeta conciben el universo como una serie de procesos mentales, que no se desenvuelven en el espacio sino de modo sucesivo en el tiempo. Spinoza atribuye a su inagotable divinidad los atributos de la extensién y del pensamiento; nadie comprenderia en Tlon la yuxtaposicién del primero (que sdélo es tipico de ciertos estados) y del segundo -que es un sinénimo perfecto del cosmos-, Dicho sea con otras palabras: no conciben que lo espacial perdure en el tiempo. La percepci6n de una humareda en el horizonte y después del campo incendiado y después del cigarro a medio apagar que produjo la quemazon es considerada un ejemplo de asociacién de ideas. Este monismo o idealismo total invalida la ciencia. Explicar (0 juzgar) un hecho es unirlo a otro; esa vinculacién, en Tlén, es un estado posterior del sujeto, que no puede afectar o iluminar el estado anterior. Todo estado mental es irreductible: el mero hecho 10 Ficciones Jorge Luis Borges de nombrarlo -id est, de clasificarlo- importa un falseo. De ello cabria deducir que no hay ciencias en Tl6n -ni siquiera razonamientos. La paraddjica verdad es que existen, en casi innumerable numero. Con las filosofias acontece lo que acontece con los sustantivos en el hemisferio boreal. El hecho de que toda filosofia sea de antemano un juego dialéctico, una Philosophie des Als Ob, ha contribuido a multiplicarlas. Abundan los sistemas increibles, pero de arquitectura agradable o de tipo sensacional. Los metafisicos de Tlén no buscan la verdad ni siquiera la verosimilitud: buscan el asombro. Juzgan que la metafisica es una rama de la literatura fantastica. Saben que un sistema no es otra cosa que la subordinacién de todos los aspectos del universo a uno cualquiera de ellos. Hasta la frase «todos los aspectos» es rechazable, porque supone la imposible -adiciédn del instante presente y de los pretéritos. Tampoco es licito el plural «los pretéritos», porque supone otra operacién imposible... Una de las escuelas de Tlén llega a negar el tiempo: razona que el presente es indefinido, que el futuro no tiene realidad sino como esperanza presente, que el pasado no tiene realidad sino como recuerdo presente.’ Otra escuela declara que ha transcurrido ya todo el tiempo y que nuestra vida es apenas el recuerdo o reflejo crepuscular, y sin duda falseado y mutilado, de un proceso irrecuperable. Otra, que la historia del universo -y en ellas nuestras vidas y el mas tenue detalle de nuestras vidas- es la escritura que produce un dios subalterno para entenderse con un demonio. Otra, que el universo es comparable a esas criptografias en las que no valen todos los simbolos y que sdlo es verdad lo que sucede cada trescientas noches. Otra, que mientras dormimos aqui, estamos despiertos en otro lado y que asi cada hombre es dos hombres. Entre las doctrinas de Tlén, ninguna ha merecido tanto escandalo como el materialismo. Algunos pensadores lo han formulado, con menos claridad que fervor, como quien adelanta una paradoja. Para facilitar el entendimiento de esa tesis inconcebible, un heresiarca del undécimo siglo* ideé el sofisma de las nueve monedas de cobre, cuyo renombre escandaloso equivale en Tlén. al de las aporias eledaticas. De ese «razonamiento especioso» hay muchas versiones, que varian el numero de monedas y el numero de hallazgos; he aqui la mas comun: El martes, X atraviesa un camino desierto y pierde nueve monedas de cobre. El jueves, Y encuentra en el camino cuatro monedas, algo herrumbradas por la Uuvia del miércoles. El viernes, Z descubre tres monedas en el camino. El viernes de mafiana, X encuentra dos monedas en el corredor de su casa. El heresiarca queria deducir de esa historia la realidad -id est la continuidad- de las nueve monedas recuperadas. Es absurdo (afirmaba) imaginar que cuatro de las monedas no han existido entre el martes y el jueves, tres entre el martes y la tarde del viernes, dos entre el martes y la madrugada del viernes. Es Idgico pensar que han existido -siquiera de algtin modo secreto, de comprensié6n vedada a los hombres- en todos los momentos de esos tres plazos. El lenguaje de Tlon se resistia a formular esa paradoja; los mas no la entendieron. Los defensores del sentido comun se limitaron, al principio, a negar la veracidad de la anécdota. Repitieron que era una falacia verbal, basada en el empleo temerario de dos voces neoldégicas, no autorizadas por el uso y ajenas a todo pensamiento severo: los 1 Russell (The Analysfs of Mind, 1921, pagina 159) supone que el planeta ha sido creado hace pocos minutos, provisto de una humanidad que «recuerda» un pasado ilusorio. 2a : . aoe . . x Siglo, de acuerdo con el sistema duodecimal, significa un periodo de ciento cuarenta y cuatro afios. 11 Ficciones Jorge Luis Borges verbos encontrar y perder, que comportaban una peticién de principio, porque presuponian la identidad de las nueve primeras monedas y de las ultimas. Recordaron que todo sustantivo (hombre, moneda, jueves, miércoles, lluvia) sdlo tiene un valor metaférico. Denunciaron la pérfida circunstancia algo herrumbradas por la Wuvia del miércoles, que presupone lo que se trata de demostrar: la persistencia de las cuatro monedas, entre el jueves y el martes. Explicaron que una cosa es igualdad y otra identidad y formularon una especie de reductio ad absurdum, o sea el caso hipotético de nueve hombres que en nueve sucesivas noches padecen un vivo dolor. éNo seria ridiculo -interrogaron- pretender que ese dolor es el mismo?’ Dijeron que al heresiarca no lo movia sino el blasfematorio proposito de atribuir la divina categoria de sera unas simples monedas y que a veces negaba la pluralidad y otras no. Argumentaron: si la igualdad comporta la identidad, habria que admitir asimismo que las nueve monedas son una sola. Increiblemente, esas refutaciones no resultaron definitivas. A los cien afios de enunciado el problema, un pensador no menos brillante que el heresiarca pero de tradici6n ortodoxa, formulé una hipdtesis muy audaz. Esa conjetura feliz afirma que hay un solo sujeto, que ese sujeto indivisible es cada uno de los seres del universo y que éstos son los érganos y mascaras de la divinidad. X es Y y es Z. * descubre tres monedas porque recuerda que se le perdieron a X; X encuentra dos en el corredor porque recuerda que han sido recuperadas las otras... El onceno tomo deja entender que tres razones capitales determinaron la victoria total de ese panteismo idealista. La primera, el repudio del solipsismo; la segunda, la posibilidad de conservar la base psicolégica de las ciencias; la tercera, la posibilidad de conservar el culto de los dioses. Schopenhauer (el apasionado y lucido Schopenhauer) formula una doctrina muy parecida en el primer volumen de Parerga und Paralipomena. La geometria de Tl6n comprende dos disciplinas algo distintas: la visual y la tactil. La ultima corresponde a la nuestra y la subordinan a la primera. La base de la geometria visual es la superficie, no el punto. Esta geometria desconoce las paralelas y declara que el hombre que se desplaza modifica las formas que lo circundan. La base de su aritmética es la nocién de numeros indefinidos. Acentuan la importancia de los conceptos de mayor y menor, que nuestros matematicos simbolizan por > y por <. Afirman que la operacién de contar modifica las cantidades y las convierte de indefinidas en definidas. El hecho de que varios individuos que cuentan una misma cantidad logren un resultado igual, es para los psicdlogos un ejemplo de asociacién de ideas o de buen ejercicio de la memoria. Ya sabemos que en Tl6n el sujeto del conocimiento es uno y eterno. En los habitos literarios también es todopoderosa la idea de un sujeto unico. Es raro que los libros estén firmados. No existe el concepto del plagio: se ha establecido que todas las obras son obra de un solo autor, que es intemporal y es anénimo. La critica suele inventar autores: elige dos obras disimiles -el Tao Te King y Las mil y una noches, digamos-, las atribuye a un mismo escritor y luego de termina con probidad la psicologia de ese interesante homme de letres... También son distintos los libros. Los de ficcién abarcan un solo argumento, con todas las permutaciones imaginables. Los de naturaleza filoséfica invariablemente contienen la tesis y la antitesis, el riguroso pro y el contra de una doctrina. Un libro que no encierra su contralibro es considerado incompleto. 1 En el dia de hoy, una de las iglesias de Tlén. sostiene platénicamente que tal dolor, que tal matiz verdoso del amarillo, que tal temperatura, que tal sonido, son la Unica realidad. Todos los hombres, en el vertiginoso instante del coito, son el mismo hombre. Todos los hombres que repiten una linea de Shakespeare, son William Shakespeare. 12 Ficciones Jorge Luis Borges Siglos y siglos de idealismo no han dejado de influir en la realidad. No es infrecuente, en las regiones mas antiguas de Tlén, la duplicacién de objetos perdidos. Dos personas buscan un lapiz; la primera lo encuentra y no dice nada; la segunda encuentra un segundo lapiz no menos real, pero mdas-ajustado a su expectativa. Esos objetos secundarios se llaman hrénir y son, aunque de forma desairada, un poco mas largos. Hasta hace poco los hrénir fueron hijos casuales de la distraccién y el olvido. Parece mentira que su metddica produccién cuente apenas cien afios, pero asi lo declara el onceno tomo. Los primeros intentos fueron estériles. El modus operandi, sin embargo, merece recordacién. El director de una de las carceles del estado comunicé a los presos que en el antiguo lecho de un rio habia ciertos sepulcros y prometi6 la libertad a quienes trajeran un hallazgo importante. Durante los meses que precedieron a la excavaci6én les mostraron laminas fotograficas de lo que iban a hallar. Ese primer intento probé que la esperanza y la avidez pueden inhibir; una semana de trabajo con la pala y el pico no logr6é exhumar otro hrén que una rueda herrumbrada, de fecha posterior al experimento. Este se mantuvo secreto y se repitid después en cuatro colegios. En tres fue casi total el fracaso; en el cuarto (cuyo director muri6d casualmente durante las primeras excavaciones) los discipulos exhumaron -o produjeron- una mascara de oro, una espada arcaica, dos o tres anforas de barro y el verdinoso y mutilado torso de un rey con una inscripci6n en el pecho que no se ha logrado aun descifrar. Asi se descubrid la improcedencia de testigos que conocieran la naturaleza experimental de la busca... Las investigaciones en masa producen objetos contradictorios; ahora se prefiere los trabajos individuales y casi improvisados. La metédica elaboracién de hrénir (dice el onceno tomo) ha prestado servicios prodigiosos a los arquedlogos. Ha permitido interrogar y hasta modificar el pasado, que ahora no es menos plastico y menos docil que el porvenir. Hecho curioso: los hrénir de segundo y tercer grado -los hrénir derivados de otro hrén, los hrénir derivados del hrén de un hrén- exageran las aberraciones del inicial; los de quinto son casi uniformes; los de noveno se confunden con los de segundo; en los de undécimo hay una pureza de lineas que los originales no tienen. El proceso es periddico; el hrén de duodécimo grado ya empieza a decaer. Mas extrafio y mas puro que todo hrén es a veces el ur. la cosa producida por sugestién, el objeto educido por la esperanza. La gran mascara de oro que he mencionado es un ilustre ejemplo. Las cosas se duplican en Tl6én; propenden asimismo a borrarse y a perder los detalles cuando los olvida la gente. Es clasico el ejemplo de un umbral que perduréd mientras lo visitaba un mendigo y que se perdiéd de vista a su muerte. A veces unos pajaros, un caballo, han salvado las ruinas de un anfiteatro. 1940, Salto Oriental Posdata de 1947. Reproduzco el articulo anterior tal como aparecié en la Antologia de la literatura fantdstica, 1940, sin otra escisi6n que algunas metaforas y que una especie de resumen burlén que ahora resulta frivolo. Han ocurrido tantas cosas desde esa fecha... Me limitaré a recordarlas. En marzo de 1941 se descubrié una carta manuscrita de Gunnar Erfjord en un libro de Hinton que habia sido de Herbert Ashe. El sobre tenia el sello postal de Ouro Preto; la carta elucidaba enteramente el misterio de Tlén. Su texto corrobora las hipdtesis de Martinez Estrada. A principios del siglo XVII, en una noche de Lucerna o de Londres, empezo la espléndida historia. Una sociedad secreta y benévola (que entre sus afiliados 13 Ficciones Jorge Luis Borges tuvo a Dalgarno y después a George Berkeley) surgiéd para inventar un pais. En el vago programa inicial figuraban los «estudios herméticos», la filantropia y la cabala. De esa primera época data el curioso libro de Andrea. Al cabo de unos afios de concilidbulos y de sintesis prematuras comprendieron que una generacién no bastaba para articular un pais. Resolvieron que cada uno de los maestros que la integraban eligiera un discipulo para la continuacién de la obra. Esa disposicién hereditaria prevalecié; después de un hiato de dos siglos la perseguida fraternidad resurge en América. Hacia 1824, en Memphis (Tennessee) uno de los afiliados conversa con el ascético millonario Ezra Buckley. Este lo deja hablar con algun desdén -y se rie de la modestia del proyecto-. Le dice que en América es absurdo inventar un pais y le propone la invenciédn de un planeta. A esa gigantesca idea afiade otra, hija de su nihilismo:* la de guardar en el silencio la empresa enorme. Circulaban entonces los veinte tomos de la Encyclopaedia Britannica; Buckley sugiere una enciclopedia metddica del planeta ilusorio. Les dejara sus cordilleras auriferas, sus rios navegables, sus praderas holladas por el toro y por el bisonte, sus negros, sus prostibulos y sus ddlares, bajo una condici6én: «La obra no pactarad con el impostor Jesucristo». Buckley descree de Dios, pero quiere demostrar al Dios no existente que los hombres mortales son capaces de concebir un mundo. Buckley es envenenado en Baton Rouge en 1828; en 1914 la sociedad remite a sus colaboradores, que son trescientos, el volumen final de la Primera Enciclopedia de Tlén. La ediciédn es secreta: los cuarenta volumenes que comprende (la obra mas vasta que han acometido los hombres) serian la base de otra mas minuciosa, redactada no ya en inglés, sino en alguna de las lenguas de Tlén. Esa revisi6n de un mundo ilusorio se llama provisoriamente Orbis Tertius y uno de sus modestos demiurgos fue Herbert Ashe, no sé si como agente de Gunnar Erfjord o como afiliado. Su recepciédn de un ejemplar del onceno tomo parece favorecer lo segundo. Pero ¢y los otros? Hacia 1942 arreciaron los hechos. Recuerdo con singular nitidez uno de los primeros y me parece que algo senti de su cardcter premonitorio. Ocurrid en un departamento de la calle Laprida, frente a un claro y alto balcén que miraba el ocaso. La princesa de Faucigny Lucinge habia recibido de Poitiers su vajilla de plata. Del vasto fondo de un cajén rubricado de sellos internacionales iban saliendo finas cosas inméviles: plateria de Utrecht y de Paris con dura fauna heraldica, un samovar. Entre ellas -con un perceptible y tenue temblor de pajaro dormido- latia misteriosamente una brujula. La princesa no la reconocié. La aguja azul anhelaba el norte magnético; la caja de metal era céncava; las letras de la esfera correspondian a uno de los alfabetos de Tlén. Tal fue la primera intrusi6n del mundo fantastico en el mundo real. Un azar que me inquieta hizo que yo también fuera testigo de la segunda. Ocurrid unos meses después, en la pulperia de un brasilero, en la Cuchilla Negra. Amorim y yo regresdbamos de Sant'Anna. Una creciente del rio Tacuarembé nos obligd a probar (y a sobrellevar) esa rudimentaria hospitalidad. El pulpero nos acomoddé unos catres crujientes en una pieza grande, entorpecida de barriles y cueros. Nos acostamos, pero no nos dejé dormir hasta el alba la borrachera de un vecino invisible, que alternaba denuestos inextricables con rachas de milongas -mas bien con rachas de una sola milonga-. Como es de suponer, atribuimos a la fogosa cafia del patrén ese griterio insistente... A la madrugada, el hombre estaba muerto en el corredor. La aspereza de la voz nos habia engafiado: era un muchacho joven. En el delirio se le habian caido del tirador unas cuantas monedas y un cono de metal reluciente, del didmetro de un dado. En vano un chico trat6 de recoger ese cono. Un hombre apenas acerté a levantarlo. Yo lo tuve en la palma de la mano algunos minutos: recuerdo que su peso era intolerable y que después de retirado el cono, la 1 Buckley era librepensador, fatalista y defensor de la esclavitud. 14 Ficciones Jorge Luis Borges opresién perduré. También recuerdo el circulo preciso que me grabo en la carne. Esa evidencia de un objeto muy chico y a la vez pesadisimo dejaba una impresién desagradable de asco y de miedo. Un paisano propuso que lo tiraran al rio correntoso. Amorim lo adquiri6 mediante unos pesos. Nadie sabia nada del muerto, salvo «que venia de la frontera». Esos conos pequefios y muy pesados (hechos de un metal que no es de este mundo) son imagen de la divinidad, en ciertas religiones de Tl6n. Aqui doy término a la parte personal de mi narraciédn. Lo demas esta en la memoria (cuando no en la esperanza o en el temor) de todos mis lectores. Basteme recordar o mencionar los hechos subsiguientes, con una mera brevedad de palabras que el céncavo recuerdo general enriquecera o ampliara. Hacia 1944 un investigador del diario The American (de Nashville, Tennessee) exhumo6 en una biblioteca de Memphis los cuarenta volumenes de la Primera Enciclopedia de Tidn. Hasta el dia de hoy se discute si ese descubrimiento fue casual o si lo consintieron los directores del todavia nebuloso Orbis Tertius. Es verosimil lo segundo. Algunos rasgos increibles del onceno tomo (verbigracia, la multiplicacién de los hrénir) han sido eliminados o atenuados en el ejemplar de Memphis; es razonable imaginar que esas tachaduras obedecen al plan de exhibir un mundo que no sea demasiado incompatible con el mundo real. La diseminacién de objetos de Tlén en diversos paises complementaria ese plan...‘ El hecho es que la prensa internacional vocedé infinitamente el «hallazgo». Manuales, antologias, restimenes, versiones literales, reimpresiones autorizadas y reimpresiones pirdticas de la Obra Mayor de los Hombres abarrotaron y siguen abarrotando la tierra. Casi inmediatamente, la realidad cedi6 en mas de un punto. Lo cierto es que anhelaba ceder. Hace diez afios bastaba cualquier simetria con apariencia de orden -el materialismo dialéctico, el antisemitismo, el nazismo- para embelesar a los hombres. éCémo no someterse a Tlén, a la minuciosa y vasta evidencia de un planeta ordenado? Inutil responder que la realidad también esta ordenada. Quiza lo esté, pero de acuerdo a leyes divinas -traduzco: a leyes inhumanas- que no acabamos nunca de percibir. Tlén sera un laberinto, pero es un laberinto urdido por hombres, un laberinto destinado a que lo descifren los hombres. El contacto y el habito de Tlén han desintegrado este mundo. Encantada por su rigor, la humanidad olvida y torna a olvidar que es un rigor de ajedrecistas, no de angeles. Ya ha penetrado en las escuelas el (conjetural) « idioma primitivo» de Tlén; ya la ensefianza de su historia armoniosa (y llena de episodios conmovedores) ha obliterado a la que presidié mi nifiez; ya en las memorias un pasado ficticio ocupa el sitio de otro, del que nada sabemos con certidumbre -ni siquiera que es falso-. Han sido reformadas la numismatica, la farmacologia y la arqueologia. Entiendo que la biologia y las matemdaticas aguardan también su avatar... Una dispersa dinastia de solitarios ha cambiado la faz del mundo. Su tarea prosigue. Si nuestras previsiones no yerran, de aqui a cien anos alguien descubrira los cien tomos de la Segunda Enciclopedia de Tl6én. Entonces desapareceran del planeta el inglés y el francés y el mero espafiol. El mundo sera Tlén. Yo no hago caso, yo sigo revisando en los quietos dias del hotel de Adrogué una indecisa traduccién quevediana (que no pienso dar a la imprenta) del Um Burial de Browne. 1 Queda, naturalmente, el problema de la materia de algunos objetos. 15 Ficciones\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenamos todos los rows en un solo valor\n",
    "corpus = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=0)[0]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_KlsYd7_uOez"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tlén',\n",
       " 'uqbar',\n",
       " 'orbis',\n",
       " 'tertius',\n",
       " 'debo',\n",
       " 'a',\n",
       " 'la',\n",
       " 'conjuncién',\n",
       " 'de',\n",
       " 'un',\n",
       " 'espejo',\n",
       " 'y',\n",
       " 'de',\n",
       " 'una',\n",
       " 'enciclopedia',\n",
       " 'el',\n",
       " 'descubrimiento',\n",
       " 'de',\n",
       " 'uqbar',\n",
       " 'el']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformar el corpus a tokens\n",
    "tokens=text_to_word_sequence(corpus)\n",
    "# Vistazo general de los primeros tokens\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "GlqpZSJOJ1xQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens en el corpus: 5742\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de tokens en el corpus:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "RhQevOynuYk2"
   },
   "outputs": [],
   "source": [
    "# Código para hacer el desfazaje de las palabras\n",
    "# según el train_len\n",
    "text_sequences = []\n",
    "for i in range(train_len, len(tokens)):\n",
    "  seq = tokens[i-train_len:i]\n",
    "  text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "FU3FuqHSuhzq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tlén', 'uqbar', 'orbis', 'tertius'],\n",
       " ['uqbar', 'orbis', 'tertius', 'debo'],\n",
       " ['orbis', 'tertius', 'debo', 'a'],\n",
       " ['tertius', 'debo', 'a', 'la'],\n",
       " ['debo', 'a', 'la', 'conjuncién'],\n",
       " ['a', 'la', 'conjuncién', 'de'],\n",
       " ['la', 'conjuncién', 'de', 'un'],\n",
       " ['conjuncién', 'de', 'un', 'espejo'],\n",
       " ['de', 'un', 'espejo', 'y'],\n",
       " ['un', 'espejo', 'y', 'de'],\n",
       " ['espejo', 'y', 'de', 'una'],\n",
       " ['y', 'de', 'una', 'enciclopedia'],\n",
       " ['de', 'una', 'enciclopedia', 'el'],\n",
       " ['una', 'enciclopedia', 'el', 'descubrimiento'],\n",
       " ['enciclopedia', 'el', 'descubrimiento', 'de'],\n",
       " ['el', 'descubrimiento', 'de', 'uqbar'],\n",
       " ['descubrimiento', 'de', 'uqbar', 'el'],\n",
       " ['de', 'uqbar', 'el', 'espejo'],\n",
       " ['uqbar', 'el', 'espejo', 'inquietaba'],\n",
       " ['el', 'espejo', 'inquietaba', 'el']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo a nuestros vectores para entrenar el modelo\n",
    "text_sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "064N2jtLvHRg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 40, 137, 113],\n",
       " [40, 137, 113, 558],\n",
       " [137, 113, 558, 9],\n",
       " [113, 558, 9, 2],\n",
       " [558, 9, 2, 559],\n",
       " [9, 2, 559, 1],\n",
       " [2, 559, 1, 7],\n",
       " [559, 1, 7, 187],\n",
       " [1, 7, 187, 5],\n",
       " [7, 187, 5, 1],\n",
       " [187, 5, 1, 13],\n",
       " [5, 1, 13, 46],\n",
       " [1, 13, 46, 3],\n",
       " [13, 46, 3, 188],\n",
       " [46, 3, 188, 1],\n",
       " [3, 188, 1, 40],\n",
       " [188, 1, 40, 3],\n",
       " [1, 40, 3, 187],\n",
       " [40, 3, 187, 560],\n",
       " [3, 187, 560, 3]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceso de tokenizacion\n",
    "tok = Tokenizer() \n",
    "tok.fit_on_texts(text_sequences) \n",
    "\n",
    "# Convertimos las palabras a números\n",
    "# entran palabras -> salen números\n",
    "sequences = tok.texts_to_sequences(text_sequences)\n",
    "\n",
    "# Damos un vistazo\n",
    "sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "vwsvmvDKKXSP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows del dataset: 5738\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de rows del dataset:\", len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMVP4bj0vL2e"
   },
   "source": [
    "### 3 - Input y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "mx2xwdz3KloJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con numpy es muy fácil realizar el slicing de vectores\n",
    "ex = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "BEod7qghvTVt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: (2, 4)\n",
      "Todos los elementos: [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "Todos los elementos menos el último: [[1 2 3]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "# Con numpy es muy fácil realizar el slicing de vectores\n",
    "print(\"Dimension:\", ex.shape)\n",
    "print(\"Todos los elementos:\", ex)\n",
    "print(\"Todos los elementos menos el último:\", ex[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "i95xWqtCvp8T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1 2 3]\n",
      " [5 6 7]]\n",
      "Target: [4 8]\n"
     ]
    }
   ],
   "source": [
    "input = ex[:,:-1] # todos los rows, menos la ultima col\n",
    "target = ex[:, -1] # última col de cada row\n",
    "\n",
    "print(\"Input:\", input)\n",
    "print(\"Target:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "e1vJTG65v4Qn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5738, 3)\n",
      "(5738,)\n"
     ]
    }
   ],
   "source": [
    "arr_sequences = np.array(sequences)\n",
    "x_data = arr_sequences[:,:-1]\n",
    "y_data_int = arr_sequences[:,-1] # aún falta el oneHotEncoder\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "ln6kVWVlwBBs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'de',\n",
       " 2: 'la',\n",
       " 3: 'el',\n",
       " 4: 'que',\n",
       " 5: 'y',\n",
       " 6: 'en',\n",
       " 7: 'un',\n",
       " 8: 'los',\n",
       " 9: 'a',\n",
       " 10: 'es',\n",
       " 11: 'del',\n",
       " 12: 'no',\n",
       " 13: 'una',\n",
       " 14: 'las',\n",
       " 15: 'con',\n",
       " 16: 'se',\n",
       " 17: 'lo',\n",
       " 18: 'su',\n",
       " 19: 'por',\n",
       " 20: 'sus',\n",
       " 21: 'o',\n",
       " 22: 'tlén',\n",
       " 23: 'ese',\n",
       " 24: 'esa',\n",
       " 25: 'mas',\n",
       " 26: 'habia',\n",
       " 27: 'era',\n",
       " 28: 'son',\n",
       " 29: 'pero',\n",
       " 30: 'uno',\n",
       " 31: 'al',\n",
       " 32: 'dos',\n",
       " 33: 'ha',\n",
       " 34: 'han',\n",
       " 35: 'hombres',\n",
       " 36: 'me',\n",
       " 37: 'como',\n",
       " 38: 'hay',\n",
       " 39: 'monedas',\n",
       " 40: 'uqbar',\n",
       " 41: 'otra',\n",
       " 42: 'mundo',\n",
       " 43: 'hecho',\n",
       " 44: 'para',\n",
       " 45: 'entre',\n",
       " 46: 'enciclopedia',\n",
       " 47: 'sobre',\n",
       " 48: 'realidad',\n",
       " 49: 'porque',\n",
       " 50: 'obra',\n",
       " 51: 'después',\n",
       " 52: 'sido',\n",
       " 53: 'también',\n",
       " 54: 'unos',\n",
       " 55: 'afios',\n",
       " 56: 'bioy',\n",
       " 57: 'nos',\n",
       " 58: 'primera',\n",
       " 59: 'muy',\n",
       " 60: 'tal',\n",
       " 61: 'yo',\n",
       " 62: 'ya',\n",
       " 63: 'tomo',\n",
       " 64: 'todos',\n",
       " 65: 'objetos',\n",
       " 66: 'articulo',\n",
       " 67: 'esas',\n",
       " 68: 'ficciones',\n",
       " 69: 'jorge',\n",
       " 70: 'luis',\n",
       " 71: 'borges',\n",
       " 72: 'tres',\n",
       " 73: 'sino',\n",
       " 74: 'ahora',\n",
       " 75: 'recuerdo',\n",
       " 76: 'planeta',\n",
       " 77: 'todo',\n",
       " 78: 'onceno',\n",
       " 79: 'le',\n",
       " 80: 'volumen',\n",
       " 81: 'si',\n",
       " 82: 'universo',\n",
       " 83: 'cuatro',\n",
       " 84: 'hasta',\n",
       " 85: 'tl6n',\n",
       " 86: 'este',\n",
       " 87: 'hombre',\n",
       " 88: 'segundo',\n",
       " 89: 'nueve',\n",
       " 90: 'hrénir',\n",
       " 91: 'corredor',\n",
       " 92: 'the',\n",
       " 93: 'noche',\n",
       " 94: 'algo',\n",
       " 95: 'palabra',\n",
       " 96: 'heresiarca',\n",
       " 97: 'casi',\n",
       " 98: 'esos',\n",
       " 99: 'sin',\n",
       " 100: 'solo',\n",
       " 101: 'esta',\n",
       " 102: 'ashe',\n",
       " 103: 'siquiera',\n",
       " 104: 'cada',\n",
       " 105: 'libro',\n",
       " 106: 'historia',\n",
       " 107: 'menos',\n",
       " 108: 'dice',\n",
       " 109: 'sea',\n",
       " 110: 'otro',\n",
       " 111: 'x',\n",
       " 112: 'hrén',\n",
       " 113: 'tertius',\n",
       " 114: 'american',\n",
       " 115: 'pocos',\n",
       " 116: 'espejos',\n",
       " 117: 'numero',\n",
       " 118: 'ejemplar',\n",
       " 119: 'paginas',\n",
       " 120: 'ni',\n",
       " 121: 'tomos',\n",
       " 122: 'menor',\n",
       " 123: 'pais',\n",
       " 124: 'vez',\n",
       " 125: 'siglo',\n",
       " 126: 'otros',\n",
       " 127: 'estado',\n",
       " 128: '1',\n",
       " 129: 'veces',\n",
       " 130: 'rio',\n",
       " 131: 'ejemplo',\n",
       " 132: 'hemisferio',\n",
       " 133: 'sujeto',\n",
       " 134: 'martes',\n",
       " 135: 'encuentra',\n",
       " 136: 'buckley',\n",
       " 137: 'orbis',\n",
       " 138: 'algunos',\n",
       " 139: 'fondo',\n",
       " 140: 'anglo',\n",
       " 141: 'cyclopaedia',\n",
       " 142: 'encyclopaedia',\n",
       " 143: 'hechos',\n",
       " 144: 'entonces',\n",
       " 145: 'xlvi',\n",
       " 146: 'poco',\n",
       " 147: 'vano',\n",
       " 148: 'dijo',\n",
       " 149: 'mi',\n",
       " 150: 'dia',\n",
       " 151: 'nombre',\n",
       " 152: 'palabras',\n",
       " 153: 'él',\n",
       " 154: 'verdad',\n",
       " 155: 'general',\n",
       " 156: 'modo',\n",
       " 157: 'principio',\n",
       " 158: 'pagina',\n",
       " 159: 'literatura',\n",
       " 160: 'of',\n",
       " 161: 'nadie',\n",
       " 162: 'herbert',\n",
       " 163: 'hotel',\n",
       " 164: 'ilusorio',\n",
       " 165: 'nada',\n",
       " 166: 'dios',\n",
       " 167: 'noches',\n",
       " 168: 'hacia',\n",
       " 169: 'total',\n",
       " 170: 'plan',\n",
       " 171: 'objeto',\n",
       " 172: 'real',\n",
       " 173: 'sola',\n",
       " 174: 'hace',\n",
       " 175: 'otras',\n",
       " 176: 'tiempo',\n",
       " 177: 'presente',\n",
       " 178: 'esperanza',\n",
       " 179: 'pasado',\n",
       " 180: 'aqui',\n",
       " 181: 'jueves',\n",
       " 182: 'viernes',\n",
       " 183: 'cien',\n",
       " 184: 'segunda',\n",
       " 185: 'base',\n",
       " 186: 'fue',\n",
       " 187: 'espejo',\n",
       " 188: 'descubrimiento',\n",
       " 189: 'llama',\n",
       " 190: 'britannica',\n",
       " 191: 'vasta',\n",
       " 192: 'cuyo',\n",
       " 193: 'e',\n",
       " 194: 'lectores',\n",
       " 195: 'desde',\n",
       " 196: 'primeras',\n",
       " 197: 'todas',\n",
       " 198: 'modestia',\n",
       " 199: 'duda',\n",
       " 200: 'tenia',\n",
       " 201: 'doctrina',\n",
       " 202: 'aunque',\n",
       " 203: 'texto',\n",
       " 204: 'dias',\n",
       " 205: 'volumenes',\n",
       " 206: 'reimpresiones',\n",
       " 207: 'tantos',\n",
       " 208: 'algun',\n",
       " 209: 'fundamental',\n",
       " 210: 'nombres',\n",
       " 211: 'sdlo',\n",
       " 212: 'donde',\n",
       " 213: 'todavia',\n",
       " 214: 'tl6én',\n",
       " 215: 'par',\n",
       " 216: 'nunca',\n",
       " 217: 'quien',\n",
       " 218: 'vida',\n",
       " 219: 'muerto',\n",
       " 220: 'unas',\n",
       " 221: 'verbo',\n",
       " 222: 'libros',\n",
       " 223: 'cielo',\n",
       " 224: 'tarde',\n",
       " 225: 'sistema',\n",
       " 226: '10',\n",
       " 227: 'estaba',\n",
       " 228: 'mayor',\n",
       " 229: 'meses',\n",
       " 230: 'senti',\n",
       " 231: 'inglés',\n",
       " 232: 'fecha',\n",
       " 233: 'falso',\n",
       " 234: 'vasto',\n",
       " 235: 'lenguas',\n",
       " 236: 'metafisica',\n",
       " 237: 'existen',\n",
       " 238: 'muchos',\n",
       " 239: 'problema',\n",
       " 240: 'sociedad',\n",
       " 241: 'secreta',\n",
       " 242: 'abundan',\n",
       " 243: 'capaces',\n",
       " 244: 'tan',\n",
       " 245: 'mero',\n",
       " 246: 'leyes',\n",
       " 247: 'orden',\n",
       " 248: 'minutos',\n",
       " 249: 'lenguaje',\n",
       " 250: 'idealismo',\n",
       " 251: 'sustantivos',\n",
       " 252: 'idiomas',\n",
       " 253: 'luna',\n",
       " 254: 'anterior',\n",
       " 255: 'boreal',\n",
       " 256: 'tenue',\n",
       " 257: 'caso',\n",
       " 258: 'visual',\n",
       " 259: 'deja',\n",
       " 260: 'grado',\n",
       " 261: 'proceso',\n",
       " 262: 'comprende',\n",
       " 263: 'he',\n",
       " 264: 'divinidad',\n",
       " 265: 'buscan',\n",
       " 266: 'cosa',\n",
       " 267: 'supone',\n",
       " 268: 'tiene',\n",
       " 269: 'declara',\n",
       " 270: 'apenas',\n",
       " 271: 'nuestras',\n",
       " 272: 'asi',\n",
       " 273: 'camino',\n",
       " 274: 'miércoles',\n",
       " 275: 'cuarenta',\n",
       " 276: 'identidad',\n",
       " 277: 'dolor',\n",
       " 278: 'mismo',\n",
       " 279: 'sera',\n",
       " 280: 'geometria',\n",
       " 281: 'inventar',\n",
       " 282: 'siglos',\n",
       " 283: 'parece',\n",
       " 284: 'cosas',\n",
       " 285: 'memphis',\n",
       " 286: 'metal',\n",
       " 287: 'cono',\n",
       " 288: 'laberinto',\n",
       " 289: 'quinta',\n",
       " 290: 'calle',\n",
       " 291: 'produjo',\n",
       " 292: 'casares',\n",
       " 293: 'diversas',\n",
       " 294: 'contradicciones',\n",
       " 295: 'remoto',\n",
       " 296: 'descubrimos',\n",
       " 297: 'inevitable',\n",
       " 298: 'tienen',\n",
       " 299: 'abominables',\n",
       " 300: 'multiplican',\n",
       " 301: 'memorable',\n",
       " 302: 'registraba',\n",
       " 303: 'ultimas',\n",
       " 304: 'interrogé',\n",
       " 305: 'indice',\n",
       " 306: 'imaginables',\n",
       " 307: 'antes',\n",
       " 308: 'regién',\n",
       " 309: 'alguna',\n",
       " 310: 'anénimo',\n",
       " 311: 'eran',\n",
       " 312: 'frase',\n",
       " 313: 'atlas',\n",
       " 314: 'siguiente',\n",
       " 315: 'vista',\n",
       " 316: 'constaba',\n",
       " 317: 'recordado',\n",
       " 318: 'and',\n",
       " 319: 'mirrors',\n",
       " 320: 'are',\n",
       " 321: 'abominable',\n",
       " 322: 'visible',\n",
       " 323: 'precisamente',\n",
       " 324: 'sofisma',\n",
       " 325: 'trajo',\n",
       " 326: 'falsa',\n",
       " 327: 'lomo',\n",
       " 328: 'indicacién',\n",
       " 329: 'alfabética',\n",
       " 330: 'leimos',\n",
       " 331: 'unico',\n",
       " 332: 'parecia',\n",
       " 333: 'verosimil',\n",
       " 334: 'ajustado',\n",
       " 335: 'tono',\n",
       " 336: 'bajo',\n",
       " 337: 'escritura',\n",
       " 338: 'figuraban',\n",
       " 339: 'parte',\n",
       " 340: 'impostor',\n",
       " 341: 'bien',\n",
       " 342: 'rios',\n",
       " 343: 'misma',\n",
       " 344: 'verbigracia',\n",
       " 345: 'delta',\n",
       " 346: 'sur',\n",
       " 347: 'islas',\n",
       " 348: 'seccién',\n",
       " 349: 'raro',\n",
       " 350: 'exhumar',\n",
       " 351: 'piedra',\n",
       " 352: 'cardcter',\n",
       " 353: 'fantastico',\n",
       " 354: 'regiones',\n",
       " 355: 'hemos',\n",
       " 356: 'haslam',\n",
       " 357: 'land',\n",
       " 358: 'libreria',\n",
       " 359: 'primero',\n",
       " 360: 'und',\n",
       " 361: 'data',\n",
       " 362: 'andrea',\n",
       " 363: 'principios',\n",
       " 364: 'xvii',\n",
       " 365: 'rosa',\n",
       " 366: 'luego',\n",
       " 367: 'biblioteca',\n",
       " 368: 'memorias',\n",
       " 369: 'tampoco',\n",
       " 370: 'negros',\n",
       " 371: 'naturalmente',\n",
       " 372: 'it',\n",
       " 373: 'adrogué',\n",
       " 374: 'alto',\n",
       " 375: 'entiendo',\n",
       " 376: 'hijos',\n",
       " 377: 'sol',\n",
       " 378: 'solian',\n",
       " 379: 'mano',\n",
       " 380: 'colores',\n",
       " 381: 'hablamos',\n",
       " 382: 'duodecimal',\n",
       " 383: 'escribe',\n",
       " 384: 'sé',\n",
       " 385: 'duodecimales',\n",
       " 386: 'trabajo',\n",
       " 387: 'grande',\n",
       " 388: 'mencionado',\n",
       " 389: 'gaucho',\n",
       " 390: 'recibido',\n",
       " 391: 'dejé',\n",
       " 392: 'mis',\n",
       " 393: 'puertas',\n",
       " 394: 'agua',\n",
       " 395: 'integraban',\n",
       " 396: 'amarillo',\n",
       " 397: 'laminas',\n",
       " 398: 'azul',\n",
       " 399: 'azar',\n",
       " 400: 'pajaros',\n",
       " 401: 'ello',\n",
       " 402: 'clasico',\n",
       " 403: 'martinez',\n",
       " 404: 'estrada',\n",
       " 405: 'quiza',\n",
       " 406: 'estériles',\n",
       " 407: 'propone',\n",
       " 408: 'generacién',\n",
       " 409: 'puede',\n",
       " 410: 'plural',\n",
       " 411: 'infinito',\n",
       " 412: 'conjetura',\n",
       " 413: 'metafisicos',\n",
       " 414: 'oscuro',\n",
       " 415: 'individuos',\n",
       " 416: 'disciplinas',\n",
       " 417: 'riguroso',\n",
       " 418: 'escritor',\n",
       " 419: 'cosmos',\n",
       " 420: 'basteme',\n",
       " 421: 'recordar',\n",
       " 422: 'pienso',\n",
       " 423: 'concepto',\n",
       " 424: 'berkeley',\n",
       " 425: 'tierra',\n",
       " 426: 'letras',\n",
       " 427: 'ellos',\n",
       " 428: 'espacio',\n",
       " 429: 'serie',\n",
       " 430: 'sucesivo',\n",
       " 431: 'espacial',\n",
       " 432: 'conjetural',\n",
       " 433: 'ursprache',\n",
       " 434: 'verbos',\n",
       " 435: 'valor',\n",
       " 436: 'seria',\n",
       " 437: 'espafiol',\n",
       " 438: '0',\n",
       " 439: 'brevedad',\n",
       " 440: 'sustantivo',\n",
       " 441: 'forma',\n",
       " 442: 'adjetivos',\n",
       " 443: 'claro',\n",
       " 444: 'cualquier',\n",
       " 445: 'masa',\n",
       " 446: 'corresponde',\n",
       " 447: 'mera',\n",
       " 448: 'compuestos',\n",
       " 449: 'pajaro',\n",
       " 450: 'contra',\n",
       " 451: 'pecho',\n",
       " 452: 'vago',\n",
       " 453: 'pueden',\n",
       " 454: 'mediante',\n",
       " 455: 'ciertas',\n",
       " 456: 'enorme',\n",
       " 457: 'creado',\n",
       " 458: 'autor',\n",
       " 459: 'psicologia',\n",
       " 460: 'dicho',\n",
       " 461: 'conciben',\n",
       " 462: 'atribuye',\n",
       " 463: 'pensamiento',\n",
       " 464: 'tlon',\n",
       " 465: 'ciertos',\n",
       " 466: 'asociacién',\n",
       " 467: 'ideas',\n",
       " 468: 'posterior',\n",
       " 469: 'id',\n",
       " 470: 'est',\n",
       " 471: 'deducir',\n",
       " 472: 'ciencias',\n",
       " 473: 'acontece',\n",
       " 474: 'dialéctico',\n",
       " 475: 'increibles',\n",
       " 476: 'imposible',\n",
       " 477: 'instante',\n",
       " 478: 'operacién',\n",
       " 479: 'escuelas',\n",
       " 480: 'negar',\n",
       " 481: '’',\n",
       " 482: 'nuestra',\n",
       " 483: 'mutilado',\n",
       " 484: 'ellas',\n",
       " 485: 'vidas',\n",
       " 486: 'mientras',\n",
       " 487: 'materialismo',\n",
       " 488: 'paradoja',\n",
       " 489: 'tesis',\n",
       " 490: 'undécimo',\n",
       " 491: 'cobre',\n",
       " 492: 'versiones',\n",
       " 493: 'comun',\n",
       " 494: 'herrumbradas',\n",
       " 495: 'z',\n",
       " 496: 'descubre',\n",
       " 497: 'recuperadas',\n",
       " 498: 'absurdo',\n",
       " 499: 'imaginar',\n",
       " 500: 'existido',\n",
       " 501: 'madrugada',\n",
       " 502: 'secreto',\n",
       " 503: 'autorizadas',\n",
       " 504: 'humanidad',\n",
       " 505: 'acuerdo',\n",
       " 506: 'perder',\n",
       " 507: 'demostrar',\n",
       " 508: 'igualdad',\n",
       " 509: 'especie',\n",
       " 510: 'asimismo',\n",
       " 511: 'hipdtesis',\n",
       " 512: 'recuerda',\n",
       " 513: 'posibilidad',\n",
       " 514: 'conservar',\n",
       " 515: 'schopenhauer',\n",
       " 516: 'primer',\n",
       " 517: 'punto',\n",
       " 518: 'modifica',\n",
       " 519: 'memoria',\n",
       " 520: 'sabemos',\n",
       " 521: 'idea',\n",
       " 522: 'obras',\n",
       " 523: 'naturaleza',\n",
       " 524: 'hoy',\n",
       " 525: 'shakespeare',\n",
       " 526: 'lapiz',\n",
       " 527: 'fueron',\n",
       " 528: 'metddica',\n",
       " 529: 'primeros',\n",
       " 530: 'director',\n",
       " 531: 'durante',\n",
       " 532: 'les',\n",
       " 533: 'iban',\n",
       " 534: 'mascara',\n",
       " 535: 'oro',\n",
       " 536: 'curioso',\n",
       " 537: 'derivados',\n",
       " 538: 'inicial',\n",
       " 539: 'cuando',\n",
       " 540: 'olvida',\n",
       " 541: '1940',\n",
       " 542: 'carta',\n",
       " 543: 'gunnar',\n",
       " 544: 'erfjord',\n",
       " 545: 'afiliados',\n",
       " 546: 'bastaba',\n",
       " 547: 'américa',\n",
       " 548: 'tennessee',\n",
       " 549: 'minuciosa',\n",
       " 550: 'ocurrid',\n",
       " 551: 'princesa',\n",
       " 552: 'anhelaba',\n",
       " 553: 'amorim',\n",
       " 554: 'rachas',\n",
       " 555: 'chico',\n",
       " 556: 'evidencia',\n",
       " 557: 'rigor',\n",
       " 558: 'debo',\n",
       " 559: 'conjuncién',\n",
       " 560: 'inquietaba',\n",
       " 561: 'gaona',\n",
       " 562: 'ramos',\n",
       " 563: 'mejia',\n",
       " 564: 'falazmente',\n",
       " 565: 'nueva',\n",
       " 566: 'york',\n",
       " 567: '1917',\n",
       " 568: 'reimpresion',\n",
       " 569: 'literal',\n",
       " 570: 'morosa',\n",
       " 571: '1902',\n",
       " 572: 'hard',\n",
       " 573: 'cinco',\n",
       " 574: 'cenado',\n",
       " 575: 'conmigo',\n",
       " 576: 'demor6',\n",
       " 577: 'polémica',\n",
       " 578: 'ejecucién',\n",
       " 579: 'novela',\n",
       " 580: 'persona',\n",
       " 581: 'narrador',\n",
       " 582: 'omitiera',\n",
       " 583: 'desfigurara',\n",
       " 584: 'incurriera',\n",
       " 585: 'permitieran',\n",
       " 586: 'adivinacién',\n",
       " 587: 'atroz',\n",
       " 588: 'banal',\n",
       " 589: 'acechaba',\n",
       " 590: 'alta',\n",
       " 591: 'monstruoso',\n",
       " 592: 'record6é',\n",
       " 593: 'heresiarcas',\n",
       " 594: 'declarado',\n",
       " 595: 'cépula',\n",
       " 596: 'pregunté',\n",
       " 597: 'origen',\n",
       " 598: 'sentencia',\n",
       " 599: 'contest6',\n",
       " 600: 'habiamos',\n",
       " 601: 'alquilado',\n",
       " 602: 'amueblada',\n",
       " 603: 'poseia',\n",
       " 604: 'dimos',\n",
       " 605: 'upsala',\n",
       " 606: 'xlvii',\n",
       " 607: 'uralt',\n",
       " 608: 'altaic',\n",
       " 609: 'languages',\n",
       " 610: 'azorado',\n",
       " 611: 'agot6',\n",
       " 612: 'lecciones',\n",
       " 613: 'ukbar',\n",
       " 614: 'ucbar',\n",
       " 615: 'ooqbar',\n",
       " 616: 'ookbar',\n",
       " 617: 'oukbahr',\n",
       " 618: 'irse',\n",
       " 619: 'irak',\n",
       " 620: 'asia',\n",
       " 621: 'confieso',\n",
       " 622: 'asenti',\n",
       " 623: 'incomodidad',\n",
       " 624: 'conjeturé',\n",
       " 625: 'indocumentado',\n",
       " 626: 'ficci6n',\n",
       " 627: 'improvisada',\n",
       " 628: 'justificar',\n",
       " 629: 'examen',\n",
       " 630: 'estéril',\n",
       " 631: 'justus',\n",
       " 632: 'perthes',\n",
       " 633: 'fortaleciéd',\n",
       " 634: 'llam6é',\n",
       " 635: 'buenos',\n",
       " 636: 'aires',\n",
       " 637: 'noticia',\n",
       " 638: 'formulada',\n",
       " 639: 'idénticas',\n",
       " 640: 'repetidas',\n",
       " 641: 'literariamente',\n",
       " 642: 'inferiores',\n",
       " 643: 'copulation',\n",
       " 644: 'decia',\n",
       " 645: '«para',\n",
       " 646: 'gnosticos',\n",
       " 647: 'ilusi6n',\n",
       " 648: 'paternidad',\n",
       " 649: 'fatherhood',\n",
       " 650: 'divulgan»',\n",
       " 651: 'dije',\n",
       " 652: 'faltar',\n",
       " 653: 'gustaria',\n",
       " 654: 'ver',\n",
       " 655: 'cual',\n",
       " 656: 'sorprendiéd',\n",
       " 657: 'escrupulosos',\n",
       " 658: 'indices',\n",
       " 659: 'cartograficos',\n",
       " 660: 'erdkunde',\n",
       " 661: 'ritter',\n",
       " 662: 'ignoraban',\n",
       " 663: 'plenitud',\n",
       " 664: 'efectivamente',\n",
       " 665: 'caradtula',\n",
       " 666: 'tor',\n",
       " 667: 'ups',\n",
       " 668: 'nuestro',\n",
       " 669: '917',\n",
       " 670: '921',\n",
       " 671: 'adicionales',\n",
       " 672: 'comprendian',\n",
       " 673: 'previsto',\n",
       " 674: 'habra',\n",
       " 675: 'advertido',\n",
       " 676: 'lector',\n",
       " 677: 'comprobamos',\n",
       " 678: 'diferencia',\n",
       " 679: 'seguin',\n",
       " 680: 'creo',\n",
       " 681: 'haber',\n",
       " 682: 'indicado',\n",
       " 683: 'décima',\n",
       " 684: 'adquirido',\n",
       " 685: 'remates',\n",
       " 686: 'cuidado',\n",
       " 687: 'pasaje',\n",
       " 688: 'sorprendente',\n",
       " 689: 'resto',\n",
       " 690: 'natural',\n",
       " 691: 'aburrido',\n",
       " 692: 'releyéndolo',\n",
       " 693: 'rigurosa',\n",
       " 694: 'vaguedad',\n",
       " 695: 'catorce',\n",
       " 696: 'geografica',\n",
       " 697: 'reconocimos',\n",
       " 698: 'jorasan',\n",
       " 699: 'armenia',\n",
       " 700: 'erzerum',\n",
       " 701: 'interpolados',\n",
       " 702: 'ambiguo',\n",
       " 703: 'histéricos',\n",
       " 704: 'esmerdis',\n",
       " 705: 'mago',\n",
       " 706: 'invocado',\n",
       " 707: 'metdfora',\n",
       " 708: 'nota',\n",
       " 709: 'precisar',\n",
       " 710: 'fronteras',\n",
       " 711: 'nebulosos',\n",
       " 712: 'puntos',\n",
       " 713: 'referencia',\n",
       " 714: 'crateres',\n",
       " 715: 'cadenas',\n",
       " 716: 'region',\n",
       " 717: 'tierras',\n",
       " 718: 'bajas',\n",
       " 719: 'tsai',\n",
       " 720: 'jaldun',\n",
       " 721: 'axa',\n",
       " 722: 'definen',\n",
       " 723: 'frontera',\n",
       " 724: 'procrean',\n",
       " 725: 'caballos',\n",
       " 726: 'salvajes',\n",
       " 727: 'eso',\n",
       " 728: '918',\n",
       " 729: 'histérica',\n",
       " 730: '920',\n",
       " 731: 'supimos',\n",
       " 732: 'raiz',\n",
       " 733: 'persecuciones',\n",
       " 734: 'religiosas',\n",
       " 735: 'xiii',\n",
       " 736: 'ortodoxos',\n",
       " 737: 'buscaron',\n",
       " 738: 'amparo',\n",
       " 739: 'perduran',\n",
       " 740: 'obeliscos',\n",
       " 741: '«idioma',\n",
       " 742: 'literatura»',\n",
       " 743: 'breve',\n",
       " 744: 'rasgo',\n",
       " 745: 'anotaba',\n",
       " 746: 'epopeyas',\n",
       " 747: 'leyendas',\n",
       " 748: 'referian',\n",
       " 749: 'jamas',\n",
       " 750: 'imaginarias',\n",
       " 751: 'mlejnas',\n",
       " 752: 'bibliografia',\n",
       " 753: 'enumeraba',\n",
       " 754: 'volimenes',\n",
       " 755: 'encontrado',\n",
       " 756: 'tercero',\n",
       " 757: 'silas',\n",
       " 758: 'hystory',\n",
       " 759: 'called',\n",
       " 760: '1874',\n",
       " 761: 'figura',\n",
       " 762: 'catalogos',\n",
       " 763: 'bernard',\n",
       " 764: 'quaritch’',\n",
       " 765: 'lesbare',\n",
       " 766: 'lesenswerthe',\n",
       " 767: 'bemerkungen',\n",
       " 768: 'tiber',\n",
       " 769: 'das',\n",
       " 770: 'ukkbar',\n",
       " 771: 'in',\n",
       " 772: 'klein',\n",
       " 773: 'asien',\n",
       " 774: '1641',\n",
       " 775: 'johannes',\n",
       " 776: 'valentinus',\n",
       " 777: 'significativo',\n",
       " 778: 'afos',\n",
       " 779: 'di',\n",
       " 780: 'inesperadas',\n",
       " 781: 'quincey',\n",
       " 782: 'writings',\n",
       " 783: 'decimotercer',\n",
       " 784: 'supe',\n",
       " 785: 'tedlogo',\n",
       " 786: 'aleman',\n",
       " 787: 'describié',\n",
       " 788: 'imaginaria',\n",
       " 789: 'comunidad',\n",
       " 790: 'cruz',\n",
       " 791: 'fundaron',\n",
       " 792: 'imitacién',\n",
       " 793: 'prefigurado',\n",
       " 794: 'visitamos',\n",
       " 795: 'nacional',\n",
       " 796: 'fatigamos',\n",
       " 797: 'catdalogos',\n",
       " 798: 'anuarios',\n",
       " 799: 'sociedades',\n",
       " 800: 'geograficas',\n",
       " 801: 'viajeros',\n",
       " 802: 'historiadores',\n",
       " 803: 'carlos',\n",
       " 804: 'mastronardi',\n",
       " 805: 'referido',\n",
       " 806: 'asunto',\n",
       " 807: 'advirtid',\n",
       " 808: 'corrientes',\n",
       " 809: 'talcahuano',\n",
       " 810: 'dorados',\n",
       " 811: 'lomos',\n",
       " 812: 'entré',\n",
       " 813: 'dio',\n",
       " 814: 'indicio',\n",
       " 815: 'limitado',\n",
       " 816: 'menguante',\n",
       " 817: 'ingeniero',\n",
       " 818: 'ferrocarriles',\n",
       " 819: 'persiste',\n",
       " 820: 'efusivas',\n",
       " 821: 'madreselvas',\n",
       " 822: 'padecié',\n",
       " 823: 'irrealidad',\n",
       " 824: 'ingleses',\n",
       " 825: 'fantasma',\n",
       " 826: 'desganado',\n",
       " 827: 'cansada',\n",
       " 828: 'barba',\n",
       " 829: 'rectangular',\n",
       " 830: 'roja',\n",
       " 831: 'viudo',\n",
       " 832: 'iba',\n",
       " 833: 'inglaterra',\n",
       " 834: 'visitar',\n",
       " 835: 'juzgo',\n",
       " 836: 'fotografias',\n",
       " 837: 'mostré',\n",
       " 838: 'reloj',\n",
       " 839: 'robles',\n",
       " 840: 'padre',\n",
       " 841: 'estrechado',\n",
       " 842: 'excesivo',\n",
       " 843: 'amistades',\n",
       " 844: 'inglesas',\n",
       " 845: 'empiezan',\n",
       " 846: 'excluir',\n",
       " 847: 'confidencia',\n",
       " 848: 'pronto',\n",
       " 849: 'omiten',\n",
       " 850: 'didlogo',\n",
       " 851: 'ejercer',\n",
       " 852: 'intercambio',\n",
       " 853: 'periddicos',\n",
       " 854: 'batirse',\n",
       " 855: 'ajedrez',\n",
       " 856: 'publicado',\n",
       " 857: 'history',\n",
       " 858: 'labyrinths',\n",
       " 859: 'taciturnamente',\n",
       " 860: 'matematicas',\n",
       " 861: 'mirando',\n",
       " 862: 'irrecuperables',\n",
       " 863: 'numeracién',\n",
       " 864: 'doce',\n",
       " 865: 'trasladando',\n",
       " 866: 'qué',\n",
       " 867: 'tablas',\n",
       " 868: 'sexagesimales',\n",
       " 869: 'sesenta',\n",
       " 870: 'agregé',\n",
       " 871: 'encargado',\n",
       " 872: 'noruego',\n",
       " 873: 'do',\n",
       " 874: 'sul',\n",
       " 875: 'ocho',\n",
       " 876: 'conociamos',\n",
       " 877: 'estadia',\n",
       " 878: 'pastoril',\n",
       " 879: 'capangas',\n",
       " 880: 'etimologia',\n",
       " 881: 'brasilera',\n",
       " 882: 'viejos',\n",
       " 883: 'orientales',\n",
       " 884: 'pronuncian',\n",
       " 885: 'perdone',\n",
       " 886: 'funciones',\n",
       " 887: 'septiembre',\n",
       " 888: '1937',\n",
       " 889: 'estabamos',\n",
       " 890: 'nosotros',\n",
       " 891: 'murié',\n",
       " 892: 'rotura',\n",
       " 893: 'aneurisma',\n",
       " 894: 'brasil',\n",
       " 895: 'paquete',\n",
       " 896: 'sellado',\n",
       " 897: 'certificado',\n",
       " 898: 'octavo',\n",
       " 899: 'bar',\n",
       " 900: 'encontré',\n",
       " 901: 'puse',\n",
       " 902: 'hojearlo',\n",
       " 903: 'vértigo',\n",
       " 904: 'asombrado',\n",
       " 905: 'ligero',\n",
       " 906: 'describiré',\n",
       " 907: 'ésta',\n",
       " 908: 'emociones',\n",
       " 909: 'islam',\n",
       " 910: 'abren',\n",
       " 911: 'secretas',\n",
       " 912: 'dulce',\n",
       " 913: 'cantaros',\n",
       " 914: 'abrieran',\n",
       " 915: 'sentiria',\n",
       " 916: 'redactado',\n",
       " 917: '1001',\n",
       " 918: 'cuero',\n",
       " 919: 'lei',\n",
       " 920: 'estas',\n",
       " 921: 'curiosas',\n",
       " 922: 'cardtula',\n",
       " 923: 'repetia',\n",
       " 924: 'first',\n",
       " 925: 'vol',\n",
       " 926: 'xi',\n",
       " 927: 'hlaer',\n",
       " 928: 'to',\n",
       " 929: 'jangr',\n",
       " 930: 'indicaciédn',\n",
       " 931: 'lugar',\n",
       " 932: 'hoja',\n",
       " 933: 'papel',\n",
       " 934: 'seda',\n",
       " 935: 'cubria',\n",
       " 936: 'estampado',\n",
       " 937: '6valo',\n",
       " 938: 'inscripcién',\n",
       " 939: 'descubierto',\n",
       " 940: 'cierta',\n",
       " 941: 'pirdtica',\n",
       " 942: 'somera',\n",
       " 943: 'descripcién',\n",
       " 944: 'deparaba',\n",
       " 945: 'precioso',\n",
       " 946: 'arduo',\n",
       " 947: 'manos',\n",
       " 948: 'fragmento',\n",
       " 949: 'metéddico',\n",
       " 950: 'desconocido',\n",
       " 951: 'arquitecturas',\n",
       " 952: 'barajas',\n",
       " 953: 'pavor',\n",
       " 954: 'mitologias',\n",
       " 955: 'rumor',\n",
       " 956: 'emperadores',\n",
       " 957: 'mares',\n",
       " 958: 'minerales',\n",
       " 959: 'peces',\n",
       " 960: 'algebra',\n",
       " 961: 'fuego',\n",
       " 962: 'controversia',\n",
       " 963: 'teolédgica',\n",
       " 964: 'articulado',\n",
       " 965: 'coherente',\n",
       " 966: 'propdsito',\n",
       " 967: 'doctrinal',\n",
       " 968: 'parddico',\n",
       " 969: 'hablo',\n",
       " 970: 'alusiones',\n",
       " 971: 'ulteriores',\n",
       " 972: 'precedentes',\n",
       " 973: 'néstor',\n",
       " 974: 'ibarra',\n",
       " 975: 'nrf',\n",
       " 976: 'negado',\n",
       " 977: 'aldteres',\n",
       " 978: 'ezequiel',\n",
       " 979: 'drieu',\n",
       " 980: 'rochelle',\n",
       " 981: 'refutado',\n",
       " 982: 'victoriosamente',\n",
       " 983: 'pesquisas',\n",
       " 984: 'diligentes',\n",
       " 985: 'desordenado',\n",
       " 986: 'bibliotecas',\n",
       " 987: 'américas',\n",
       " 988: 'europa',\n",
       " 989: 'alfonso',\n",
       " 990: 'reyes',\n",
       " 991: 'harto',\n",
       " 992: 'fatigas',\n",
       " 993: 'subalternas',\n",
       " 994: 'indole',\n",
       " 995: 'policial',\n",
       " 996: 'acometamos',\n",
       " 997: 'reconstruir',\n",
       " 998: 'macizos',\n",
       " 999: 'faltan',\n",
       " 1000: 'ex',\n",
       " ...}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras del vocabulario\n",
    "tok.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "gJgVhq1zwEpf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de palabras en el vocabulario\n",
    "vocab_size = len(tok.word_counts)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "wKV85M2uwKJ9"
   },
   "outputs": [],
   "source": [
    "# Transformar los datos a oneHotEncoding\n",
    "y_data = to_categorical(y_data_int-1, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "qWjNrNx9wM1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En el caso anterior explota porque y_data_int comienza en \"1\" en vez de \"0\"\n",
    "# valor minimo:\n",
    "min(y_data_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "gIg2e2WCwXbG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5738, 2078)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_int_offset = y_data_int - 1\n",
    "y_data = to_categorical(y_data_int_offset, num_classes=vocab_size) \n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmJWNyxQwfCE"
   },
   "source": [
    "### 4 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "0cOmNZT_weK2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# largo de la secuencia de entrada\n",
    "input_seq_len = x_data.shape[1] \n",
    "input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "qtwITjgnwlgp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Largo del vector de salida --> vocab_size\n",
    "output_size = vocab_size\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "jzTZRXrrwrvi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 3, 5)              10395     \n",
      "                                                                 \n",
      " lstm_35 (LSTM)              (None, 3, 64)             17920     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " lstm_36 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 2078)              68574     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,993\n",
      "Trainable params: 131,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding:\n",
    "# input_seq_len = 3 --> ingreso 3 palabras\n",
    "# input_dim = vocab_size --> 1628 palabras distintas\n",
    "# output_dim = 5 --> crear embeddings de tamaño 3 (tamaño variable y ajustable)\n",
    "model.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_length=input_seq_len))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(LSTM(64)) # La última capa LSTM no lleva return_sequences\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Predicción de clasificación con softmax\n",
    "# La salida vuelve al espacio de 1628 palabras posibles\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Clasificación multiple categórica --> loss = categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "oQq1PHDkxDvN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "144/144 [==============================] - 11s 19ms/step - loss: 6.9613 - accuracy: 0.0508 - val_loss: 6.7422 - val_accuracy: 0.0767\n",
      "Epoch 2/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 6.2771 - accuracy: 0.0656 - val_loss: 7.0110 - val_accuracy: 0.0767\n",
      "Epoch 3/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 6.1991 - accuracy: 0.0656 - val_loss: 7.1941 - val_accuracy: 0.0767\n",
      "Epoch 4/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 6.1453 - accuracy: 0.0656 - val_loss: 7.3908 - val_accuracy: 0.0767\n",
      "Epoch 5/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 6.0775 - accuracy: 0.0656 - val_loss: 7.5090 - val_accuracy: 0.0767\n",
      "Epoch 6/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 6.0279 - accuracy: 0.0660 - val_loss: 7.5624 - val_accuracy: 0.0767\n",
      "Epoch 7/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.9644 - accuracy: 0.0660 - val_loss: 7.8852 - val_accuracy: 0.0767\n",
      "Epoch 8/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.9193 - accuracy: 0.0660 - val_loss: 7.9811 - val_accuracy: 0.0767\n",
      "Epoch 9/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.8862 - accuracy: 0.0660 - val_loss: 8.0432 - val_accuracy: 0.0767\n",
      "Epoch 10/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.8643 - accuracy: 0.0660 - val_loss: 8.1673 - val_accuracy: 0.0767\n",
      "Epoch 11/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.8386 - accuracy: 0.0660 - val_loss: 8.2155 - val_accuracy: 0.0767\n",
      "Epoch 12/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.8206 - accuracy: 0.0660 - val_loss: 8.3056 - val_accuracy: 0.0758\n",
      "Epoch 13/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.8010 - accuracy: 0.0660 - val_loss: 8.4062 - val_accuracy: 0.0749\n",
      "Epoch 14/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.7750 - accuracy: 0.0660 - val_loss: 8.5891 - val_accuracy: 0.0740\n",
      "Epoch 15/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.7396 - accuracy: 0.0660 - val_loss: 8.6768 - val_accuracy: 0.0740\n",
      "Epoch 16/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.6982 - accuracy: 0.0660 - val_loss: 8.8074 - val_accuracy: 0.0767\n",
      "Epoch 17/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.6322 - accuracy: 0.0673 - val_loss: 8.6299 - val_accuracy: 0.0758\n",
      "Epoch 18/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.5637 - accuracy: 0.0673 - val_loss: 8.7601 - val_accuracy: 0.0784\n",
      "Epoch 19/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.5018 - accuracy: 0.0697 - val_loss: 9.0845 - val_accuracy: 0.0697\n",
      "Epoch 20/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.4547 - accuracy: 0.0693 - val_loss: 8.9464 - val_accuracy: 0.0758\n",
      "Epoch 21/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.4080 - accuracy: 0.0699 - val_loss: 9.2499 - val_accuracy: 0.0775\n",
      "Epoch 22/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.3747 - accuracy: 0.0688 - val_loss: 9.1495 - val_accuracy: 0.0767\n",
      "Epoch 23/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.3292 - accuracy: 0.0717 - val_loss: 9.3645 - val_accuracy: 0.0688\n",
      "Epoch 24/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.3041 - accuracy: 0.0708 - val_loss: 9.8076 - val_accuracy: 0.0740\n",
      "Epoch 25/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.2764 - accuracy: 0.0693 - val_loss: 9.7367 - val_accuracy: 0.0732\n",
      "Epoch 26/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.2551 - accuracy: 0.0710 - val_loss: 10.0267 - val_accuracy: 0.0697\n",
      "Epoch 27/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.2355 - accuracy: 0.0712 - val_loss: 9.9745 - val_accuracy: 0.0749\n",
      "Epoch 28/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.2194 - accuracy: 0.0725 - val_loss: 10.1191 - val_accuracy: 0.0688\n",
      "Epoch 29/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.1903 - accuracy: 0.0721 - val_loss: 10.2609 - val_accuracy: 0.0688\n",
      "Epoch 30/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.1670 - accuracy: 0.0739 - val_loss: 10.4642 - val_accuracy: 0.0706\n",
      "Epoch 31/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.1516 - accuracy: 0.0732 - val_loss: 10.5349 - val_accuracy: 0.0627\n",
      "Epoch 32/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 5.1320 - accuracy: 0.0719 - val_loss: 10.6245 - val_accuracy: 0.0618\n",
      "Epoch 33/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.1130 - accuracy: 0.0725 - val_loss: 10.8443 - val_accuracy: 0.0723\n",
      "Epoch 34/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.0860 - accuracy: 0.0771 - val_loss: 10.7120 - val_accuracy: 0.0645\n",
      "Epoch 35/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 5.0633 - accuracy: 0.0776 - val_loss: 11.0721 - val_accuracy: 0.0584\n",
      "Epoch 36/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 5.0324 - accuracy: 0.0745 - val_loss: 11.3157 - val_accuracy: 0.0592\n",
      "Epoch 37/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.9990 - accuracy: 0.0758 - val_loss: 11.3757 - val_accuracy: 0.0610\n",
      "Epoch 38/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.9763 - accuracy: 0.0778 - val_loss: 11.4837 - val_accuracy: 0.0636\n",
      "Epoch 39/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.9440 - accuracy: 0.0752 - val_loss: 11.6985 - val_accuracy: 0.0584\n",
      "Epoch 40/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.9087 - accuracy: 0.0739 - val_loss: 11.9505 - val_accuracy: 0.0592\n",
      "Epoch 41/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 4.8838 - accuracy: 0.0808 - val_loss: 11.9643 - val_accuracy: 0.0505\n",
      "Epoch 42/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.8441 - accuracy: 0.0837 - val_loss: 12.2565 - val_accuracy: 0.0584\n",
      "Epoch 43/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.8234 - accuracy: 0.0854 - val_loss: 12.5651 - val_accuracy: 0.0584\n",
      "Epoch 44/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.7960 - accuracy: 0.0828 - val_loss: 12.6675 - val_accuracy: 0.0566\n",
      "Epoch 45/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.7674 - accuracy: 0.0817 - val_loss: 12.8987 - val_accuracy: 0.0557\n",
      "Epoch 46/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.7342 - accuracy: 0.0817 - val_loss: 12.5514 - val_accuracy: 0.0540\n",
      "Epoch 47/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.6952 - accuracy: 0.0850 - val_loss: 12.9481 - val_accuracy: 0.0557\n",
      "Epoch 48/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.6635 - accuracy: 0.0856 - val_loss: 12.9887 - val_accuracy: 0.0540\n",
      "Epoch 49/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.6330 - accuracy: 0.0856 - val_loss: 13.4132 - val_accuracy: 0.0531\n",
      "Epoch 50/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.6092 - accuracy: 0.0869 - val_loss: 13.5799 - val_accuracy: 0.0523\n",
      "Epoch 51/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.5781 - accuracy: 0.0895 - val_loss: 13.5741 - val_accuracy: 0.0523\n",
      "Epoch 52/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.5465 - accuracy: 0.0902 - val_loss: 13.5672 - val_accuracy: 0.0549\n",
      "Epoch 53/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.5235 - accuracy: 0.0928 - val_loss: 14.2210 - val_accuracy: 0.0549\n",
      "Epoch 54/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.5051 - accuracy: 0.0924 - val_loss: 14.3085 - val_accuracy: 0.0523\n",
      "Epoch 55/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.4735 - accuracy: 0.0900 - val_loss: 14.4474 - val_accuracy: 0.0531\n",
      "Epoch 56/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.4458 - accuracy: 0.0954 - val_loss: 14.2731 - val_accuracy: 0.0523\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 2s 13ms/step - loss: 4.4277 - accuracy: 0.0989 - val_loss: 14.6964 - val_accuracy: 0.0514\n",
      "Epoch 58/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.4075 - accuracy: 0.0948 - val_loss: 14.6548 - val_accuracy: 0.0479\n",
      "Epoch 59/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.3784 - accuracy: 0.0998 - val_loss: 14.9063 - val_accuracy: 0.0523\n",
      "Epoch 60/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.3509 - accuracy: 0.0950 - val_loss: 14.8920 - val_accuracy: 0.0514\n",
      "Epoch 61/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.3192 - accuracy: 0.1041 - val_loss: 15.1186 - val_accuracy: 0.0531\n",
      "Epoch 62/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.3009 - accuracy: 0.1026 - val_loss: 15.5003 - val_accuracy: 0.0540\n",
      "Epoch 63/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.2868 - accuracy: 0.1044 - val_loss: 15.3232 - val_accuracy: 0.0497\n",
      "Epoch 64/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.2589 - accuracy: 0.1033 - val_loss: 15.4422 - val_accuracy: 0.0497\n",
      "Epoch 65/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.2400 - accuracy: 0.1041 - val_loss: 15.6102 - val_accuracy: 0.0557\n",
      "Epoch 66/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.2162 - accuracy: 0.1037 - val_loss: 15.7593 - val_accuracy: 0.0523\n",
      "Epoch 67/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.1966 - accuracy: 0.1046 - val_loss: 15.9057 - val_accuracy: 0.0444\n",
      "Epoch 68/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.1859 - accuracy: 0.1070 - val_loss: 16.2485 - val_accuracy: 0.0531\n",
      "Epoch 69/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.1557 - accuracy: 0.1054 - val_loss: 16.2696 - val_accuracy: 0.0488\n",
      "Epoch 70/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 4.1286 - accuracy: 0.1131 - val_loss: 16.0173 - val_accuracy: 0.0479\n",
      "Epoch 71/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 4.1167 - accuracy: 0.1113 - val_loss: 16.3025 - val_accuracy: 0.0488\n",
      "Epoch 72/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.0828 - accuracy: 0.1163 - val_loss: 16.6661 - val_accuracy: 0.0497\n",
      "Epoch 73/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.0694 - accuracy: 0.1150 - val_loss: 16.7528 - val_accuracy: 0.0523\n",
      "Epoch 74/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.0457 - accuracy: 0.1148 - val_loss: 17.0029 - val_accuracy: 0.0523\n",
      "Epoch 75/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.0241 - accuracy: 0.1168 - val_loss: 17.0423 - val_accuracy: 0.0427\n",
      "Epoch 76/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.0082 - accuracy: 0.1131 - val_loss: 17.3934 - val_accuracy: 0.0549\n",
      "Epoch 77/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.9715 - accuracy: 0.1231 - val_loss: 17.3399 - val_accuracy: 0.0557\n",
      "Epoch 78/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.9584 - accuracy: 0.1183 - val_loss: 17.5150 - val_accuracy: 0.0514\n",
      "Epoch 79/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.9442 - accuracy: 0.1200 - val_loss: 17.8853 - val_accuracy: 0.0488\n",
      "Epoch 80/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.9189 - accuracy: 0.1198 - val_loss: 17.8566 - val_accuracy: 0.0462\n",
      "Epoch 81/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.8906 - accuracy: 0.1266 - val_loss: 17.8535 - val_accuracy: 0.0514\n",
      "Epoch 82/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.8879 - accuracy: 0.1235 - val_loss: 18.0812 - val_accuracy: 0.0497\n",
      "Epoch 83/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.8509 - accuracy: 0.1281 - val_loss: 18.1671 - val_accuracy: 0.0462\n",
      "Epoch 84/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.8357 - accuracy: 0.1272 - val_loss: 18.4958 - val_accuracy: 0.0497\n",
      "Epoch 85/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.7992 - accuracy: 0.1316 - val_loss: 18.4448 - val_accuracy: 0.0497\n",
      "Epoch 86/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.7834 - accuracy: 0.1264 - val_loss: 18.6188 - val_accuracy: 0.0523\n",
      "Epoch 87/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.7595 - accuracy: 0.1349 - val_loss: 19.0666 - val_accuracy: 0.0497\n",
      "Epoch 88/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.7638 - accuracy: 0.1303 - val_loss: 18.9885 - val_accuracy: 0.0488\n",
      "Epoch 89/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.7259 - accuracy: 0.1322 - val_loss: 19.0469 - val_accuracy: 0.0488\n",
      "Epoch 90/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.7261 - accuracy: 0.1325 - val_loss: 19.3117 - val_accuracy: 0.0453\n",
      "Epoch 91/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.6902 - accuracy: 0.1381 - val_loss: 19.5984 - val_accuracy: 0.0514\n",
      "Epoch 92/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.6821 - accuracy: 0.1331 - val_loss: 19.6582 - val_accuracy: 0.0462\n",
      "Epoch 93/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.6455 - accuracy: 0.1383 - val_loss: 19.8204 - val_accuracy: 0.0531\n",
      "Epoch 94/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.6390 - accuracy: 0.1381 - val_loss: 20.1208 - val_accuracy: 0.0436\n",
      "Epoch 95/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.6153 - accuracy: 0.1427 - val_loss: 19.7575 - val_accuracy: 0.0523\n",
      "Epoch 96/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.6223 - accuracy: 0.1368 - val_loss: 20.0250 - val_accuracy: 0.0366\n",
      "Epoch 97/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.5812 - accuracy: 0.1344 - val_loss: 20.3498 - val_accuracy: 0.0531\n",
      "Epoch 98/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.5848 - accuracy: 0.1399 - val_loss: 20.4215 - val_accuracy: 0.0523\n",
      "Epoch 99/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.5539 - accuracy: 0.1388 - val_loss: 20.6589 - val_accuracy: 0.0453\n",
      "Epoch 100/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.5345 - accuracy: 0.1468 - val_loss: 20.9339 - val_accuracy: 0.0392\n",
      "Epoch 101/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.5078 - accuracy: 0.1473 - val_loss: 20.9540 - val_accuracy: 0.0444\n",
      "Epoch 102/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 3.5169 - accuracy: 0.1458 - val_loss: 20.9321 - val_accuracy: 0.0462\n",
      "Epoch 103/500\n",
      "144/144 [==============================] - 2s 17ms/step - loss: 3.4809 - accuracy: 0.1510 - val_loss: 21.3513 - val_accuracy: 0.0523\n",
      "Epoch 104/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 3.4827 - accuracy: 0.1475 - val_loss: 21.2696 - val_accuracy: 0.0479\n",
      "Epoch 105/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.4422 - accuracy: 0.1508 - val_loss: 21.5472 - val_accuracy: 0.0523\n",
      "Epoch 106/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.4390 - accuracy: 0.1479 - val_loss: 21.6933 - val_accuracy: 0.0462\n",
      "Epoch 107/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.4112 - accuracy: 0.1575 - val_loss: 21.9593 - val_accuracy: 0.0418\n",
      "Epoch 108/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.4140 - accuracy: 0.1575 - val_loss: 21.9667 - val_accuracy: 0.0549\n",
      "Epoch 109/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.3980 - accuracy: 0.1599 - val_loss: 21.7197 - val_accuracy: 0.0497\n",
      "Epoch 110/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.3994 - accuracy: 0.1525 - val_loss: 22.0405 - val_accuracy: 0.0444\n",
      "Epoch 111/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 3.3741 - accuracy: 0.1643 - val_loss: 22.3593 - val_accuracy: 0.0418\n",
      "Epoch 112/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.3539 - accuracy: 0.1612 - val_loss: 22.5162 - val_accuracy: 0.0488\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 2s 14ms/step - loss: 3.3256 - accuracy: 0.1680 - val_loss: 22.7671 - val_accuracy: 0.0488\n",
      "Epoch 114/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.3284 - accuracy: 0.1671 - val_loss: 22.4865 - val_accuracy: 0.0453\n",
      "Epoch 115/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.3214 - accuracy: 0.1667 - val_loss: 22.6886 - val_accuracy: 0.0427\n",
      "Epoch 116/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2919 - accuracy: 0.1647 - val_loss: 22.7326 - val_accuracy: 0.0462\n",
      "Epoch 117/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2659 - accuracy: 0.1730 - val_loss: 23.0971 - val_accuracy: 0.0497\n",
      "Epoch 118/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2522 - accuracy: 0.1704 - val_loss: 23.1688 - val_accuracy: 0.0453\n",
      "Epoch 119/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2455 - accuracy: 0.1745 - val_loss: 23.0327 - val_accuracy: 0.0418\n",
      "Epoch 120/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2322 - accuracy: 0.1715 - val_loss: 23.2238 - val_accuracy: 0.0470\n",
      "Epoch 121/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2335 - accuracy: 0.1804 - val_loss: 23.5832 - val_accuracy: 0.0514\n",
      "Epoch 122/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.2197 - accuracy: 0.1758 - val_loss: 23.5631 - val_accuracy: 0.0392\n",
      "Epoch 123/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1909 - accuracy: 0.1752 - val_loss: 23.6129 - val_accuracy: 0.0427\n",
      "Epoch 124/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1629 - accuracy: 0.1834 - val_loss: 23.8029 - val_accuracy: 0.0401\n",
      "Epoch 125/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1501 - accuracy: 0.1810 - val_loss: 23.9821 - val_accuracy: 0.0409\n",
      "Epoch 126/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1484 - accuracy: 0.1874 - val_loss: 24.1236 - val_accuracy: 0.0427\n",
      "Epoch 127/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1479 - accuracy: 0.1893 - val_loss: 24.0170 - val_accuracy: 0.0392\n",
      "Epoch 128/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1185 - accuracy: 0.1943 - val_loss: 24.3302 - val_accuracy: 0.0305\n",
      "Epoch 129/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.1152 - accuracy: 0.1882 - val_loss: 24.7292 - val_accuracy: 0.0427\n",
      "Epoch 130/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0901 - accuracy: 0.1915 - val_loss: 24.7972 - val_accuracy: 0.0401\n",
      "Epoch 131/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0827 - accuracy: 0.2031 - val_loss: 24.6612 - val_accuracy: 0.0392\n",
      "Epoch 132/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0686 - accuracy: 0.1980 - val_loss: 24.6591 - val_accuracy: 0.0401\n",
      "Epoch 133/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0547 - accuracy: 0.1985 - val_loss: 25.0459 - val_accuracy: 0.0427\n",
      "Epoch 134/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0561 - accuracy: 0.2000 - val_loss: 24.8991 - val_accuracy: 0.0470\n",
      "Epoch 135/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0453 - accuracy: 0.1980 - val_loss: 25.1511 - val_accuracy: 0.0436\n",
      "Epoch 136/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0368 - accuracy: 0.2002 - val_loss: 24.9369 - val_accuracy: 0.0401\n",
      "Epoch 137/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 3.0293 - accuracy: 0.2022 - val_loss: 25.0896 - val_accuracy: 0.0418\n",
      "Epoch 138/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9947 - accuracy: 0.2092 - val_loss: 25.3858 - val_accuracy: 0.0418\n",
      "Epoch 139/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9880 - accuracy: 0.2098 - val_loss: 25.4396 - val_accuracy: 0.0409\n",
      "Epoch 140/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9683 - accuracy: 0.2102 - val_loss: 25.4296 - val_accuracy: 0.0375\n",
      "Epoch 141/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9688 - accuracy: 0.2129 - val_loss: 25.7303 - val_accuracy: 0.0357\n",
      "Epoch 142/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9400 - accuracy: 0.2194 - val_loss: 25.6079 - val_accuracy: 0.0401\n",
      "Epoch 143/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9379 - accuracy: 0.2190 - val_loss: 25.7838 - val_accuracy: 0.0331\n",
      "Epoch 144/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9292 - accuracy: 0.2266 - val_loss: 25.8839 - val_accuracy: 0.0401\n",
      "Epoch 145/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.9169 - accuracy: 0.2270 - val_loss: 25.9663 - val_accuracy: 0.0331\n",
      "Epoch 146/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8973 - accuracy: 0.2216 - val_loss: 26.2341 - val_accuracy: 0.0409\n",
      "Epoch 147/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8955 - accuracy: 0.2216 - val_loss: 26.1336 - val_accuracy: 0.0383\n",
      "Epoch 148/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8894 - accuracy: 0.2320 - val_loss: 26.2595 - val_accuracy: 0.0427\n",
      "Epoch 149/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8534 - accuracy: 0.2342 - val_loss: 26.8870 - val_accuracy: 0.0348\n",
      "Epoch 150/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8657 - accuracy: 0.2283 - val_loss: 26.7246 - val_accuracy: 0.0348\n",
      "Epoch 151/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8672 - accuracy: 0.2296 - val_loss: 26.9115 - val_accuracy: 0.0357\n",
      "Epoch 152/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8318 - accuracy: 0.2340 - val_loss: 26.8844 - val_accuracy: 0.0357\n",
      "Epoch 153/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.8421 - accuracy: 0.2305 - val_loss: 26.7499 - val_accuracy: 0.0366\n",
      "Epoch 154/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.8450 - accuracy: 0.2362 - val_loss: 26.6994 - val_accuracy: 0.0366\n",
      "Epoch 155/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7959 - accuracy: 0.2488 - val_loss: 26.9132 - val_accuracy: 0.0279\n",
      "Epoch 156/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.8003 - accuracy: 0.2388 - val_loss: 27.0509 - val_accuracy: 0.0305\n",
      "Epoch 157/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7773 - accuracy: 0.2405 - val_loss: 27.5676 - val_accuracy: 0.0436\n",
      "Epoch 158/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7846 - accuracy: 0.2484 - val_loss: 27.6007 - val_accuracy: 0.0436\n",
      "Epoch 159/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7771 - accuracy: 0.2453 - val_loss: 27.6268 - val_accuracy: 0.0357\n",
      "Epoch 160/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7403 - accuracy: 0.2560 - val_loss: 27.6371 - val_accuracy: 0.0322\n",
      "Epoch 161/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7630 - accuracy: 0.2479 - val_loss: 27.7218 - val_accuracy: 0.0418\n",
      "Epoch 162/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7542 - accuracy: 0.2564 - val_loss: 27.4113 - val_accuracy: 0.0348\n",
      "Epoch 163/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7570 - accuracy: 0.2462 - val_loss: 27.8743 - val_accuracy: 0.0383\n",
      "Epoch 164/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.7151 - accuracy: 0.2603 - val_loss: 27.9147 - val_accuracy: 0.0392\n",
      "Epoch 165/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.7014 - accuracy: 0.2632 - val_loss: 27.7102 - val_accuracy: 0.0322\n",
      "Epoch 166/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6965 - accuracy: 0.2649 - val_loss: 28.0841 - val_accuracy: 0.0331\n",
      "Epoch 167/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6946 - accuracy: 0.2610 - val_loss: 28.0140 - val_accuracy: 0.0366\n",
      "Epoch 168/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6530 - accuracy: 0.2691 - val_loss: 27.9391 - val_accuracy: 0.0331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6697 - accuracy: 0.2756 - val_loss: 28.1508 - val_accuracy: 0.0375\n",
      "Epoch 170/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6729 - accuracy: 0.2625 - val_loss: 28.2774 - val_accuracy: 0.0340\n",
      "Epoch 171/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6691 - accuracy: 0.2717 - val_loss: 28.7865 - val_accuracy: 0.0357\n",
      "Epoch 172/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6288 - accuracy: 0.2706 - val_loss: 28.9057 - val_accuracy: 0.0383\n",
      "Epoch 173/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6237 - accuracy: 0.2765 - val_loss: 28.6615 - val_accuracy: 0.0383\n",
      "Epoch 174/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6277 - accuracy: 0.2771 - val_loss: 28.8848 - val_accuracy: 0.0314\n",
      "Epoch 175/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.6239 - accuracy: 0.2874 - val_loss: 28.5798 - val_accuracy: 0.0392\n",
      "Epoch 176/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.5858 - accuracy: 0.2874 - val_loss: 29.0665 - val_accuracy: 0.0366\n",
      "Epoch 177/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5950 - accuracy: 0.2826 - val_loss: 28.7786 - val_accuracy: 0.0331\n",
      "Epoch 178/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5831 - accuracy: 0.2935 - val_loss: 29.0202 - val_accuracy: 0.0375\n",
      "Epoch 179/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5626 - accuracy: 0.2891 - val_loss: 29.0026 - val_accuracy: 0.0357\n",
      "Epoch 180/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.5371 - accuracy: 0.2965 - val_loss: 29.3305 - val_accuracy: 0.0331\n",
      "Epoch 181/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5528 - accuracy: 0.2904 - val_loss: 29.4599 - val_accuracy: 0.0314\n",
      "Epoch 182/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5400 - accuracy: 0.2906 - val_loss: 29.3709 - val_accuracy: 0.0348\n",
      "Epoch 183/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5469 - accuracy: 0.3000 - val_loss: 29.2804 - val_accuracy: 0.0340\n",
      "Epoch 184/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5342 - accuracy: 0.3024 - val_loss: 29.4209 - val_accuracy: 0.0340\n",
      "Epoch 185/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.5133 - accuracy: 0.3028 - val_loss: 29.3551 - val_accuracy: 0.0322\n",
      "Epoch 186/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5225 - accuracy: 0.3078 - val_loss: 29.4363 - val_accuracy: 0.0418\n",
      "Epoch 187/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.5005 - accuracy: 0.3059 - val_loss: 29.8887 - val_accuracy: 0.0261\n",
      "Epoch 188/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.5036 - accuracy: 0.3011 - val_loss: 30.1805 - val_accuracy: 0.0375\n",
      "Epoch 189/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4708 - accuracy: 0.3126 - val_loss: 29.6310 - val_accuracy: 0.0279\n",
      "Epoch 190/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4722 - accuracy: 0.3070 - val_loss: 30.1102 - val_accuracy: 0.0322\n",
      "Epoch 191/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4678 - accuracy: 0.3111 - val_loss: 30.2849 - val_accuracy: 0.0375\n",
      "Epoch 192/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4590 - accuracy: 0.3148 - val_loss: 30.2303 - val_accuracy: 0.0348\n",
      "Epoch 193/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4461 - accuracy: 0.3159 - val_loss: 30.0306 - val_accuracy: 0.0314\n",
      "Epoch 194/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4223 - accuracy: 0.3161 - val_loss: 30.2764 - val_accuracy: 0.0383\n",
      "Epoch 195/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4275 - accuracy: 0.3231 - val_loss: 30.6058 - val_accuracy: 0.0314\n",
      "Epoch 196/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4077 - accuracy: 0.3298 - val_loss: 30.6718 - val_accuracy: 0.0383\n",
      "Epoch 197/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.4320 - accuracy: 0.3187 - val_loss: 30.5699 - val_accuracy: 0.0340\n",
      "Epoch 198/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3882 - accuracy: 0.3281 - val_loss: 30.9891 - val_accuracy: 0.0305\n",
      "Epoch 199/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3902 - accuracy: 0.3285 - val_loss: 30.8483 - val_accuracy: 0.0348\n",
      "Epoch 200/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3626 - accuracy: 0.3296 - val_loss: 30.9479 - val_accuracy: 0.0322\n",
      "Epoch 201/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.3746 - accuracy: 0.3346 - val_loss: 31.3144 - val_accuracy: 0.0322\n",
      "Epoch 202/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3660 - accuracy: 0.3283 - val_loss: 31.1900 - val_accuracy: 0.0270\n",
      "Epoch 203/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.3462 - accuracy: 0.3386 - val_loss: 31.1353 - val_accuracy: 0.0270\n",
      "Epoch 204/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.3340 - accuracy: 0.3373 - val_loss: 31.4051 - val_accuracy: 0.0348\n",
      "Epoch 205/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3445 - accuracy: 0.3434 - val_loss: 31.5849 - val_accuracy: 0.0279\n",
      "Epoch 206/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.3241 - accuracy: 0.3434 - val_loss: 31.6892 - val_accuracy: 0.0314\n",
      "Epoch 207/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3392 - accuracy: 0.3364 - val_loss: 31.5435 - val_accuracy: 0.0331\n",
      "Epoch 208/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.3368 - accuracy: 0.3373 - val_loss: 32.0000 - val_accuracy: 0.0357\n",
      "Epoch 209/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.2946 - accuracy: 0.3468 - val_loss: 32.3139 - val_accuracy: 0.0322\n",
      "Epoch 210/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.3192 - accuracy: 0.3468 - val_loss: 31.9405 - val_accuracy: 0.0366\n",
      "Epoch 211/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.2896 - accuracy: 0.3490 - val_loss: 32.0793 - val_accuracy: 0.0287\n",
      "Epoch 212/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.2760 - accuracy: 0.3566 - val_loss: 31.9267 - val_accuracy: 0.0279\n",
      "Epoch 213/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.2681 - accuracy: 0.3547 - val_loss: 31.9320 - val_accuracy: 0.0305\n",
      "Epoch 214/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.2894 - accuracy: 0.3558 - val_loss: 32.5447 - val_accuracy: 0.0314\n",
      "Epoch 215/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2523 - accuracy: 0.3625 - val_loss: 32.4687 - val_accuracy: 0.0296\n",
      "Epoch 216/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2365 - accuracy: 0.3678 - val_loss: 32.4662 - val_accuracy: 0.0305\n",
      "Epoch 217/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2217 - accuracy: 0.3749 - val_loss: 32.6467 - val_accuracy: 0.0279\n",
      "Epoch 218/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2088 - accuracy: 0.3682 - val_loss: 32.7674 - val_accuracy: 0.0331\n",
      "Epoch 219/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2216 - accuracy: 0.3651 - val_loss: 32.7197 - val_accuracy: 0.0331\n",
      "Epoch 220/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2255 - accuracy: 0.3638 - val_loss: 32.8860 - val_accuracy: 0.0357\n",
      "Epoch 221/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2028 - accuracy: 0.3756 - val_loss: 32.9050 - val_accuracy: 0.0305\n",
      "Epoch 222/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1697 - accuracy: 0.3756 - val_loss: 33.0174 - val_accuracy: 0.0296\n",
      "Epoch 223/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2077 - accuracy: 0.3793 - val_loss: 33.1960 - val_accuracy: 0.0296\n",
      "Epoch 224/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.2116 - accuracy: 0.3682 - val_loss: 33.1257 - val_accuracy: 0.0244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1689 - accuracy: 0.3874 - val_loss: 33.3046 - val_accuracy: 0.0287\n",
      "Epoch 226/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1756 - accuracy: 0.3824 - val_loss: 33.1428 - val_accuracy: 0.0279\n",
      "Epoch 227/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.1775 - accuracy: 0.3852 - val_loss: 33.4071 - val_accuracy: 0.0253\n",
      "Epoch 228/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.1647 - accuracy: 0.3863 - val_loss: 33.5429 - val_accuracy: 0.0322\n",
      "Epoch 229/500\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.1630 - accuracy: 0.3865 - val_loss: 33.2214 - val_accuracy: 0.0322\n",
      "Epoch 230/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1335 - accuracy: 0.3856 - val_loss: 33.6183 - val_accuracy: 0.0279\n",
      "Epoch 231/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1412 - accuracy: 0.3946 - val_loss: 33.3422 - val_accuracy: 0.0244\n",
      "Epoch 232/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1793 - accuracy: 0.3865 - val_loss: 33.6842 - val_accuracy: 0.0322\n",
      "Epoch 233/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1320 - accuracy: 0.3959 - val_loss: 33.3381 - val_accuracy: 0.0270\n",
      "Epoch 234/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1056 - accuracy: 0.3980 - val_loss: 33.6119 - val_accuracy: 0.0270\n",
      "Epoch 235/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1196 - accuracy: 0.4007 - val_loss: 33.5977 - val_accuracy: 0.0270\n",
      "Epoch 236/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1026 - accuracy: 0.3913 - val_loss: 33.8138 - val_accuracy: 0.0253\n",
      "Epoch 237/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0850 - accuracy: 0.4020 - val_loss: 34.0917 - val_accuracy: 0.0331\n",
      "Epoch 238/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1133 - accuracy: 0.3972 - val_loss: 34.2139 - val_accuracy: 0.0305\n",
      "Epoch 239/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0916 - accuracy: 0.4013 - val_loss: 34.2370 - val_accuracy: 0.0322\n",
      "Epoch 240/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 2.1019 - accuracy: 0.4065 - val_loss: 34.1865 - val_accuracy: 0.0314\n",
      "Epoch 241/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0997 - accuracy: 0.4081 - val_loss: 33.9968 - val_accuracy: 0.0340\n",
      "Epoch 242/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0728 - accuracy: 0.4113 - val_loss: 34.3232 - val_accuracy: 0.0296\n",
      "Epoch 243/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0458 - accuracy: 0.4190 - val_loss: 34.1055 - val_accuracy: 0.0279\n",
      "Epoch 244/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0133 - accuracy: 0.4220 - val_loss: 34.6844 - val_accuracy: 0.0226\n",
      "Epoch 245/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0573 - accuracy: 0.4229 - val_loss: 34.5116 - val_accuracy: 0.0305\n",
      "Epoch 246/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0444 - accuracy: 0.4183 - val_loss: 34.6071 - val_accuracy: 0.0244\n",
      "Epoch 247/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0282 - accuracy: 0.4122 - val_loss: 34.7047 - val_accuracy: 0.0296\n",
      "Epoch 248/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0190 - accuracy: 0.4222 - val_loss: 34.6587 - val_accuracy: 0.0226\n",
      "Epoch 249/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0212 - accuracy: 0.4270 - val_loss: 34.7412 - val_accuracy: 0.0279\n",
      "Epoch 250/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9814 - accuracy: 0.4288 - val_loss: 35.2081 - val_accuracy: 0.0296\n",
      "Epoch 251/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0059 - accuracy: 0.4275 - val_loss: 34.9299 - val_accuracy: 0.0261\n",
      "Epoch 252/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9966 - accuracy: 0.4229 - val_loss: 34.9343 - val_accuracy: 0.0270\n",
      "Epoch 253/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0155 - accuracy: 0.4196 - val_loss: 34.9200 - val_accuracy: 0.0322\n",
      "Epoch 254/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.0058 - accuracy: 0.4179 - val_loss: 34.9594 - val_accuracy: 0.0287\n",
      "Epoch 255/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 2.0009 - accuracy: 0.4292 - val_loss: 35.4452 - val_accuracy: 0.0270\n",
      "Epoch 256/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9780 - accuracy: 0.4318 - val_loss: 34.7750 - val_accuracy: 0.0253\n",
      "Epoch 257/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9574 - accuracy: 0.4364 - val_loss: 35.5529 - val_accuracy: 0.0340\n",
      "Epoch 258/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9503 - accuracy: 0.4405 - val_loss: 35.3642 - val_accuracy: 0.0305\n",
      "Epoch 259/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9401 - accuracy: 0.4429 - val_loss: 35.2642 - val_accuracy: 0.0314\n",
      "Epoch 260/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9572 - accuracy: 0.4397 - val_loss: 35.5166 - val_accuracy: 0.0287\n",
      "Epoch 261/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9396 - accuracy: 0.4475 - val_loss: 35.1717 - val_accuracy: 0.0261\n",
      "Epoch 262/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9486 - accuracy: 0.4373 - val_loss: 35.7308 - val_accuracy: 0.0340\n",
      "Epoch 263/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.9194 - accuracy: 0.4516 - val_loss: 35.9858 - val_accuracy: 0.0348\n",
      "Epoch 264/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9099 - accuracy: 0.4560 - val_loss: 35.9707 - val_accuracy: 0.0322\n",
      "Epoch 265/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8801 - accuracy: 0.4610 - val_loss: 36.0542 - val_accuracy: 0.0253\n",
      "Epoch 266/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9244 - accuracy: 0.4492 - val_loss: 35.6803 - val_accuracy: 0.0296\n",
      "Epoch 267/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.9310 - accuracy: 0.4425 - val_loss: 35.8182 - val_accuracy: 0.0348\n",
      "Epoch 268/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8700 - accuracy: 0.4654 - val_loss: 35.9223 - val_accuracy: 0.0322\n",
      "Epoch 269/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.9024 - accuracy: 0.4486 - val_loss: 35.7823 - val_accuracy: 0.0314\n",
      "Epoch 270/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.8951 - accuracy: 0.4556 - val_loss: 35.9382 - val_accuracy: 0.0296\n",
      "Epoch 271/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.8938 - accuracy: 0.4636 - val_loss: 36.3603 - val_accuracy: 0.0261\n",
      "Epoch 272/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8965 - accuracy: 0.4564 - val_loss: 35.9616 - val_accuracy: 0.0270\n",
      "Epoch 273/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.8625 - accuracy: 0.4612 - val_loss: 36.0789 - val_accuracy: 0.0226\n",
      "Epoch 274/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8658 - accuracy: 0.4641 - val_loss: 35.9880 - val_accuracy: 0.0235\n",
      "Epoch 275/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8830 - accuracy: 0.4656 - val_loss: 36.0593 - val_accuracy: 0.0270\n",
      "Epoch 276/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8264 - accuracy: 0.4741 - val_loss: 36.6624 - val_accuracy: 0.0261\n",
      "Epoch 277/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8262 - accuracy: 0.4660 - val_loss: 36.4581 - val_accuracy: 0.0340\n",
      "Epoch 278/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8351 - accuracy: 0.4760 - val_loss: 36.2656 - val_accuracy: 0.0331\n",
      "Epoch 279/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.8473 - accuracy: 0.4706 - val_loss: 36.4479 - val_accuracy: 0.0287\n",
      "Epoch 280/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8207 - accuracy: 0.4739 - val_loss: 36.4446 - val_accuracy: 0.0322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8035 - accuracy: 0.4710 - val_loss: 36.4178 - val_accuracy: 0.0270\n",
      "Epoch 282/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8043 - accuracy: 0.4739 - val_loss: 36.7900 - val_accuracy: 0.0314\n",
      "Epoch 283/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8274 - accuracy: 0.4632 - val_loss: 36.5045 - val_accuracy: 0.0253\n",
      "Epoch 284/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8255 - accuracy: 0.4712 - val_loss: 36.8747 - val_accuracy: 0.0314\n",
      "Epoch 285/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8211 - accuracy: 0.4747 - val_loss: 36.9071 - val_accuracy: 0.0270\n",
      "Epoch 286/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8007 - accuracy: 0.4730 - val_loss: 36.7979 - val_accuracy: 0.0209\n",
      "Epoch 287/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7812 - accuracy: 0.4946 - val_loss: 37.0101 - val_accuracy: 0.0287\n",
      "Epoch 288/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7862 - accuracy: 0.4885 - val_loss: 37.1027 - val_accuracy: 0.0261\n",
      "Epoch 289/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7702 - accuracy: 0.4895 - val_loss: 37.2135 - val_accuracy: 0.0270\n",
      "Epoch 290/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7861 - accuracy: 0.4841 - val_loss: 37.2136 - val_accuracy: 0.0235\n",
      "Epoch 291/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7654 - accuracy: 0.4948 - val_loss: 37.3988 - val_accuracy: 0.0218\n",
      "Epoch 292/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7754 - accuracy: 0.4815 - val_loss: 37.2258 - val_accuracy: 0.0253\n",
      "Epoch 293/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7797 - accuracy: 0.4950 - val_loss: 37.2020 - val_accuracy: 0.0287\n",
      "Epoch 294/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7727 - accuracy: 0.4915 - val_loss: 37.4824 - val_accuracy: 0.0296\n",
      "Epoch 295/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7276 - accuracy: 0.5059 - val_loss: 37.3438 - val_accuracy: 0.0296\n",
      "Epoch 296/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7652 - accuracy: 0.4967 - val_loss: 37.6054 - val_accuracy: 0.0244\n",
      "Epoch 297/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7265 - accuracy: 0.4969 - val_loss: 37.7018 - val_accuracy: 0.0287\n",
      "Epoch 298/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7287 - accuracy: 0.5083 - val_loss: 37.5599 - val_accuracy: 0.0279\n",
      "Epoch 299/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7189 - accuracy: 0.5072 - val_loss: 37.5559 - val_accuracy: 0.0314\n",
      "Epoch 300/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7306 - accuracy: 0.5041 - val_loss: 37.4218 - val_accuracy: 0.0287\n",
      "Epoch 301/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7365 - accuracy: 0.5033 - val_loss: 37.6087 - val_accuracy: 0.0261\n",
      "Epoch 302/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7285 - accuracy: 0.5000 - val_loss: 37.7669 - val_accuracy: 0.0270\n",
      "Epoch 303/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7149 - accuracy: 0.4980 - val_loss: 37.6439 - val_accuracy: 0.0305\n",
      "Epoch 304/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6828 - accuracy: 0.5155 - val_loss: 37.9600 - val_accuracy: 0.0270\n",
      "Epoch 305/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7004 - accuracy: 0.5070 - val_loss: 37.9193 - val_accuracy: 0.0244\n",
      "Epoch 306/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6837 - accuracy: 0.5002 - val_loss: 37.4153 - val_accuracy: 0.0279\n",
      "Epoch 307/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7080 - accuracy: 0.5122 - val_loss: 37.7951 - val_accuracy: 0.0279\n",
      "Epoch 308/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6516 - accuracy: 0.5163 - val_loss: 37.9525 - val_accuracy: 0.0331\n",
      "Epoch 309/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7076 - accuracy: 0.5092 - val_loss: 38.0771 - val_accuracy: 0.0314\n",
      "Epoch 310/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6803 - accuracy: 0.5163 - val_loss: 37.9626 - val_accuracy: 0.0261\n",
      "Epoch 311/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6749 - accuracy: 0.5157 - val_loss: 37.8495 - val_accuracy: 0.0287\n",
      "Epoch 312/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6576 - accuracy: 0.5218 - val_loss: 38.5248 - val_accuracy: 0.0287\n",
      "Epoch 313/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6733 - accuracy: 0.5074 - val_loss: 38.6707 - val_accuracy: 0.0253\n",
      "Epoch 314/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6739 - accuracy: 0.5214 - val_loss: 37.9315 - val_accuracy: 0.0218\n",
      "Epoch 315/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6788 - accuracy: 0.5200 - val_loss: 38.0926 - val_accuracy: 0.0305\n",
      "Epoch 316/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6463 - accuracy: 0.5285 - val_loss: 38.1727 - val_accuracy: 0.0296\n",
      "Epoch 317/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6481 - accuracy: 0.5277 - val_loss: 38.2046 - val_accuracy: 0.0287\n",
      "Epoch 318/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.6286 - accuracy: 0.5187 - val_loss: 38.4031 - val_accuracy: 0.0253\n",
      "Epoch 319/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.6337 - accuracy: 0.5229 - val_loss: 38.8186 - val_accuracy: 0.0322\n",
      "Epoch 320/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.6190 - accuracy: 0.5329 - val_loss: 38.4165 - val_accuracy: 0.0253\n",
      "Epoch 321/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5989 - accuracy: 0.5381 - val_loss: 38.4691 - val_accuracy: 0.0253\n",
      "Epoch 322/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.6079 - accuracy: 0.5405 - val_loss: 38.8377 - val_accuracy: 0.0261\n",
      "Epoch 323/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.6203 - accuracy: 0.5183 - val_loss: 38.4543 - val_accuracy: 0.0244\n",
      "Epoch 324/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.6230 - accuracy: 0.5242 - val_loss: 38.8456 - val_accuracy: 0.0331\n",
      "Epoch 325/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.6072 - accuracy: 0.5353 - val_loss: 38.9438 - val_accuracy: 0.0235\n",
      "Epoch 326/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.6014 - accuracy: 0.5338 - val_loss: 38.8754 - val_accuracy: 0.0226\n",
      "Epoch 327/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5858 - accuracy: 0.5497 - val_loss: 39.2496 - val_accuracy: 0.0218\n",
      "Epoch 328/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.6143 - accuracy: 0.5346 - val_loss: 39.3855 - val_accuracy: 0.0235\n",
      "Epoch 329/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5655 - accuracy: 0.5508 - val_loss: 38.9855 - val_accuracy: 0.0218\n",
      "Epoch 330/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5850 - accuracy: 0.5381 - val_loss: 39.3969 - val_accuracy: 0.0305\n",
      "Epoch 331/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.6084 - accuracy: 0.5440 - val_loss: 39.3650 - val_accuracy: 0.0218\n",
      "Epoch 332/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5888 - accuracy: 0.5412 - val_loss: 39.1951 - val_accuracy: 0.0244\n",
      "Epoch 333/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5527 - accuracy: 0.5475 - val_loss: 39.4514 - val_accuracy: 0.0287\n",
      "Epoch 334/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.5691 - accuracy: 0.5451 - val_loss: 39.2651 - val_accuracy: 0.0209\n",
      "Epoch 335/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.6063 - accuracy: 0.5366 - val_loss: 39.5299 - val_accuracy: 0.0226\n",
      "Epoch 336/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5605 - accuracy: 0.5468 - val_loss: 39.9370 - val_accuracy: 0.0235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5513 - accuracy: 0.5410 - val_loss: 39.8300 - val_accuracy: 0.0235\n",
      "Epoch 338/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5764 - accuracy: 0.5532 - val_loss: 40.0517 - val_accuracy: 0.0279\n",
      "Epoch 339/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5202 - accuracy: 0.5580 - val_loss: 39.4259 - val_accuracy: 0.0253\n",
      "Epoch 340/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5524 - accuracy: 0.5423 - val_loss: 39.7766 - val_accuracy: 0.0279\n",
      "Epoch 341/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5324 - accuracy: 0.5547 - val_loss: 40.0326 - val_accuracy: 0.0270\n",
      "Epoch 342/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5061 - accuracy: 0.5562 - val_loss: 39.5395 - val_accuracy: 0.0296\n",
      "Epoch 343/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5332 - accuracy: 0.5473 - val_loss: 40.1508 - val_accuracy: 0.0244\n",
      "Epoch 344/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5511 - accuracy: 0.5503 - val_loss: 40.0071 - val_accuracy: 0.0279\n",
      "Epoch 345/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5544 - accuracy: 0.5492 - val_loss: 40.2161 - val_accuracy: 0.0253\n",
      "Epoch 346/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5606 - accuracy: 0.5499 - val_loss: 39.8270 - val_accuracy: 0.0218\n",
      "Epoch 347/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5083 - accuracy: 0.5573 - val_loss: 40.2697 - val_accuracy: 0.0261\n",
      "Epoch 348/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5219 - accuracy: 0.5542 - val_loss: 40.0250 - val_accuracy: 0.0261\n",
      "Epoch 349/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5297 - accuracy: 0.5473 - val_loss: 40.2240 - val_accuracy: 0.0218\n",
      "Epoch 350/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5080 - accuracy: 0.5647 - val_loss: 40.5238 - val_accuracy: 0.0296\n",
      "Epoch 351/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4629 - accuracy: 0.5708 - val_loss: 40.5223 - val_accuracy: 0.0261\n",
      "Epoch 352/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5046 - accuracy: 0.5621 - val_loss: 40.3398 - val_accuracy: 0.0253\n",
      "Epoch 353/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5385 - accuracy: 0.5514 - val_loss: 40.1711 - val_accuracy: 0.0192\n",
      "Epoch 354/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5089 - accuracy: 0.5508 - val_loss: 40.5191 - val_accuracy: 0.0235\n",
      "Epoch 355/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.5017 - accuracy: 0.5647 - val_loss: 40.5147 - val_accuracy: 0.0209\n",
      "Epoch 356/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4894 - accuracy: 0.5627 - val_loss: 40.4222 - val_accuracy: 0.0296\n",
      "Epoch 357/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4799 - accuracy: 0.5684 - val_loss: 40.5338 - val_accuracy: 0.0270\n",
      "Epoch 358/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4727 - accuracy: 0.5732 - val_loss: 40.1368 - val_accuracy: 0.0200\n",
      "Epoch 359/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4718 - accuracy: 0.5706 - val_loss: 40.6724 - val_accuracy: 0.0270\n",
      "Epoch 360/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4995 - accuracy: 0.5619 - val_loss: 40.7782 - val_accuracy: 0.0270\n",
      "Epoch 361/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4821 - accuracy: 0.5627 - val_loss: 41.0097 - val_accuracy: 0.0209\n",
      "Epoch 362/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4411 - accuracy: 0.5826 - val_loss: 40.7536 - val_accuracy: 0.0200\n",
      "Epoch 363/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4596 - accuracy: 0.5712 - val_loss: 40.6783 - val_accuracy: 0.0314\n",
      "Epoch 364/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4259 - accuracy: 0.5858 - val_loss: 41.0767 - val_accuracy: 0.0261\n",
      "Epoch 365/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4370 - accuracy: 0.5769 - val_loss: 41.3772 - val_accuracy: 0.0209\n",
      "Epoch 366/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4505 - accuracy: 0.5715 - val_loss: 40.7633 - val_accuracy: 0.0287\n",
      "Epoch 367/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4261 - accuracy: 0.5843 - val_loss: 40.8932 - val_accuracy: 0.0200\n",
      "Epoch 368/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4791 - accuracy: 0.5673 - val_loss: 41.4449 - val_accuracy: 0.0235\n",
      "Epoch 369/500\n",
      "144/144 [==============================] - 2s 17ms/step - loss: 1.4713 - accuracy: 0.5728 - val_loss: 40.7527 - val_accuracy: 0.0296\n",
      "Epoch 370/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4183 - accuracy: 0.5869 - val_loss: 41.0390 - val_accuracy: 0.0244\n",
      "Epoch 371/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4106 - accuracy: 0.5834 - val_loss: 41.2874 - val_accuracy: 0.0253\n",
      "Epoch 372/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4205 - accuracy: 0.5843 - val_loss: 41.3376 - val_accuracy: 0.0287\n",
      "Epoch 373/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4119 - accuracy: 0.5856 - val_loss: 40.6850 - val_accuracy: 0.0209\n",
      "Epoch 374/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4140 - accuracy: 0.5778 - val_loss: 40.9718 - val_accuracy: 0.0253\n",
      "Epoch 375/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4268 - accuracy: 0.5867 - val_loss: 40.8811 - val_accuracy: 0.0322\n",
      "Epoch 376/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4100 - accuracy: 0.5950 - val_loss: 41.1468 - val_accuracy: 0.0235\n",
      "Epoch 377/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.4214 - accuracy: 0.5832 - val_loss: 41.0419 - val_accuracy: 0.0235\n",
      "Epoch 378/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4207 - accuracy: 0.5898 - val_loss: 40.6593 - val_accuracy: 0.0226\n",
      "Epoch 379/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4141 - accuracy: 0.5898 - val_loss: 41.2439 - val_accuracy: 0.0270\n",
      "Epoch 380/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4048 - accuracy: 0.5976 - val_loss: 41.2533 - val_accuracy: 0.0261\n",
      "Epoch 381/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3870 - accuracy: 0.5915 - val_loss: 40.8228 - val_accuracy: 0.0253\n",
      "Epoch 382/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4069 - accuracy: 0.5937 - val_loss: 41.3375 - val_accuracy: 0.0200\n",
      "Epoch 383/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3864 - accuracy: 0.5959 - val_loss: 41.3085 - val_accuracy: 0.0235\n",
      "Epoch 384/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3904 - accuracy: 0.5967 - val_loss: 41.4229 - val_accuracy: 0.0322\n",
      "Epoch 385/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3661 - accuracy: 0.6061 - val_loss: 40.9495 - val_accuracy: 0.0226\n",
      "Epoch 386/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3644 - accuracy: 0.5902 - val_loss: 41.3701 - val_accuracy: 0.0270\n",
      "Epoch 387/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3817 - accuracy: 0.5952 - val_loss: 41.3007 - val_accuracy: 0.0305\n",
      "Epoch 388/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3463 - accuracy: 0.6081 - val_loss: 41.8723 - val_accuracy: 0.0200\n",
      "Epoch 389/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3620 - accuracy: 0.5974 - val_loss: 41.4422 - val_accuracy: 0.0253\n",
      "Epoch 390/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3564 - accuracy: 0.6037 - val_loss: 41.3869 - val_accuracy: 0.0253\n",
      "Epoch 391/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3648 - accuracy: 0.5928 - val_loss: 41.8597 - val_accuracy: 0.0174\n",
      "Epoch 392/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3707 - accuracy: 0.6017 - val_loss: 41.5425 - val_accuracy: 0.0235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3478 - accuracy: 0.6041 - val_loss: 41.5137 - val_accuracy: 0.0253\n",
      "Epoch 394/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3756 - accuracy: 0.5943 - val_loss: 41.6282 - val_accuracy: 0.0296\n",
      "Epoch 395/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3580 - accuracy: 0.6020 - val_loss: 41.7060 - val_accuracy: 0.0253\n",
      "Epoch 396/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3440 - accuracy: 0.6100 - val_loss: 42.3603 - val_accuracy: 0.0244\n",
      "Epoch 397/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3132 - accuracy: 0.6129 - val_loss: 41.9299 - val_accuracy: 0.0261\n",
      "Epoch 398/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3491 - accuracy: 0.6031 - val_loss: 41.8159 - val_accuracy: 0.0244\n",
      "Epoch 399/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3525 - accuracy: 0.6048 - val_loss: 41.9650 - val_accuracy: 0.0296\n",
      "Epoch 400/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3526 - accuracy: 0.5963 - val_loss: 41.8239 - val_accuracy: 0.0226\n",
      "Epoch 401/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3388 - accuracy: 0.6081 - val_loss: 41.6803 - val_accuracy: 0.0235\n",
      "Epoch 402/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3505 - accuracy: 0.6059 - val_loss: 41.8984 - val_accuracy: 0.0226\n",
      "Epoch 403/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3045 - accuracy: 0.6148 - val_loss: 42.1463 - val_accuracy: 0.0261\n",
      "Epoch 404/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3434 - accuracy: 0.6092 - val_loss: 42.1895 - val_accuracy: 0.0244\n",
      "Epoch 405/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3085 - accuracy: 0.6098 - val_loss: 41.9395 - val_accuracy: 0.0287\n",
      "Epoch 406/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3201 - accuracy: 0.6139 - val_loss: 42.0295 - val_accuracy: 0.0296\n",
      "Epoch 407/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3431 - accuracy: 0.6098 - val_loss: 41.7843 - val_accuracy: 0.0296\n",
      "Epoch 408/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3646 - accuracy: 0.6015 - val_loss: 42.1349 - val_accuracy: 0.0218\n",
      "Epoch 409/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3449 - accuracy: 0.6085 - val_loss: 41.9387 - val_accuracy: 0.0253\n",
      "Epoch 410/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3434 - accuracy: 0.6098 - val_loss: 42.1555 - val_accuracy: 0.0209\n",
      "Epoch 411/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2765 - accuracy: 0.6277 - val_loss: 41.8802 - val_accuracy: 0.0218\n",
      "Epoch 412/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2925 - accuracy: 0.6253 - val_loss: 41.7447 - val_accuracy: 0.0209\n",
      "Epoch 413/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3082 - accuracy: 0.6153 - val_loss: 42.1388 - val_accuracy: 0.0279\n",
      "Epoch 414/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2647 - accuracy: 0.6246 - val_loss: 42.0002 - val_accuracy: 0.0218\n",
      "Epoch 415/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2655 - accuracy: 0.6264 - val_loss: 42.5775 - val_accuracy: 0.0270\n",
      "Epoch 416/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3209 - accuracy: 0.6065 - val_loss: 42.1411 - val_accuracy: 0.0270\n",
      "Epoch 417/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3119 - accuracy: 0.6181 - val_loss: 42.6275 - val_accuracy: 0.0235\n",
      "Epoch 418/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3007 - accuracy: 0.6194 - val_loss: 42.4491 - val_accuracy: 0.0244\n",
      "Epoch 419/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2878 - accuracy: 0.6133 - val_loss: 42.7844 - val_accuracy: 0.0253\n",
      "Epoch 420/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2768 - accuracy: 0.6224 - val_loss: 42.6809 - val_accuracy: 0.0261\n",
      "Epoch 421/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2958 - accuracy: 0.6198 - val_loss: 43.0376 - val_accuracy: 0.0261\n",
      "Epoch 422/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2786 - accuracy: 0.6290 - val_loss: 42.4626 - val_accuracy: 0.0270\n",
      "Epoch 423/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2856 - accuracy: 0.6325 - val_loss: 42.7127 - val_accuracy: 0.0235\n",
      "Epoch 424/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2718 - accuracy: 0.6320 - val_loss: 42.8467 - val_accuracy: 0.0279\n",
      "Epoch 425/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2735 - accuracy: 0.6194 - val_loss: 42.6422 - val_accuracy: 0.0226\n",
      "Epoch 426/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.3110 - accuracy: 0.6174 - val_loss: 42.8842 - val_accuracy: 0.0209\n",
      "Epoch 427/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2445 - accuracy: 0.6305 - val_loss: 42.9440 - val_accuracy: 0.0261\n",
      "Epoch 428/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2950 - accuracy: 0.6133 - val_loss: 42.7034 - val_accuracy: 0.0287\n",
      "Epoch 429/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2437 - accuracy: 0.6294 - val_loss: 42.9095 - val_accuracy: 0.0287\n",
      "Epoch 430/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2677 - accuracy: 0.6264 - val_loss: 43.0499 - val_accuracy: 0.0270\n",
      "Epoch 431/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2362 - accuracy: 0.6322 - val_loss: 43.1170 - val_accuracy: 0.0261\n",
      "Epoch 432/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.2387 - accuracy: 0.6355 - val_loss: 43.0436 - val_accuracy: 0.0287\n",
      "Epoch 433/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.2516 - accuracy: 0.6336 - val_loss: 43.0305 - val_accuracy: 0.0235\n",
      "Epoch 434/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2509 - accuracy: 0.6298 - val_loss: 42.7674 - val_accuracy: 0.0270\n",
      "Epoch 435/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1918 - accuracy: 0.6497 - val_loss: 42.7329 - val_accuracy: 0.0261\n",
      "Epoch 436/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.2493 - accuracy: 0.6331 - val_loss: 43.0511 - val_accuracy: 0.0270\n",
      "Epoch 437/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2354 - accuracy: 0.6362 - val_loss: 43.3092 - val_accuracy: 0.0279\n",
      "Epoch 438/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2414 - accuracy: 0.6353 - val_loss: 43.0937 - val_accuracy: 0.0209\n",
      "Epoch 439/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2333 - accuracy: 0.6338 - val_loss: 43.5782 - val_accuracy: 0.0253\n",
      "Epoch 440/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2416 - accuracy: 0.6444 - val_loss: 43.1365 - val_accuracy: 0.0235\n",
      "Epoch 441/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2172 - accuracy: 0.6386 - val_loss: 43.0978 - val_accuracy: 0.0270\n",
      "Epoch 442/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2336 - accuracy: 0.6325 - val_loss: 43.6184 - val_accuracy: 0.0200\n",
      "Epoch 443/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2256 - accuracy: 0.6403 - val_loss: 43.1584 - val_accuracy: 0.0235\n",
      "Epoch 444/500\n",
      "144/144 [==============================] - 2s 16ms/step - loss: 1.2377 - accuracy: 0.6405 - val_loss: 43.2743 - val_accuracy: 0.0261\n",
      "Epoch 445/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1993 - accuracy: 0.6453 - val_loss: 43.1664 - val_accuracy: 0.0244\n",
      "Epoch 446/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1971 - accuracy: 0.6442 - val_loss: 43.4052 - val_accuracy: 0.0261\n",
      "Epoch 447/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2147 - accuracy: 0.6436 - val_loss: 43.3891 - val_accuracy: 0.0261\n",
      "Epoch 448/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1840 - accuracy: 0.6503 - val_loss: 43.3104 - val_accuracy: 0.0331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1996 - accuracy: 0.6434 - val_loss: 43.3488 - val_accuracy: 0.0261\n",
      "Epoch 450/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1923 - accuracy: 0.6525 - val_loss: 43.1811 - val_accuracy: 0.0244\n",
      "Epoch 451/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1934 - accuracy: 0.6410 - val_loss: 43.6225 - val_accuracy: 0.0261\n",
      "Epoch 452/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2428 - accuracy: 0.6336 - val_loss: 44.0825 - val_accuracy: 0.0296\n",
      "Epoch 453/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1917 - accuracy: 0.6547 - val_loss: 43.4728 - val_accuracy: 0.0279\n",
      "Epoch 454/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1943 - accuracy: 0.6475 - val_loss: 43.2811 - val_accuracy: 0.0253\n",
      "Epoch 455/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2399 - accuracy: 0.6301 - val_loss: 43.3924 - val_accuracy: 0.0270\n",
      "Epoch 456/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1814 - accuracy: 0.6540 - val_loss: 43.5651 - val_accuracy: 0.0279\n",
      "Epoch 457/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1901 - accuracy: 0.6468 - val_loss: 43.6809 - val_accuracy: 0.0253\n",
      "Epoch 458/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1648 - accuracy: 0.6545 - val_loss: 43.6130 - val_accuracy: 0.0261\n",
      "Epoch 459/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1806 - accuracy: 0.6529 - val_loss: 43.4103 - val_accuracy: 0.0261\n",
      "Epoch 460/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2093 - accuracy: 0.6458 - val_loss: 43.8662 - val_accuracy: 0.0279\n",
      "Epoch 461/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1899 - accuracy: 0.6488 - val_loss: 43.7012 - val_accuracy: 0.0226\n",
      "Epoch 462/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1666 - accuracy: 0.6536 - val_loss: 43.9133 - val_accuracy: 0.0287\n",
      "Epoch 463/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1968 - accuracy: 0.6534 - val_loss: 43.5184 - val_accuracy: 0.0209\n",
      "Epoch 464/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2084 - accuracy: 0.6471 - val_loss: 43.8288 - val_accuracy: 0.0261\n",
      "Epoch 465/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1651 - accuracy: 0.6466 - val_loss: 43.8763 - val_accuracy: 0.0226\n",
      "Epoch 466/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1428 - accuracy: 0.6582 - val_loss: 43.8248 - val_accuracy: 0.0244\n",
      "Epoch 467/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1689 - accuracy: 0.6521 - val_loss: 43.7415 - val_accuracy: 0.0261\n",
      "Epoch 468/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1895 - accuracy: 0.6590 - val_loss: 43.7783 - val_accuracy: 0.0279\n",
      "Epoch 469/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1631 - accuracy: 0.6582 - val_loss: 44.5428 - val_accuracy: 0.0287\n",
      "Epoch 470/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1667 - accuracy: 0.6510 - val_loss: 44.1920 - val_accuracy: 0.0296\n",
      "Epoch 471/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1575 - accuracy: 0.6595 - val_loss: 43.9513 - val_accuracy: 0.0244\n",
      "Epoch 472/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1540 - accuracy: 0.6586 - val_loss: 43.9020 - val_accuracy: 0.0253\n",
      "Epoch 473/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1310 - accuracy: 0.6575 - val_loss: 44.1584 - val_accuracy: 0.0226\n",
      "Epoch 474/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1557 - accuracy: 0.6584 - val_loss: 44.4490 - val_accuracy: 0.0226\n",
      "Epoch 475/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1357 - accuracy: 0.6736 - val_loss: 44.0788 - val_accuracy: 0.0287\n",
      "Epoch 476/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1370 - accuracy: 0.6630 - val_loss: 44.1372 - val_accuracy: 0.0235\n",
      "Epoch 477/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1372 - accuracy: 0.6623 - val_loss: 43.7056 - val_accuracy: 0.0244\n",
      "Epoch 478/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1469 - accuracy: 0.6575 - val_loss: 43.9505 - val_accuracy: 0.0270\n",
      "Epoch 479/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1215 - accuracy: 0.6586 - val_loss: 44.4343 - val_accuracy: 0.0253\n",
      "Epoch 480/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1501 - accuracy: 0.6614 - val_loss: 44.1025 - val_accuracy: 0.0253\n",
      "Epoch 481/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1470 - accuracy: 0.6712 - val_loss: 44.2865 - val_accuracy: 0.0235\n",
      "Epoch 482/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1417 - accuracy: 0.6601 - val_loss: 44.1377 - val_accuracy: 0.0253\n",
      "Epoch 483/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1246 - accuracy: 0.6721 - val_loss: 44.6440 - val_accuracy: 0.0244\n",
      "Epoch 484/500\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.1148 - accuracy: 0.6786 - val_loss: 44.1426 - val_accuracy: 0.0279\n",
      "Epoch 485/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1018 - accuracy: 0.6684 - val_loss: 44.4832 - val_accuracy: 0.0235\n",
      "Epoch 486/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1660 - accuracy: 0.6560 - val_loss: 44.0883 - val_accuracy: 0.0235\n",
      "Epoch 487/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1126 - accuracy: 0.6741 - val_loss: 44.5300 - val_accuracy: 0.0279\n",
      "Epoch 488/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1330 - accuracy: 0.6638 - val_loss: 44.5043 - val_accuracy: 0.0244\n",
      "Epoch 489/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1079 - accuracy: 0.6658 - val_loss: 44.5817 - val_accuracy: 0.0244\n",
      "Epoch 490/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1329 - accuracy: 0.6697 - val_loss: 44.4108 - val_accuracy: 0.0270\n",
      "Epoch 491/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.0881 - accuracy: 0.6732 - val_loss: 44.5048 - val_accuracy: 0.0218\n",
      "Epoch 492/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1148 - accuracy: 0.6649 - val_loss: 44.3714 - val_accuracy: 0.0218\n",
      "Epoch 493/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1152 - accuracy: 0.6730 - val_loss: 44.8599 - val_accuracy: 0.0287\n",
      "Epoch 494/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1052 - accuracy: 0.6791 - val_loss: 44.1250 - val_accuracy: 0.0287\n",
      "Epoch 495/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1221 - accuracy: 0.6678 - val_loss: 44.8186 - val_accuracy: 0.0200\n",
      "Epoch 496/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1255 - accuracy: 0.6621 - val_loss: 44.8077 - val_accuracy: 0.0244\n",
      "Epoch 497/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.0969 - accuracy: 0.6743 - val_loss: 44.5540 - val_accuracy: 0.0244\n",
      "Epoch 498/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.0920 - accuracy: 0.6834 - val_loss: 44.7094 - val_accuracy: 0.0287\n",
      "Epoch 499/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.1016 - accuracy: 0.6802 - val_loss: 44.8916 - val_accuracy: 0.0314\n",
      "Epoch 500/500\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.0978 - accuracy: 0.6747 - val_loss: 45.2536 - val_accuracy: 0.0253\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_data, y_data, epochs=500, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "q_orBXOrCsNn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3D0lEQVR4nO3dd3hUVfrA8e9JJ4V0EkghAUIJHUJogoKCqAgqIGBfC+rKqmvZxS2u66o/V13r2lBxdV1BxMYqLgqIWGihSidAIAXSGyE95/fHmUwKAQZIMpnJ+3kensw9986d94bJe88995xzldYaIYQQjs/F3gEIIYRoHpLQhRDCSUhCF0IIJyEJXQghnIQkdCGEcBJu9vrgkJAQHRMTY6+PF0IIh7Rp06YcrXVoU+vsltBjYmJISkqy18cLIYRDUkodPtU6m5pclFKTlFJ7lVLJSql5Tax/QSm11fJvn1Kq4DziFUIIcQ7OWENXSrkCrwITgDRgo1JqqdZ6V+02Wuvf1tv+N8DgFohVCCHEadhSQ08EkrXWB7XWFcAiYOpptp8NLGyO4IQQQtjOljb0CCC13nIaMLypDZVSXYFYYNUp1s8B5gBER0eftL6yspK0tDTKyspsCMuxeXl5ERkZibu7u71DEUI4iea+KToLWKK1rm5qpdZ6PjAfICEh4aRJZNLS0vDz8yMmJgalVDOH1nZorcnNzSUtLY3Y2Fh7hyOEcBK2NLmkA1H1liMtZU2ZxXk0t5SVlREcHOzUyRxAKUVwcHC7uBIRQrQeWxL6RiBOKRWrlPLAJO2ljTdSSvUGAoG15xOQsyfzWu3lOIUQreeMCV1rXQXMBZYDu4HFWuudSqnHlVJT6m06C1ikZT5eIYQ4yYmKKv6z/jAVVTUt9hk29UPXWi/TWvfUWnfXWj9pKXtUa7203jaPaa1P6qPuSAoKCnjttdfO+n2XX345BQUFzR+QEMJhVNdomqrPFpVVUlxWyeKNqfzxsx08983eFotB5nKp51QJvaqq6rTvW7ZsGQEBAS0UlRDCESQ+uYIHF287qTzhbyuY9OIP7MwoAmDRhiOUVTbZb+S8SUKvZ968eRw4cIBBgwYxbNgwxowZw5QpU4iPjwfgqquuYujQofTt25f58+db3xcTE0NOTg4pKSn06dOHO+64g759+zJx4kRKS0vtdThCiBZQeKKS11YnU11TVxtPzjpObkkFn25J56llu9mRXsjh3BIWbjhCRXUN6QWlbD6Sj4ebC0VlVazYndkisdltLpcz+et/d7LLckZrLvFdOvKXK/uecv3TTz/Njh072Lp1K6tXr+aKK65gx44d1q6FCxYsICgoiNLSUoYNG8a0adMIDg5usI/9+/ezcOFC3nrrLa699lo++eQTbrjhhmY9DiGE/Tz3zV7+ve4w3UJ8mdQvnLd/OMgTX+22rp+/5iDv/ZxC91Bfdh2ty2EHsku4d3wPMgrL6Ozv1SKxtdmE3hYkJiY26Cf+8ssv89lnnwGQmprK/v37T0rosbGxDBo0CIChQ4eSkpLSWuEKIVpBSblpgk0vMFff9ZN5rfKqmgbJvFbvzh15YGKvFoutzSb009WkW4uPj4/19erVq1mxYgVr167F29ubiy66qMl+5J6entbXrq6u0uQihJMptbR/L995jBkJkdZyd1dFZXVdM0xcJ1/2Zx2ns78XRwtNrugZ5tuisbXZhG4Pfn5+FBcXN7musLCQwMBAvL292bNnD+vWrWvl6IQQ9lBcVkleSQWV1ZoTFVUcyTsBwIZDecx5v24K8LCOXqTlmwpcYkwQL8waxO6MIuK7dOS295LYfbSIrsE+TX5Gc5GEXk9wcDCjR4+mX79+dOjQgbCwMOu6SZMm8cYbb9CnTx969erFiBEj7BipEKI13PLuBlbvzT6p/MYRXUk6nM+6g3nEBHvTwcONP1/Rh+veXg/AR3eOQClFREAHABbfOYKMgjLcXVu2H4ok9EY+/PDDJss9PT35+uuvm1xX204eEhLCjh07rOUPPfRQs8cnhDh/hScqyS0p560fDnK8vJouAV5sPJRHqJ8nj1zWhzfXHCSha+BJyXxMXAjdQ325Z1wP0vJP8MXWDO68sBud/U3iHtU9mJ8P5J40EtzPy51e4S0/EZ8kdCGEU9mRXojW0D/Sv8n1H208wu8/+eWU71++03QpXLjhyEnr7rs4joSYIABC/TwZHB3YYP37tyZSbcfB8pLQhRBOZfIrPwKQ8vQVDcrLKqspKqs8bTKv5enmQrlliP7fp/VnbM9QPtqYelICb8zN1cWuSVUGFgkhnNYrK/cz75PtADz48TYSn1x52u1vu8B0U1469wIu7x/OwKgArhocQWf/Dtx/SU9cXdr2pHpSQxdCOI36c6mcqKjii20ZHMg+jouL4qvtRwEYEOlP/okKUvMadin+922JjOwWzNxxPQj08eC164e2auzNQRK6EMIplFZUszezrtvx/y3bQ3LWcQA+XH+EEF8Plt8/lmBfM1YkZt5XDd4f7OOJm6sLgT4erRd0M5OELoRwOD8l5xDs60Hv8I6AqZnP+XcSP+zPsW7z73WHG7zn2RkDrckcIDrI29qnHCDE13ETeS1pQz8Pvr5m1FdGRgbTp09vcpuLLrqIpKSkJtcJIc7eT8k5XP/2eia9+AMfbTzCX77YwZ8+39EgmQPcfkEss4ZFcccY0y6eaOmdUmvBLQlc3j/cuuzINfNaUkNvBl26dGHJkiX2DkMIp7ExJY/fLdnOP68bzObD+QyLDbLWxt/+4aB1u8Y9Vt791TBeXLGfD25LxM/L9PvWWvPAhF508HBtsG2PTn68dv1QVuzKZMXuzBYf9NMaJKHXM2/ePKKiorjnnnsAeOyxx3Bzc+O7774jPz+fyspKnnjiCaZOndrgfSkpKUyePJkdO3ZQWlrKr371K7Zt20bv3r1lLhchzsFPyTkcyinhipdNF8QrB3bhldmD2Xwkn+/2ZlsH8NR314XdGderE+N6dWpQrpQ6KZnXd0l8GJfEh51yvSNpuwn963lw7Mz9Rc9KeH+47OlTrp45cyb333+/NaEvXryY5cuXc++999KxY0dycnIYMWIEU6ZMOeUzQV9//XW8vb3ZvXs327dvZ8iQIc17DEK0A7nHKxosZxeXsXDDER751OSEKQO7nJTQ513Wu9Xia6vabkK3g8GDB5OVlUVGRgbZ2dkEBgYSHh7Ob3/7W9asWYOLiwvp6elkZmYSHh7e5D7WrFnDvffeC8CAAQMYMGBAax6CEA7peHkVvp516SijoOGV7bqDeaw7mGddHte7E91DfTiQXdJqMTqCtpvQT1OTbkkzZsxgyZIlHDt2jJkzZ/Kf//yH7OxsNm3ahLu7OzExMU1OmyuEsN2nm9OI79KR3uEd2ZiSx4w31nLn2G5sSS1gwS3DyCgso4u/FxmFJ/+trXrwQsI6erHkrlG8sGIf76893OYH/LQWx78L0MxmzpzJokWLWLJkCTNmzKCwsJBOnTrh7u7Od999x+HDh0/7/rFjx1on+NqxYwfbt29vjbCFaJO01pRWNHx+5srdmTyweBsPf7wdrbV1wM+baw6y4VAe/1yVzJ5jRYzv04kffjeOX42OAWB4bBDXDI4gNsRMQRvo48Ffp/RlxtBIPrx9eKseV1tlU0JXSk1SSu1VSiUrpeadYptrlVK7lFI7lVJNT1noAPr27UtxcTERERF07tyZ66+/nqSkJPr378/7779P796nb6e7++67OX78OH369OHRRx9l6FDHG20mxPmqqKrhnR8P8fYPh+jz6P/ILi63rluclArA0cIy3vrhIP/6OQWAjl6mweCN7w+gNVzcO4yoIG+Ol5knBE0dFMHzMwc1uH+llOLZGQMZ3q3hk8PaK6XPMDOYUsoV2AdMANKAjcBsrfWuetvEAYuB8VrrfKVUJ6111un2m5CQoBv3z969ezd9+vQ5pwNxRO3teEX7MX/NAZ5atqdB2f2XxLFkU5r1IRD1PTSxJ/eM68EL3+7j5VXJjO/diQW3DAMgNe8Ef/3vLl6YOdDaFbE9U0pt0lonNLXOlhp6IpCstT6ota4AFgFTG21zB/Cq1jof4EzJXAjh3PZnHj+p7MUV+63J/KGJPa3lw2ICmTs+DqWUdWra8b3ruh5GBXnz9s0JksxtYEtCjwBS6y2nWcrq6wn0VEr9pJRap5Sa1NSOlFJzlFJJSqmk7OyTnwIihHA8BScqyCupYFtqAbGPfMXK3Zms3HP6Ot3MYdG8d2sigHWaWoCxPUP58jcXcP3w6BaN2Vk1Vy8XNyAOuAiIBNYopfprrQvqb6S1ng/MB9Pk0tSOtNan7OPtTM7U1CWEo0h8ciWVNTXMGhaF1nDbe0kNuiCOiQs5aVh+qJ8nw72CuKhXKPdf0rPBun4RTT+YQpyZLQk9HYiqtxxpKasvDVivta4EDiml9mES/MazCcbLy4vc3FyCg4OdOqlrrcnNzcXLy8veoQhxXsqrqqmoNjXshRvMhbyfpxsL54zgyn/+iNbw9LQBrD+Yy+q92fx6XHfre73cXfnXrxLtErezsiWhbwTilFKxmEQ+C7iu0TafA7OBd5VSIZgmmIOcpcjISNLS0mgPzTFeXl5ERkbaOwwhbLJkUxpj40II9fPkl/RCnv56D3PH92DDobwG2z15dT8mxocT6ufJf24bTnL2cSICOnDNkEiuGSLf95Z2xoSuta5SSs0FlgOuwAKt9U6l1ONAktZ6qWXdRKXULqAaeFhrnXvqvTbN3d2d2NjYs32bEKKFlFVW849v9vLWD4esZX27dGRnRtFJQ+97h/tx/fCu1uVRPUIY1SOk1WIVNnRbbClNdVsUQrQt7/x4iL99uavJdQldA0k6nA/AzIQo7rywG91CfVszvHbpfLstCiHaqarqmibLR3YL5gPL6MxBUQH8ffoASeZtQNudy0UI0WpqajQu9eZDWbjhCJlFZVRVN30FHxPig5e7K1/fN4bO/nJzv62QhC5EO1deVU2vP/2PByf05DcXx1FaUW2dprZ2OH5jAyJN18I+nTu2WpzizKTJRYh27kdLH/F/fLuP0opqXl+dbF1XVFZF12Bv1j4yni1/nkCI5Zmcl/fvbJdYxelJDV2Idu67vWZUp7ur4vlvTY+WIB8PBkb6893ebHqG+dHZvwMAn949iqOFpfh3kGH4bZHU0IVoB7TWHMg+eX6V6hpNap6ZX6WyWrPIMjjoqav7M7FvOC7KPB2oVnSwt8xs2IZJDV2IduDNNQd5+us93HVhd8qrqvnzFfEs23GUBz7aRmVNDb3C/NibWUxxeRWT+oYzqZ95ItesYVFOPWrb2UhCF8KJaK0pr6rBy73hQ5FfWrEfMHONA8xOjObD9Uesw/ZHdg/mcF4JZZU1xIb6WN8nydyxSJOLEE5i2S9HGfy3b+n95/+RVVTGiYoqtNaUVVZTWtnwqUETX1jTYKRniK8HQ6IDAYjrJP3JHZXU0IVwEr/+z2br63WH8vjbl7vo5OfJzoyiJrcfExfCkOhAXlq5H39vD968cShJKfmMluH6DksSuhAOrKZGU6M1bq4NL7bvXbgFoMGj32otnTuagA4eRAd7o7VmUFQAY+JCcHN1YVy9B0sIxyMJXQgHVVFVw+Uv/0CIrwcL7xhxxu0fnNCTrOJyBkQGWMuUUpLEnYgkdCEc1Odb0knOOk5yFizdltFgXbcQHxbNGUHiUysB+P7hi+ga7NPUboQTkYQuhINanFT3ZMj7Fm21vr5pZFf+PDked1cXPrx9ONVaSzJvJyShC+FASsqrWLjhCON7d2JLagEB3u4UnKgEYHSPYN68MYEO7q64WibakvnI2xfptiiEA/l0cxpPfLWb8f/4nuoazcW9w6zr7hnXA19PN2syF+2PJHQhHMD6g7ms2JXJSyuTG5Rf3Kfuhma8zHzY7kmTixBtXE2NZub8ddZlP083isurAOjXxd9aHuDt0eqxibZFEroQbczPB3JIyy9lRGwwd36wid1HGw4M+vTXo/jj5zt4cEJPwvw9CfLx4KGJvewUrWhLJKEL0cZc99Z6ACbGh52UzMf2DCUuzI/Fd460lm3+84RWjU+0XdKGLoSdlVVW8/7aFLKKyhqUf7Mr0/r6b1P7AiC3O8Xp2JTQlVKTlFJ7lVLJSql5Tay/RSmVrZTaavl3e/OHKoRzKaus5q5/b+KVVft59IudXPyP78k93nCo/pi4EB6c0JPEWDMH+bheofYIVTiIMza5KKVcgVeBCUAasFEptVRrvavRph9pree2QIxCOKUtRwr4385jsNMsF5dXMfrvqxpsc/uYblzY0yTxH343jsjADq0dpnAgttTQE4FkrfVBrXUFsAiY2rJhCeH8MgpKra/7R/jz0ZwRlFXWNNimV5if9XVUkLfMTy5Oy5aEHgGk1ltOs5Q1Nk0ptV0ptUQpFdXUjpRSc5RSSUqppOzs7HMIVwjH9f2+bPJKKkjJKeHTzWn84bNfrOv6RXRkeLdgnr6mP35ebiy7dwwf3zWScH8vO0YsHE1z9XL5L7BQa12ulLoTeA8Y33gjrfV8YD5AQkKCbqbPFqLNK6us5uYFG4gO8iazqIzyqrqa+D9mDORCS9v4rMRoZspj38Q5sqWGng7Ur3FHWsqstNa5WuvauzlvA0ObJzwhnENWkfnzOJJ3okEyv2VUDNOGRhLi62ktk2QuzpUtCX0jEKeUilVKeQCzgKX1N1BKda63OAXY3XwhCuH4so+XNVn+2JS+rRyJcGZnTOha6ypgLrAck6gXa613KqUeV0pNsWx2r1Jqp1JqG3AvcEtLBSyEI6p9ctATV/WzcyTCmdnUhq61XgYsa1T2aL3XjwCPNG9oQjiPLEtCv7RvOFcO6MIt/9rA7GHRdo5KOBsZ+i9EM0rNO8HSbRl8s/MYT17dH1cXRXhHL55aZlohg3w8cHVRfPbr0XaOVDgjSehCNKMxz3xnfT35lR9PWi9zlYuWJHO5CNFMSiuqT7muV5gfH94xvBWjEe2RJHQhztGKXZk8/81eco+Xk19SwUcbj1jXXdInjH1PXEZEgBmqf/dF3RnVXR4HJ1qWNLkIcZbySir4MTmHexduAeDlVeYpQi4KEroGcuXALlw1KAIPNxcSY4P4bEs6I7oF2zNk0U5IQhfiLD3zvz0s2ph6Urm3hxv/ujURX8+6P6snr+7HTSO7yhB+0SqkyUWIs5B7vJxPt6Q3uW7msKgGyRxMkh8cHdgaoQkhNXQhzsaijalUVDWcETHA251XZg+WZhVhd1JDF8JGaw/k8uH6I/SL6Egnv7q5V0b3CGFMXCjurvLnJOxLauhC2KCkvIrZb60DYGjXQPw7uJNVXE6QjwdzxnSzc3RCGFKlEOIM1h3Mpe9flluXY0J8uKhnJwAemNCTgVEBdopMiIakhi7EaVTXaB759JcGZb6ervxqdAy+Xm5cM6SpZ70IYR9SQxfiNJbvPMahnBKemT6ARyfHA3BBj1DcXF2YnRiNp5urnSMUoo7U0IU4hYqqGl5ZlUy3EB+mDYnE1UVx6wWx9g5LiFOShC6ERU2N5m9f7WLLkQKemzGAlbuz2H20iDduGCqTagmHIAldtHtlldVUVtfwU3IO7/6UAsBf/7sLTzdXuof6MKlfuH0DFMJGktBFu7Yzo5BZ89cR4O1OREAHIgM7MDsxmmeX7wVg8oDOZ9iDEG2H3BQV7dqfPt9BcVkVqXmlrDuYxyV9wrhlVAyB3u4A9A73s3OEQthOErpot7KLy9lypICHJva0lg2M8sfH043Fd47kkj5hXDGgix0jFOLsSEIX7db3+7IBGNszlNE9zDws/SP8AYgL8+PtmxOIDfGxW3xCnC2bErpSapJSaq9SKlkpNe80201TSmmlVELzhShE8zuSe4IXvt1H91Af+nbx59XrhvDCzIH06CRNLMJxnTGhK6VcgVeBy4B4YLZSKr6J7fyA+4D1zR2kEM3tgcVbKSyt5LkZA3F1UQR4e3D14Eh7hyXEebGll0sikKy1PgiglFoETAV2Ndrub8DfgYebNUIhmsmqPZks3ZrBsl+OUVFdwyOX9Za5yoVTsaXJJQKo/3iWNEuZlVJqCBCltf7qdDtSSs1RSiUppZKys7PPOlghztUP+7O54/1NfL41g4pqM5/59KFSIxfO5bz7oSulXIDngVvOtK3Wej4wHyAhIUGf72cLcSYl5VX8/pPtfLn9KG71Rns+fGkvgn09T/NOIRyPLTX0dCCq3nKkpayWH9APWK2USgFGAEvlxqhoC1bszuTL7UcBuHKg6YI4vncn7hnXw55hCdEibKmhbwTilFKxmEQ+C7iudqXWuhAIqV1WSq0GHtJaJzVvqELY5out6QR6e+Dj6cqLK/Zby++6sDv+Hdy5TSbYEk7qjAlda12llJoLLAdcgQVa651KqceBJK310pYOUoizcd+irU2W9wzz5bEpfVs3GCFakU1t6FrrZcCyRmWPnmLbi84/LCHOTWreCevrqwZ14dphUXTx70C4vxdKyYyJwrnJ5FzCafy4P4cb3jHDIK4ZEsGz0wfKtLeiXZGELhzaM//bQ/6JSmYkRFqTOcClfcMlmYt2RxK6cFhV1TW8tvoAAGEdG3ZBDO/oZY+QhLArmZxLOKytqQXW1y+u2E+IrwcDIs3kWmGS0EU7JDV04bA+2ZyOp5sL0UHe7M86TlSQN2/eMJSVe7II95eELtofqaELh1RRVcOX2zK4YkBn/nVrIpP6hnPjiK506ujF7MRoe4cnhF1IDV04nC1H8vl2VybF5VVc0b8zEQEdeOPGofYOSwi7k4QuHM6013+mRoOvpxuje4Sc+Q1CtBPS5CIcSmreCWo0hPp5svjOkXi5u9o7JCHaDKmhC4ex5Ug+jy3dCcBHc0bQLdTXzhEJ0bZIQhdtmtaad348RHlVDVtTC9iWVsifJ8dLMheiCZLQRZu2+Ug+T3y127p89eAImS1RiFOQNnTRZu1IL2Ta62sblA20DBwSQpxMauiiTfp8Szr3f7QVgFtHx5JZXMah7BIuH9DZvoEJ0YZJQhdtxqGcEjYfzmdc707WZA7w6JXx9gtKCAciCV20GS+u2McXWzMalIXIcz+FsJkkdNEmVFXXsHpvtnX547tG4uPhRoC3ux2jEsKxSEIXbULS4XwKSyuty8NiguwYjRCOSXq5CLvLKi7jia924eFqvo7DYgLtHJEQjklq6MJufkkrpKK6mrs+2Ex2cTlXDuzCo5Pj8fWUr6UQ50L+ckSr01oDcOU/f7SWfXj7cEZ2D5YHOQtxHmxqclFKTVJK7VVKJSul5jWx/i6l1C9Kqa1KqR+VUtLPTJzS89/uY9xzq63LQ6IDGNUjRJK5EOfpjAldKeUKvApcBsQDs5tI2B9qrftrrQcBzwDPN3egwjn8fCCHV1Ylk5J7AoAO7q68OHOwnaMSwjnYUkNPBJK11ge11hXAImBq/Q201kX1Fn0A3XwhCmexLbWAmxdswNfTDf8Opjvil/deQHSwt50jE8I52NKGHgGk1ltOA4Y33kgpdQ/wAOABjG9qR0qpOcAcgOhoeUxYe1FwooKXVu6nuKwKNxcXfvjdOFxdFav3ZtNdZk0Uotk0201RrfWrwKtKqeuAPwE3N7HNfGA+QEJCgtTi24GNKXnMeKNugq3hsUEE+ngAMGVgF3uFJYRTsqXJJR2IqrccaSk7lUXAVecRk3AitQ+kqDUoOsA+gQjRDthSQ98IxCmlYjGJfBZwXf0NlFJxWuv9lsUrgP2Idm17mmkvzz9hRn8OjAqgb5eO3DC8q50jE8J5nTGha62rlFJzgeWAK7BAa71TKfU4kKS1XgrMVUpdAlQC+TTR3CLal+/3ZluT+aV9w3jzxgQ7RySE87OpDV1rvQxY1qjs0Xqv72vmuISD255eiJ+XG0E+Htw8Ksbe4QjRLshIUdFsvt+XTZ9wPw7llPDtrkymDOzCy7Olj7kQrUUSumgWWUVl3LxgA91CfKiorsHHw5V7L+5h77CEaFckoYtm8c2uTAAO5pQA8OCEnvTo5GfPkIRod2T6XHHeMgpK+WJrXU9WHw9XbhkdY7+AhGinpIYuzsvaA7nMfmtdg7LnZgzEz0ueNCREa5MaujhnVdU13P/RFuvy9cOj6R3ux6geIXaMSoj2S2ro4qwlZx1n0YYjrDuUS2ZROc9MH0Bnfy9GdQ/B1UWmwBXCXiShi7P26/9sYl/mcety91AfhnaVZ4AKYW+S0IVN0vJPsDOjiPjOHRskc4CYYB87RSWEqE8SurDJTe9s4GBOCX5eJ39lgiyzJwoh7EsSurBJbf/y4rIqAJ66uj9uLors4+Xy6Dgh2ghJ6MImri6K6hozhf37tyYytmeonSMSQjQm3RbFKRWXmdkSjxaWWpP5zIQoRku3RCHaJKmhiyZ9uP4If/jsF64eHMFnW8wo0E/uHim9WYRowyShiwa0NjXxV79LBrAmc4Ah0YF2iUkIYRtJ6MIqNe8EV7/2Mz6erqQXlOLp5oK3hyv3XRzH8G7BcvNTiDZOErqwWr7zGDnHy8mxdDN/66YExsSFSCIXwkFIQhcAbDqczxNf7Sa8oxcnKqqortEMjAyQZC6EA5GELgBY8NMhAO4Z150ZCVFoDR08XO0clRDibEhCF+SXVLB6TxazE6O4cWSMvcMRQpwjSejt3PPf7uO9n1OorNbcMKKrvcMRQpwHmwYWKaUmKaX2KqWSlVLzmlj/gFJql1Jqu1JqpVJKMoMDyD1eziur9tO3S0f+fVsifbv42zskIcR5OGNCV0q5Aq8ClwHxwGylVHyjzbYACVrrAcAS4JnmDlScv/ySCia9uIZNh/PRWrN8ZyZawyOX9WF4t2B7hyeEOE+2NLkkAsla64MASqlFwFRgV+0GWuvv6m2/DrihOYMUzWP9oTz2HCtm2us/W8v6R/jTt0tHO0YlhGgutjS5RACp9ZbTLGWnchvwdVMrlFJzlFJJSqmk7Oxs26MU521HeiFbUwsalPWL6Mh7tybiIk8ZEsIpNOtNUaXUDUACcGFT67XW84H5AAkJCbo5P1uc2tHCUia/8iMAkYEd+P7hcbgo0BpJ5kI4EVsSejoQVW850lLWgFLqEuCPwIVa6/LmCU+cL601L69Mti4PiQ60PvdTxgwJ4VxsSegbgTilVCwmkc8Crqu/gVJqMPAmMElrndXsUYpzorXm+33ZLNxwhJ5hvlRU1fC7Sb3sHZYQooWcMaFrrauUUnOB5YArsEBrvVMp9TiQpLVeCjwL+AIfW4aKH9FaT2nBuMUZZBaVsWZfNg8v2Q7AF/dcICM/hXByNrWha62XAcsalT1a7/UlzRyXOA8/7s/hhnfWW5dnJ0ZJMheiHZAnFjmZ/JIKnlq227p8zeAInriqvx0jEkK0Fhn670QeW7qTf/2cYl0eFhPI9IRI601QIYRzk4Tu4GpqNAs3HqFHqG+DZP6Hy3szZ2x3+wUmhGh1ktAdWGlFNc8s38O7P6VYy+aO68EDE3pK/3Ih2iFpQ3dg761NsSbzhy/tRXznjtw0sqskcyHaKamhO6Bf0gp5eMk29hwrBmDWsCjuGdeDe8b1sHNkQgh7koTugO5btIWDOSUAeHu48vS0AXaOSAjRFkiTi4OoqKphcVIqX2xN52BOCdcmRAIQ6udp58iEEG2F1NAdwK6MIl79LpmvfjlqLbt6cCTDYoIY0jXQjpEJIdoSSehtXFZxGZNf+YGaenNT9ujky8Aof0Z2l4dSCCHqSEJvg44WljLlnz/x7PQBHMguoUbD9KGRzE6MZnBUgPRiEUI0SRJ6G7LpcB6fbk7ni60ZHC+v4pZ3N1rX/X3aABnxKYQ4LUnodqa15h/f7GNCfBjTXl/b5Db/mDFQkrkQ4owkodtRfkkFd32wifWH8vjnd3UPoUiMCeLtWxL49Qebmdg3jGlDI+0YpRDCUUhCt4Pj5VVsTyvgurfWNyi/aWRXrhzYhZ5hfnT0cueD24fbKUIhhCOShN6KsorKCPXz5O4PNvHD/pwG67b9ZSL+HdztFJkQwhlIQm8lWUVlJD61kkv6hFmT+e8n9ebahEgKSislmQshzpsk9BaWXVyOfwd3Plh3GIAVuzMBeOumBCbEhwEQ7CujPYUQ508SegupfUDzr/61EV1vUNA1QyK4NiGK4bFB9gtOCOGUJKG3kC+2ZnD/R1sBGBjpz7a0Qvw7uPP8tYPsGpcQwnnZlNCVUpOAlwBX4G2t9dON1o8FXgQGALO01kuaOU6Hkl5QymP/3QnAo5PjufWCWPJKKqiqrrFzZEIIZ3bGhK6UcgVeBSYAacBGpdRSrfWuepsdAW4BHmqJINu6b3dlsmpPFp5uLni6ufDJ5jQKTlQyZWAXbr0gFoAgHw87RymEcHa21NATgWSt9UEApdQiYCpgTeha6xTLunZVBdVasy/zOHe8n9SgvLO/FzeMiOZOeaanEKIV2ZLQI4DUestpwDmNeFFKzQHmAERHR5/LLtqMH/fncMM7dQODJg/ozJ+uiCfA2x1PNxeUkqH6QojW1aoPuNBaz9daJ2itE0JDQ1vzo5tNYWkl6QWlPPTxtgblD1/ai3B/L7zcXSWZCyHswpYaejoQVW850lLW7iRnFXPbe0kczj0BwNieoUQHdSDMz4voIG87RyeEaO9sSegbgTilVCwmkc8CrmvRqNoArbW1pl1ZXcPGQ3nWPuURAR2orK7hjRuG4O0hPT+FEG3DGbOR1rpKKTUXWI7ptrhAa71TKfU4kKS1XqqUGgZ8BgQCVyql/qq17tuikbeg9IJS7ngviV7hfkwd1IXffLiF4vIqIgI6sGjOCKKCvCmrrMbL3dXeoQohhJXS9YcxtqKEhASdlJR05g1bWU2N5prXf2ZraoG1LDrImzsv7Mbk/l3w95Y5V4QQ9qOU2qS1TmhqneO1FxzdBqkbml7n4Qu9LwdXD1j/Jmx+H0bNhYytMOg6iB5x0lsKSyt54dt9RAd5k5ZfCsDW1AKeuro/vl5ubD6cz00ju9It1LcFD0oIIc6f4yX0g6vh20dPvd7dB3xCoMBMhsWXvzU/N78HI+4BpSjLS2Vp3JMUlVayPyOXj7cco6ZRh5/L+oUT6OPBlIFdWuY4hBCimTleQh92Oww8xT3ZgiOQ9A4cWWe22/g2AEcDh9E5fyOsexUALyB3Zw0/1fTjA4//44qQy1gW+weGxQSxam8WHb3cCZSRnUIIB+OQbehlldVkF5dTXaM5VlRGTRPHsCujCLX6aW6r/ogJ5c/wrefvTr/TxwrPKRYhhGhNztWGDsyav67BTctTiQ68Fj30Hv4Y1ZkD+RF0iB2GlysEFfwCq5+GtI11Gx/PhtT1EBIHob1aLnghhGghDpfQq2s0uzKKuKRPJyb160x4Ry/cXE8emenj4Ua/iI71Rm1OrlsZEgb+UfBqYl3Zh9dCxmZzQ/W3u8DXMUeyCiHaL4dL6MeKyqiormF87zCmD4089x2F9oJbl0N5sblxmrHZlFdXwHM9wNUTelwCk54CFzfwt+GzaqrheCZ0PMWN1PzDEBANMjWAEKIFOFxCP5xTAkDX4GYYal/bjXH6u5B3EHpdBq+NgKJ0qC6H/cth71dmmxG/ho4R8OML4B8Bo++DXpdDRQmkJZntOgTCTy/BrA/N666joKYGcvfDlg/g55dh4pOmK2Wt6iqoPAFeHc1yZRkkfwtdR0POfnMyiRxq1pXkgHKBqnJ4fwpcMx+6DD7/34MQwik4XkLPM/OoNEtCrxU1zPwDk9wXTASfTnD7CvjfI5CxBda/AdoyO/CJHFhy66n3t8jSC+fX62HV32DPl3XrfnoJSrIgaw+M+g0s/wMUH4WZH0DWbvjy/pP3N+d7KC+C9640y536Qs4+WPc6XPkylOaZq4KCIybZ6xpYchtct8i2KwshhFNwuITu5e7CwEh/Ovt3aJkP6DIYel1hkm1gV5j9oSkvSDWDmrpdCJk74UQeLJpt1g2/2zS17Py04b5eazTLcNxE04/+p5fM8v7ldesWXHrqmHZ+CiW5dctZ5mlI7PrC1OIzNsOkv8Paf5orhtgxkPkLpPwEA2eevL/iTHOCGvsweMikYkI4C4fstthm7FsOZUUwYIZpPz+Ra2rjX/4WwgdA93EQPxW+ewqSV8BVr5vtlj0MPS6Gg9/DzUtNLfrbv8C2D0/9WcoFOkaCT7C5YrBFz0kwfYE5AYX3Nz8jE+CTO+CXxTDmIXPi8uwILo1mUs5PMTeI698PKM2H7581VzPxV1niOof7AVrLfQQhztHpui1KQm9uFSdg9f/BBb8F7yBTtv1j+PR2uHsthMWbdnJ3r7qfYJLc/m8hKhF+fgV+eM6UD7oetv7HvH7EMmvx/0WAhx8ExcKx7XDN27DsQUub/nHY/d+T4/KPhsIj0GcK7F0GNVV164LjYMor5kRRU2muIg6sMs1OD++v227rh/D53Zb9RUHUcHM/ILRP3XEAVFean5k7ofPAk5P3ktvMCW7e4ZPjlGQvxGlJQm8LSvPNjVJbPeZvft75A7w5BsY8CBdbpjzI2GrmrfELM8m392STyN19THt6xlbY9K65WvAJhZLshvsO6wfT3oFPboeIIbD9I6gqazqO21eatv2idCjKMFMoNDbmQQiMgeSVENLTNOeUF5l1PSeBl7+5GVzbFbT22PpNg77XmGMoTIV9/4OArnDXj3U3ievLOwjpmyFiqDmZ1Xc8y/xOpAlJODlJ6I5ow1uw5jl4cI9pn/fpdHKzyOlUnDA9dToEmh42X9xjmlbu+tE08bjUm/o3aUHdnDe1ek9ueDO3locfVBSf/fH4dTEnDZ8Qc0P3TKJHweQXYNtCM5VD/FRY/ohZF5EAd6w0r7WGda+Zm8txl8L1i+v2kXfIdEMNiDZNY35h5l5I2gZzAtr0L0icY5qjGivNNz8rSmDFX2HS04CGD6aZnkdXPAcf3wITn4A+V9a9r6ocvnzAXGnFjj35xHM2qqtgxxLoeenJlYGqCnNzvmMXc38lZQ30vfrcP+tcaW1uxgd2bVhekgtunuDZBie1O7zWxNXU/7sDkITe3uUegFeGmGT8h7SmtylIhc/uhMM/QeQw00d//ZumKSViKCy+ybSr97kSSgsg5QeTzKrKTU8eMG3ufa40XTrfHGvKfMOgxwTY+oFZdvc23TRt4ephEnJTul9smnR0jek1VGvcn0xi8/CGlwaZk1pIL8jZa5qsPr+rYZNU54Fw0SOQvsnc9+gUD0Vp8N/7TFIacResedZsGzPGHHdjw+8yCV8p01T170aJVbnC6HvNzXOfEHMy1Rr2f1PXXXbNc+ZK6vLnTJPbmAfh0Pfw6R3m5DP7I/Pz55fNPjO2mq6y847ARzeabe/bbhLrpvfAwwe6jzdXLW715iUqyTHNdN3H15Xlp5iTX/dxZvnAKgiMrTsZff8sHN1q7gG5uJnvSPgAQMOupfD1w6aiUD9BPuZvfpfdx0P/6XXda2vv/3QZDJm7oKrUfL/A9PxKXmHeExbf9P97U45nmcpCx0hzZdgh4NTb1tTA45aT4yNp5v+gz1RwbdQ/pDjTVABqZe+FrF11946OZ4Jf+Jljq6mB7D2mEuXhC/+9F9Aw9VXbj68RSejtndamBt5/OsRccG77+GCa+WOb+ir0v9bUykJ6mPby7L0Q1rdh2/euL8wfdEicWT78M5Qfh7gJcPC7uqTn1xnuXAM7PzM3fiOGwJp/mFrpf++t25+LOzy0D9a+Wnd/QbmCrjav5ybBwlmQm3zqY4geBUd+Nq8HzDInggOrTn/crp7mpFArINp8bv4hc9XgHWImhAuMNQnv3Uln/l1Oe8ckodorjqa4uJv7GfV1ijdJpb56k9Ax/V1zMvtrQN36hNvMSWLRdbDv63r7d4NZC83J6+t55hhv+cqczJ/oZLYZOBu8g+HQGnMSAHO8J3JOjveyZ01S1dWmN9fro+rWDZgJ4/4AvuHwpCVJXrfYjM4G+FO2aSL8+veANjHc8KllvMVUM+4jIBrG/dF8jzK2mGk6yovh0qfgg2vMtlhyWb/pUFZo4pn+Lvz0opk+O++gme6jtpdZt3Hmu6hczfc0INqMNdn0rll/wQMw4m5Y+VdzlQsw4z3zuUvnmpN4UTok3GqS9RdzoVNv8/vqf63pjuwXbq4eY8ea+2Gf3Wn28+C+hieMsyAJXZy/7H2mdppw6/nftKyugr8FQ2SiuS8QO6bp7b56yPzxdrsQBt8AQd1ML6HN75k/vB4T4Mha84fb42Jz4pp/kalN1rrkMVjxmHkd0NVMq3z5c5B4h9n+g2uaTurh/c1N5u+fgSE3mf28fYm5J1BTCRvmw10/mST77iSTYGr1nGQSzY5P4bsnTv176HaRqWmXFZgmLv8oc3LwCjAnm9qku+whqCytuxIZ9RtTi2+sxyXmmA6sbFgee6GpwdsiMMbU2FuCX2eT5E4nbqKpNbt6mo4F3z9dt87Nq+l7Pd4h5gZ97SDA023bWFMnzqaM+g2sfa2uAnGuaishtd/Bc9mFJHTR5tR+75q7R0t+Cuz83NQuT+Sak8DPL5uabIdA07TSZXDdPYRDa8yArb7XmHsGkcPgpi9MMlXKNAP4hZvL+Ooq876qMlNTrm0qqKk2c/RvmA9X/AMG31h3XBUnzCX3W5bmjLvXwhujTVPR3E2mZpzyEwyfY9ZXVZjPWPe6aYoJ7m72r1zM6ODUDfD7FDi2w9xTyTtkxiJk7mj6ngeYY4kZAxfNMyfRXz42N6Frm8rANButf8P23/PYh+uaonpPNvc50DDqXljxl7rtGifW4XdD9HDze3dxN/dvairB0x8mP296hjVusgJzpVGYBgm/Ms09n99dd5Ia/2cY+5CppX9yuzkRx02EZ7s13EdwD3PPZPANpqaeuQuG32lGeofFW372g38ONTX8WtPeMVe3u7+Er39nauVgTtyDbzTjSXKSzRVr2iZzDGF9zT2aqrKGHRNuX2m+D90vho6dbf991yMJXYjTqSwF9w513S1dz+ExgzU1Jjm7n2LAW8pPkHfA1PaLj5l/XQad3WecyDMJvHYqiMaydpv9eviaG9ehvU3CHH63Gb/Q2OGfzRVGxhbTJPDWeHN1M/EJk4hO5Js2+W0LTS+lHZ+YQWxXvgydB5geR6nrTbNE7fHrGtM8N/p+c2V1dBusfNy0u4OZ+M4/oi6GylJAmZOOq5vlCmOV+ZySHHOl1CneNMU1Vphujm/Ub5puN9/4thlVXV5krmhmL7LtJm1VOaT8aNrJ+17TsEuu1uZ37Bd+5spI8THTgysqET6901y11Y5IPw+S0IUQZ1ZpqUnXT2DN5fBac3VUfx4jcU6cbj50IUQLaIlEXqvrSPNPtCibOjYrpSYppfYqpZKVUvOaWO+plPrIsn69Uiqm2SMVQghxWmdM6EopV+BV4DIgHpitlGrcSfQ2IF9r3QN4Afh7cwcqhBDi9GypoScCyVrrg1rrCmARMLXRNlOB2jHhS4CLlZIJOYQQojXZktAjgNR6y2mWsia30VpXAYXASbfVlVJzlFJJSqmk7OzsxquFEEKch7OYHOT8aa3na60TtNYJoaHyzE4hhGhOtiT0dCCq3nKkpazJbZRSboA/kIsQQohWY0tC3wjEKaVilVIewCxgaaNtlgI3W15PB1Zpe3VwF0KIduqM/dC11lVKqbnAcsAVWKC13qmUehxI0lovBd4B/q2USgbyMElfCCFEK7LbSFGlVDbQxCNrbBICNDHlm1OTY24f5Jjbh/M55q5a6yZvQtotoZ8PpVTSqYa+Ois55vZBjrl9aKljbtVeLkIIIVqOJHQhhHASjprQ59s7ADuQY24f5JjbhxY5ZodsQxdCCHEyR62hCyGEaEQSuhBCOAmHSuhnmpfdUSmlFiilspRSO+qVBSmlvlVK7bf8DLSUK6XUy5bfwXalVBPP5mr7lFJRSqnvlFK7lFI7lVL3Wcqd9riVUl5KqQ1KqW2WY/6rpTzW8hyBZMtzBTws5U7znAGllKtSaotS6kvLslMfs1IqRSn1i1Jqq1IqyVLW4t9th0noNs7L7qj+BUxqVDYPWKm1jgNWWpbBHH+c5d8c4PVWirG5VQEPaq3jgRHAPZb/T2c+7nJgvNZ6IDAImKSUGoF5fsALlucJ5GOeLwDO9ZyB+4Dd9ZbbwzGP01oPqtffvOW/21prh/gHjASW11t+BHjE3nE14/HFADvqLe8FOltedwb2Wl6/CcxuajtH/gd8AUxoL8cNeAObgeGYEYNulnLr9xwz3cZIy2s3y3bK3rGfw7FGWhLYeOBLQLWDY04BQhqVtfh322Fq6Ng2L7szCdNaH7W8PgaEWV473e/Bclk9GFiPkx+3pelhK5AFfAscAAq0eY4ANDwum54z4ABeBH4H1FiWg3H+Y9bAN0qpTUqpOZayFv9uy0OiHYDWWiulnLJ/qVLKF/gEuF9rXVT/QVfOeNxa62pgkFIqAPgM6G3fiFqWUmoykKW13qSUusjO4bSmC7TW6UqpTsC3Sqk99Ve21HfbkWrotszL7kwylVKdASw/syzlTvN7UEq5Y5L5f7TWn1qKnf64AbTWBcB3mOaGAMtzBKDhcTnDcwZGA1OUUimYx1eOB17CuY8ZrXW65WcW5sSdSCt8tx0podsyL7szqT/H/M2YNuba8pssd8ZHAIX1LuMchjJV8XeA3Vrr5+utctrjVkqFWmrmKKU6YO4Z7MYk9umWzRofs0M/Z0Br/YjWOlJrHYP5m12ltb4eJz5mpZSPUsqv9jUwEdhBa3y37X3z4CxvNFwO7MO0O/7R3vE043EtBI4ClZj2s9sw7YYrgf3ACiDIsq3C9PY5APwCJNg7/nM85gsw7Yzbga2Wf5c783EDA4AtlmPeATxqKe8GbACSgY8BT0u5l2U52bK+m72P4TyP/yLgS2c/ZsuxbbP821mbq1rjuy1D/4UQwkk4UpOLEEKI05CELoQQTkISuhBCOAlJ6EII4SQkoQshhJOQhC6EEE5CEroQQjiJ/wen69jyW17kNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN6Fg_BsxJe6"
   },
   "source": [
    "### 5 - Predicción de próxima palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "iy_AXWQWzeeE"
   },
   "outputs": [],
   "source": [
    "# Keras pad_sequences\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "# Si la secuencia de entrada supera al input_seq_len (3) se trunca\n",
    "# Si la secuencia es más corta se agregna ceros al comienzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "IBvKHFPmzpy2"
   },
   "outputs": [],
   "source": [
    "# Se utilizará gradio para ensayar el modelo\n",
    "# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n",
    "# https://gradio.app/\n",
    "import sys\n",
    "!{sys.executable} -m pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "HNyBykvhzs7-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmanuel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x291d13434f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 800ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x291c62b5d20>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def model_response(human_text):\n",
    "\n",
    "    # Encodeamos\n",
    "    encoded = tok.texts_to_sequences([human_text])[0]\n",
    "    # Si tienen distinto largo\n",
    "    encoded = pad_sequences([encoded], maxlen=3, padding='pre')\n",
    "    \n",
    "    # Predicción softmax\n",
    "    y_hat = model.predict(encoded).argmax(axis=-1)\n",
    "\n",
    "    # Debemos buscar en el vocabulario la palabra\n",
    "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "    out_word = ''\n",
    "    for word, index in tok.word_index.items():\n",
    "        if index == y_hat:\n",
    "            out_word = word\n",
    "            break\n",
    "\n",
    "    # Agrego la palabra a la frase predicha\n",
    "    return human_text + ' ' + out_word\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=model_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=\"text\",\n",
    "    layout=\"vertical\")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCeMWWupxN1-"
   },
   "source": [
    "### 6 - Generación de secuencias nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "bwbS_pfhxvB3"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t\n",
    "\t\t# Predicción softmax\n",
    "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        # Debemos buscar en el vocabulario la palabra\n",
    "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_hat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += ' ' + out_word\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "JoFqRC5pxzqS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'esas refutaciones no resultaron resultaron conjetura'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='esas refutaciones no resultaron'\n",
    "\n",
    "generate_seq(model, tok, input_text, max_length=3, n_words=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'refiere a los idiomas del islas cualquiera'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='refiere a los idiomas del'\n",
    "\n",
    "generate_seq(model, tok, input_text, max_length=4, n_words=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'universo como una serie de procesos procesos nuestra'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='universo como una serie de procesos'\n",
    "\n",
    "generate_seq(model, tok, input_text, max_length=3, n_words=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Leimos con algun cuidado el objetos la'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='Leimos con algun cuidado el'\n",
    "\n",
    "generate_seq(model, tok, input_text, max_length=3, n_words=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2SHmXbgxQH9"
   },
   "source": [
    "### 7 - Conclusiones\n",
    "El modelo entrenado tuvo un muy mejor desempeño que en el caso visto en clase, en parte debido al agregado de más epochs a la RNN, aunque a la hora de predecir, las palabras no tienen relación directa con el resto de la oración propuesta.\n",
    "\n",
    "También destacaremos la menor cantidad de datos entrenados, teniendo en cuenta que el corpus se compone de una menor cantidad de texto, con lo que el entrenamiento resulta un tanto escaso.\n",
    "\n",
    "Finalmente: Se probaron varias combinaciones de capas y valores distintos de dropout entre ellas, sin embargo se volvió al modelo original con 1 sola capa hidden, y con un dropout de 0.6 (originalmente se encontraba en 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOkfjJIpCd091Vpf71hUImQ",
   "collapsed_sections": [],
   "name": "4d - predicción_palabra.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
